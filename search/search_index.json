{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"<p>Click here if you are not redirected automatically</p>"},{"location":"core-concepts/classification/","title":"Document Classification","text":"<p>In document intelligence, classification is often the crucial first step. It sets the stage for subsequent processes like data extraction and analysis. Before the rise of LLMs, this used to be accomplished (and still is) with AI models training in-house for certain use cases. Services such as Azure Document Intelligence give you this feature, but they are not dynamic and will set you up for \"Vendor lock-in\".</p> <p>LLMs may not be the most efficient for this task, but they are agnostic and near-perfect for it.</p>"},{"location":"core-concepts/classification/#classification-techniques","title":"Classification Techniques","text":"<ul> <li> <p> Basic Classification</p> <p>Simple yet powerful classification using a single LLM with contract mapping.</p> <p> Learn More</p> </li> <li> <p> Mixture of Models (MoM)</p> <p>Enhance accuracy by combining multiple models with different strategies.</p> <p> Learn More</p> </li> <li> <p> Tree-Based Classification</p> <p>Handle complex hierarchies and similar document types efficiently.</p> <p> Learn More</p> </li> <li> <p> Vision Classification</p> <p>Leverage visual features for better accuracy.</p> <p> Learn More</p> </li> </ul>"},{"location":"core-concepts/classification/#classification-response","title":"Classification Response","text":"<p>All classification methods return a standardized response:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass ClassificationResponse(BaseModel):\n    name: str\n    confidence: Optional[int] = Field(\n        description=\"From 1 to 10. 10 being the highest confidence\",\n        ge=1, \n        le=10\n    )\n</code></pre>"},{"location":"core-concepts/classification/#available-strategies","title":"Available Strategies","text":"<p>ExtractThinker supports three main classification strategies:</p> <ul> <li>CONSENSUS: All models must agree on the classification</li> <li>HIGHER_ORDER: Uses the result with highest confidence</li> <li>CONSENSUS_WITH_THRESHOLD: Requires consensus and minimum confidence</li> </ul> <p>For detailed implementation of each technique, visit their respective pages.</p>"},{"location":"core-concepts/classification/basic/","title":"Basic Classification","text":"<p>When classifying documents, the process involves extracting the content of the document and adding it to the prompt with several possible classifications. ExtractThinker simplifies this process using Pydantic models and instructor.</p>"},{"location":"core-concepts/classification/basic/#simple-classification","title":"Simple Classification","text":"<p>The most straightforward way to classify documents:</p> <pre><code>from extract_thinker import Classification, Extractor\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n        contract=DriverLicense,  # optional. Will be added to the prompt\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,  # optional. Will be added to the prompt\n    ),\n]\n\n# Initialize extractor\ntesseract_path = os.getenv(\"TESSERACT_PATH\")\ndocument_loader = DocumentLoaderTesseract(tesseract_path)\nextractor = Extractor(document_loader)\nextractor.load_llm(\"gpt-4o\")\n\n# Classify document\nresult = extractor.classify(INVOICE_FILE_PATH, classifications)\nprint(f\"Document type: {result.name}, Confidence: {result.confidence}\")\n</code></pre>"},{"location":"core-concepts/classification/basic/#type-mapping-with-contract","title":"Type Mapping with Contract","text":"<p>Adding contract structure to the classification improves accuracy:</p> <pre><code>from typing import List\nfrom extract_thinker.models.contract import Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n    total_amount: float\n\nclass DriverLicense(Contract):\n    name: str\n    age: int\n    license_number: str\n</code></pre> <p>The contract structure is automatically added to the prompt, helping the model understand the expected document structure.</p>"},{"location":"core-concepts/classification/basic/#classification-response","title":"Classification Response","text":"<p>All classifications return a standardized response:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass ClassificationResponse(BaseModel):\n    name: str\n    confidence: Optional[int] = Field(\n        description=\"From 1 to 10. 10 being the highest confidence\",\n        ge=1, \n        le=10\n    )\n</code></pre>"},{"location":"core-concepts/classification/basic/#best-practices","title":"Best Practices","text":"<ul> <li>Provide clear, distinctive descriptions for each classification</li> <li>Use contract structures when possible</li> <li>Consider using image classification for visual documents</li> <li>Monitor confidence scores</li> <li>Handle low-confidence cases appropriately</li> </ul> <p>For more advanced classification techniques, see: - Mixture of Models (MoM) - Tree-Based Classification - Vision Classification </p>"},{"location":"core-concepts/classification/mom/","title":"Mixture of Models (MoM)","text":"<p>The Mixture of Models (MoM) is a pattern that increases classification confidence by combining multiple models in parallel. This approach is particularly effective when using instructor for structured outputs.</p>"},{"location":"core-concepts/classification/mom/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Process, Classification, ClassificationStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n    ),\n]\n\n# Initialize document loader\ntesseract_path = os.getenv(\"TESSERACT_PATH\")\ndocument_loader = DocumentLoaderTesseract(tesseract_path)\n\n# Initialize multiple extractors with different models\ngpt_35_extractor = Extractor(document_loader)\ngpt_35_extractor.load_llm(\"gpt-3.5-turbo\")\n\nclaude_extractor = Extractor(document_loader)\nclaude_extractor.load_llm(\"claude-3-haiku-20240307\")\n\ngpt4_extractor = Extractor(document_loader)\ngpt4_extractor.load_llm(\"gpt-4o\")\n\n# Create process with multiple extractors\nprocess = Process()\nprocess.add_classify_extractor([\n    [gpt_35_extractor, claude_3_haiku_extractor],  # First layer\n    [gpt4_extractor],                              # Second layer\n])\n\n# Classify with consensus strategy\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#available-strategies","title":"Available Strategies","text":""},{"location":"core-concepts/classification/mom/#consensus","title":"CONSENSUS","text":"<p>All models must agree on the classification:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#higher_order","title":"HIGHER_ORDER","text":"<p>Uses the result with the highest confidence score:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.HIGHER_ORDER\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#consensus_with_threshold","title":"CONSENSUS_WITH_THRESHOLD","text":"<p>Requires both consensus and minimum confidence:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#best-practices","title":"Best Practices","text":"<ul> <li>Use smaller models in the first layer for cost efficiency</li> <li>Reserve larger models for cases where consensus isn't reached</li> <li>Set appropriate confidence thresholds based on your use case</li> <li>Consider using different model providers for better diversity</li> <li>Monitor and log classification results for each model</li> </ul>"},{"location":"core-concepts/classification/tree/","title":"Tree-Based Classification","text":"<p>In document intelligence, challenges often arise when dealing with a large number of similar document types. Tree-based classification organizes classifications into a hierarchical structure, breaking down the task into smaller, more manageable batches.</p>"},{"location":"core-concepts/classification/tree/#basic-concept","title":"Basic Concept","text":"<p>Tree-based classification offers: - Increased Accuracy: By narrowing down options at each step - Scalability: Easy addition of new document types - Reduced Context: Smaller context windows at each level</p>"},{"location":"core-concepts/classification/tree/#implementation","title":"Implementation","text":"<p>Here's how to implement a classification tree:</p> <pre><code>from extract_thinker import Classification, ClassificationNode, ClassificationTree\nfrom extract_thinker.models.contract import Contract\n\n# Define contracts for each level\nclass FinancialContract(Contract):\n    total_amount: int\n    document_number: str\n    document_date: str\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n    total_amount: float\n\nclass CreditNoteContract(Contract):\n    credit_note_number: str\n    credit_note_date: str\n    lines: List[LineItem]\n    total_amount: float\n\n# Create the classification tree\nfinancial_docs = ClassificationNode(\n    classification=Classification(\n        name=\"Financial Documents\",\n        description=\"This is a financial document\",\n        contract=FinancialContract,\n    ),\n    children=[\n        ClassificationNode(\n            classification=Classification(\n                name=\"Invoice\",\n                description=\"This is an invoice\",\n                contract=InvoiceContract,\n            )\n        ),\n        ClassificationNode(\n            classification=Classification(\n                name=\"Credit Note\",\n                description=\"This is a credit note\",\n                contract=CreditNoteContract,\n            )\n        )\n    ]\n)\n\n# Create the tree\nclassification_tree = ClassificationTree(\n    nodes=[financial_docs]\n)\n\n# Initialize process\nprocess = Process()\nprocess.add_classify_extractor([[extractor]])\n\n# Classify using tree\nresult = process.classify(\n    \"document.pdf\",\n    classification_tree,\n    threshold=0.95\n)\n</code></pre>"},{"location":"core-concepts/classification/tree/#level-based-contracts","title":"Level-Based Contracts","text":"<p>When implementing tree-based classification, consider contract complexity at each level:</p> <ul> <li> <p>First Level: Use minimal fields for broad categorization   <pre><code>class FinancialContract(Contract):\n    total_amount: int  # Just key identifying fields\n</code></pre></p> </li> <li> <p>Second Level: Include full field set for precise classification   <pre><code>class InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]  # Complete field set\n    total_amount: float\n</code></pre></p> </li> </ul>"},{"location":"core-concepts/classification/vision/","title":"Vision Classification","text":"<p>A document is not only text but also structure, color, and other numerous features that disappear when OCR is used. Vision classification leverages these visual elements to improve accuracy, particularly important for specific document types.</p>"},{"location":"core-concepts/classification/vision/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Process, Classification\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications with example images\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n        contract=DriverLicense,\n        image=\"path/to/example_license.png\"  # Example image helps model understand\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,\n        image=\"path/to/example_invoice.png\"\n    )\n]\n\n# Initialize process with vision-capable model\nprocess = Process()\nprocess.add_classify_extractor([[\n    Extractor(DocumentLoaderTesseract(tesseract_path))\n    .load_llm(\"gpt-4o\")  # Vision-capable model\n]])\n\n# Classify with vision enabled\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    image=True  # Enable vision processing\n)\n</code></pre>"},{"location":"core-concepts/classification/vision/#benefits-and-tradeoffs","title":"Benefits and Tradeoffs","text":""},{"location":"core-concepts/classification/vision/#benefits","title":"Benefits","text":"<ul> <li>Better handling of document layouts</li> <li>Recognition of visual patterns and structures</li> <li>Improved accuracy for visually distinct documents</li> <li>Ability to understand non-textual elements</li> </ul>"},{"location":"core-concepts/classification/vision/#tradeoffs","title":"Tradeoffs","text":"<ul> <li>Higher cost due to image processing</li> <li>Larger context window requirements</li> <li>Longer processing times</li> <li>Higher token usage</li> </ul>"},{"location":"core-concepts/classification/vision/#model-selection","title":"Model Selection","text":"<p>Different models offer varying capabilities for vision tasks:</p> <ul> <li>GPT-4 Vision: Supports low/high/auto quality settings (85 tokens for low)</li> <li>Claude 3 Sonnet: Full vision capabilities without quality options</li> <li>Azure Phi-3 Vision: Cost-effective alternative</li> </ul>"},{"location":"core-concepts/classification/vision/#best-practices","title":"Best Practices","text":"<ul> <li>Use compressed images when possible to reduce costs</li> <li>Provide high-quality example images for each classification</li> <li>Consider using a mix of vision and text-based classification</li> <li>Use appropriate image quality settings based on needs</li> <li>Cache vision results to avoid reprocessing</li> </ul>"},{"location":"core-concepts/classification/vision/#example-with-multiple-models","title":"Example with Multiple Models","text":"<pre><code># Initialize extractors with different vision models\ngpt4_vision = Extractor(document_loader)\ngpt4_vision.load_llm(\"gpt-4-vision\")\n\nclaude_vision = Extractor(document_loader)\nclaude_vision.load_llm(\"claude-3-sonnet\")\n\nphi3_vision = Extractor(document_loader)\nphi3_vision.load_llm(\"phi-3-vision\")\n\n# Create process with vision models\nprocess = Process()\nprocess.add_classify_extractor([\n    [phi3_vision],           # Cost-effective first attempt\n    [claude_vision, gpt4_vision]  # More capable models if needed\n])\n\n# Classify with vision and consensus\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9,\n    image=True\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/","title":"Completion Strategies","text":"<p>ExtractThinker provides different strategies for handling document content processing through LLMs, especially when dealing with content that might exceed the model's context window. There are three main strategies: Forbidden, Concatenate, and Paginate.</p>"},{"location":"core-concepts/completion-strategies/#forbidden-strategy","title":"FORBIDDEN Strategy","text":"<p>The FORBIDDEN strategy is the default approach - it prevents processing of content that exceeds the model's context window. This is the simplest strategy, while larger content can be handled using other available strategies.</p> <pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Will raise ValueError if content is too large\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.FORBIDDEN # Default\n)\n</code></pre> <p>For more advanced strategies that handle larger content, see:</p> <ul> <li>CONCATENATE Strategy - For handling content larger than the context window</li> <li>PAGINATE Strategy - For processing multi-page documents in parallel</li> </ul> <p>The choice of completion strategy depends on your specific use case:</p> <p>Use FORBIDDEN when:</p> <ul> <li>Content is guaranteed to fit in context window</li> <li>You need the simplest possible processing and default behavior</li> <li>You want to ensure content is processed as a single unit</li> </ul> <p>Use CONCATENATE when:</p> <ul> <li>Content might exceed context window</li> <li>The size exceeds the output but not the input context window.</li> <li>You want automatic handling of large content</li> </ul> <p>Use PAGINATE when:</p> <ul> <li>Processing multi-page documents</li> <li>The size exceeds the output but and the input context window.</li> <li>You need sophisticated conflict resolution between pages</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/","title":"Concatenate Strategy","text":"<p>The Concatenate strategy is designed to handle content that exceeds the LLM's context window by splitting it into manageable chunks, processing them separately, and then combining the results.</p>"},{"location":"core-concepts/completion-strategies/concatenate/#how-it-works","title":"How It Works","text":"<p>1. Initial Request</p> <ul> <li>Sends the content to the LLM with the desired response structure</li> <li>Monitors the LLM's response completion status</li> </ul> <p>2. Continuation Process</p> <ul> <li>If response is truncated (finish_reason=\"length\"), builds a continuation request</li> <li>Includes previous partial response for context</li> <li>Continues until LLM indicates completion</li> </ul> <p>3. Validation</p> <ul> <li>When LLM indicates completion (finish_reason=\"stop\")</li> <li>Validates the combined JSON response</li> <li>Raises error if invalid JSON is received on completion</li> </ul> <p>4. Response Processing</p> <ul> <li>Combines all response parts</li> <li>Validates against the specified response model</li> <li>Returns structured data</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/#usage","title":"Usage","text":"<pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.CONCATENATE\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/concatenate/#benefits","title":"Benefits","text":"<ul> <li>Handles Large Content: Can process documents larger than the output context window</li> <li>Maintains Context: Attempts to keep related content together</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/#implementation-details","title":"Implementation Details","text":"Concatenation Handler Implementation <p>The ConcatenationHandler implements the CONCATENATE strategy: <pre><code>import copy\nimport yaml\nimport json\nfrom typing import Any, Dict, List, Optional\nfrom pydantic import BaseModel\nfrom extract_thinker.completion_handler import CompletionHandler\nfrom extract_thinker.utils import encode_image, add_classification_structure\n\nclass ConcatenationHandler(CompletionHandler):\n    def __init__(self, llm):\n        super().__init__(llm)\n        self.json_parts = []\n\n    def _is_valid_json_continuation(self, response: str) -&gt; bool:\n        \"\"\"Check if the response is a valid JSON continuation.\"\"\"\n        if not response:\n            return False\n\n        cleaned_response = response.strip()\n\n        # Check if response contains JSON markers\n        has_json_markers = (\n            \"```json\" in cleaned_response or \n            \"{\" in cleaned_response or \n            \"[\" in cleaned_response\n        )\n\n        return has_json_markers\n\n    def handle(self, content: Any, response_model: type[BaseModel], vision: bool = False, extra_content: Optional[str] = None) -&gt; Any:\n        self.json_parts = []\n        messages = self._build_messages(content, vision, response_model)\n\n        if extra_content:\n            self._add_extra_content(messages, extra_content)\n\n        retry_count = 0\n        max_retries = 3\n        while True:\n            try:\n                response = self.llm.raw_completion(messages)\n\n                # Validate if it's a proper JSON continuation\n                if not self._is_valid_json_continuation(response):\n                    retry_count += 1\n                    if retry_count &gt;= max_retries:\n                        raise ValueError(\"Maximum retries reached with invalid JSON continuations\")\n                    continue\n\n                self.json_parts.append(response)\n\n                # Try to process and validate the JSON\n                result = self._process_json_parts(response_model)\n                return result\n\n            except ValueError as e:\n                if retry_count &gt;= max_retries:\n                    raise ValueError(f\"Maximum retries reached: {str(e)}\")\n                retry_count += 1\n                messages = self._build_continuation_messages(messages, response)\n\n    def _process_json_parts(self, response_model: type[BaseModel]) -&gt; Any:\n        \"\"\"Process collected JSON parts into a complete response.\"\"\"\n        if not self.json_parts:\n            raise ValueError(\"No JSON content collected\")\n\n        processed_parts = []\n        for content in self.json_parts:\n            # Remove code fences and extraneous formatting artifacts\n            cleaned = (content\n                       .replace('```json', '')\n                       .replace('```', '')\n                       .replace('\\njson', '')\n                       .replace('\\n', ' ')\n                       .strip())\n\n            # If there's still something left after cleaning, keep it\n            if cleaned:\n                processed_parts.append(cleaned)\n\n        if not processed_parts:\n            raise ValueError(\"No valid JSON content found in the response\")\n\n        # Combine all cleaned parts into one string\n        combined_json = \"\".join(processed_parts)\n\n        # Attempt to parse the combined JSON\n        try:\n            parsed = json.loads(combined_json)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Failed to parse combined JSON: {str(e)}\\nJSON: {combined_json}\")\n\n        # Validate the parsed JSON against the Pydantic model\n        try:\n            return response_model.model_validate(parsed)\n        except Exception as e:\n            raise ValueError(f\"Failed to validate parsed JSON: {str(e)}\\nJSON: {combined_json}\")\n\n    def _build_continuation_messages(\n        self,\n        messages: List[Dict[str, Any]],\n        partial_content: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for continuation request.\"\"\"\n        continuation_messages = copy.deepcopy(messages)\n\n        # Add partial response as assistant message\n        continuation_messages.append({\n            \"role\": \"assistant\",\n            \"content\": partial_content\n        })\n\n        # Add continuation prompt\n        continuation_messages.append({\n            \"role\": \"user\", \n            \"content\": \"## CONTINUE JSON\"\n        })\n\n        return continuation_messages\n\n    def _build_messages(self, content: Any, vision: bool, response_model: type[BaseModel]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for LLM request.\"\"\"\n        system_message = {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are a server API that receives document information and returns specific fields in JSON format.\\n\"\n                \"Please follow the response structure exactly as specified below.\\n\\n\"\n                f\"{add_classification_structure(response_model)}\\n\"\n            )\n        }\n\n        if vision:\n            message_content = self._build_vision_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n        else:\n            message_content = self._build_text_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n\n        return messages\n\n    def _build_vision_content(self, content: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build content for vision request.\"\"\"\n        message_content = []\n\n        if isinstance(content, list):\n            # Handle list of content items\n            for item in content:\n                # Add text content if available\n                if isinstance(item, dict) and \"content\" in item:\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": f\"##Content\\n\\n{item['content']}\"\n                    })\n\n                # Add images if available\n                if isinstance(item, dict):\n                    images = []\n                    if \"images\" in item and isinstance(item[\"images\"], list):\n                        images.extend(item[\"images\"])\n                    if \"image\" in item and item[\"image\"] is not None:\n                        images.append(item[\"image\"])\n\n                    for img in images:\n                        if img:\n                            message_content.append({\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                                }\n                            })\n        else:\n            # Handle single item\n            if isinstance(content, dict):\n                # Add text content if available\n                if \"content\" in content:\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": f\"##Content\\n\\n{content['content']}\"\n                    })\n\n                # Add images\n                images = []\n                if \"images\" in content and isinstance(content[\"images\"], list):\n                    images.extend(content[\"images\"])\n                if \"image\" in content and content[\"image\"] is not None:\n                    images.append(content[\"image\"])\n\n                for img in images:\n                    if img:\n                        message_content.append({\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                            }\n                        })\n\n        return message_content\n\n    def _build_text_content(self, content: Any) -&gt; str:\n        \"\"\"Build content for text request.\"\"\"\n        if isinstance(content, dict):\n            return f\"##Content\\n\\n{yaml.dump(content)}\"\n        elif isinstance(content, str):\n            return f\"##Content\\n\\n{content}\"\n        else:\n            return f\"##Content\\n\\n{str(content)}\"\n\n    def _add_extra_content(self, messages: List[Dict[str, Any]], extra_content: str) -&gt; None:\n        \"\"\"Add extra content to messages.\"\"\"\n        messages.insert(1, {\n            \"role\": \"user\",\n            \"content\": f\"##Extra Content\\n\\n{extra_content}\"\n        })\n</code></pre></p>"},{"location":"core-concepts/completion-strategies/concatenate/#when-to-use","title":"When to Use","text":"<p>CONCATENATE is the best choice when:</p> <p>Context window is large</p> <ul> <li>For models like gpt-4o, claude-3-5-sonnet, etc.</li> </ul> <p>The content is not too large</p> <ul> <li>Should be used for documents that are not too large (e.g. 500 pages)</li> </ul> <p>For handling bigger documents, consider using the PAGINATE strategy.</p>"},{"location":"core-concepts/completion-strategies/paginate/","title":"PAGINATE Strategy","text":"<p>The PAGINATE strategy processes multi-page documents by handling each page independently and then intelligently merging the results, including sophisticated conflict resolution when pages contain overlapping information.</p>"},{"location":"core-concepts/completion-strategies/paginate/#how-it-works","title":"How It Works","text":"<p>Page Separation</p> <ul> <li>Identifies individual pages</li> <li>Preserves page metadata</li> <li>Maintains document structure</li> </ul> <p>Parallel Processing</p> <ul> <li>Each page processed independently</li> <li>Uses full context window per page</li> <li>Handles page-specific content</li> </ul> <p>Result Collection</p> <ul> <li>Gathers results from all pages</li> <li>Validates individual page results</li> <li>Prepares for merging</li> </ul> <p>Conflict Resolution</p> <ul> <li>Detects overlapping information</li> <li>Resolves conflicts using confidence scores</li> <li>Maintains data consistency</li> </ul>"},{"location":"core-concepts/completion-strategies/paginate/#usage","title":"Usage","text":"<pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4\")\n\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.PAGINATE\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/paginate/#benefits","title":"Benefits","text":"<ul> <li>Cheaper: Reduced parallel context window would be cheaper than a long Concatenate Strategy</li> <li>Parallel Processing: Pages can be processed independently</li> <li>Conflict Resolution: Smart merging of results from different pages</li> <li>Scalability: Handles documents of any length</li> <li>Accuracy: Each page gets full context window attention</li> </ul>"},{"location":"core-concepts/completion-strategies/paginate/#implementation-details","title":"Implementation Details","text":"Pagination Handler Implementation <p>The PaginationHandler implements the PAGINATE strategy: <pre><code>import copy\nfrom typing import Any, Dict, List, Optional, get_origin, get_args, Union\nfrom pydantic import BaseModel, Field\nfrom instructor.exceptions import IncompleteOutputException\nfrom extract_thinker.completion_handler import CompletionHandler\nfrom extract_thinker.utils import encode_image, json_to_formatted_string, make_all_fields_optional\nimport yaml\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass ConflictResolution(BaseModel):\n    resolved_fields: Dict[str, Dict[str, Any]] = Field(\n        description=\"Dictionary of resolved field values with confidence scores\",\n        default_factory=dict\n    )\n\nclass PaginationHandler(CompletionHandler):\n    def __init__(self, llm):\n        super().__init__(llm)\n\n    def _make_hashable(self, item: Any) -&gt; Any:\n        \"\"\"Recursively convert a value to something hashable.\"\"\"\n        if isinstance(item, dict):\n            return tuple(sorted((k, self._make_hashable(v)) for k, v in item.items()))\n        elif isinstance(item, list):\n            return tuple(self._make_hashable(x) for x in item)\n        return item\n\n    def handle(self, \n               content: List[Dict[str, Any]],\n               response_model: type[BaseModel],\n               vision: bool = False,\n               extra_content: Optional[str] = None) -&gt; Any:\n        # Make fields optional to allow partial results\n        response_model_optional = make_all_fields_optional(response_model)\n\n        # Process pages in parallel\n        results = []\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for page in content:\n                # Build messages for each page\n                messages = self._build_messages(page, vision)\n                if extra_content:\n                    self._add_extra_content(messages, extra_content)\n\n                # Submit page processing task\n                future = executor.submit(\n                    self._process_page, \n                    messages,\n                    response_model_optional\n                )\n                futures.append(future)\n\n            # Collect results as they complete\n            for future in as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    # Log error but continue processing other pages\n                    print(f\"Error processing page: {str(e)}\")\n\n        if not results:\n            raise ValueError(\"No valid results obtained from any page\")\n\n        # Pair up the pages with their results for context\n        pages_data = list(zip(content, results))\n\n        return self._merge_results(results, response_model, pages_data)\n\n    def _process_page(self, messages: List[Dict[str, Any]], response_model: type[BaseModel]) -&gt; Any:\n        \"\"\"Process a single page with retry logic for incomplete responses\"\"\"\n        try:\n            return self.llm.request(messages, response_model)\n        except IncompleteOutputException as e:\n            # Handle partial response\n            partial_result = self._handle_partial_response(e, messages, response_model)\n            partial_dict = partial_result.model_dump()\n            if self._has_conflicts(partial_dict, response_model):\n                # We'll resolve conflicts later after merging all pages\n                return partial_result\n            return partial_result\n\n    def _merge_results(self, results: List[Any], response_model: type[BaseModel], pages_data: List[Any]) -&gt; Any:\n        \"\"\"Merge results from multiple pages into a dictionary, detect conflicts, resolve if needed, then return model.\"\"\"\n\n        # Collect all values for each field\n        field_values = {}\n        for _, result in pages_data:\n            result_dict = result.model_dump()\n            for field_name, field_value in result_dict.items():\n                field_values.setdefault(field_name, []).append(field_value)\n\n        # Merge fields\n        merged = {}\n        for field_name, values in field_values.items():\n            # Get the annotated type from the response model\n            field_type = response_model.model_fields[field_name].annotation if field_name in response_model.model_fields else None\n            non_null_values = [v for v in values if v is not None]\n\n            if field_type and get_origin(field_type) is list:\n                # Merge list fields using a more sophisticated approach\n                merged_list = self._merge_list_field(field_name, values, field_type)\n                merged[field_name] = merged_list\n            else:\n                # Scalar field handling\n                if len(non_null_values) == 0:\n                    # If the field is expected to be a string, default to an empty string.\n                    if field_type == str or (get_origin(field_type) is Union and str in get_args(field_type)):\n                        merged[field_name] = \"\"\n                    else:\n                        continue\n                else:\n                    # Build a mapping from the hashable version of each candidate to the original candidate.\n                    distinct_map = {}\n                    for candidate in non_null_values:\n                        key = self._make_hashable(candidate)\n                        if key not in distinct_map:\n                            distinct_map[key] = candidate\n                    distinct_values = list(distinct_map.values())\n\n                    if len(distinct_values) == 1:\n                        merged[field_name] = distinct_values[0]\n                    else:\n                        # Store conflicts in a special structure\n                        merged[field_name] = {\n                            \"_conflict\": True,\n                            \"candidates\": distinct_values\n                        }\n\n        # Check for conflicts and resolve if necessary\n        if self._has_conflicts(merged, response_model):\n            merged = self._resolve_conflicts(merged, response_model, pages_data, field_values)\n\n        # Clean merged dictionary to ensure it's compatible with the response model\n        merged = self._clean_merged_dict(merged, response_model)\n\n        # Filter out any keys with a None value,\n        # now every required field (e.g., a string like \"thinking\") will be non-null.\n        merged = {k: v for k, v in merged.items() if v is not None}\n\n        return response_model(**merged)\n\n    def _merge_list_field(self, field_name: str, values: List[Any], field_type: Any) -&gt; List[Any]:\n        \"\"\"\n        Merge list fields from multiple pages. If the list is a list of Pydantic models,\n        we try to merge based on a unique key field (e.g. `country` for countries, `region` for regions).\n        If it's not a list of models or no unique key is found, we just concatenate and\n        rely on conflict resolution later.\n        \"\"\"\n        # Flatten all lists\n        flattened = []\n        for v in values:\n            if isinstance(v, list):\n                flattened.extend(v)\n            elif v is not None:\n                flattened.append(v)\n\n        # Attempt to detect if we're dealing with a Pydantic model list\n        args = get_args(field_type)\n        if args:\n            model_type = args[0]\n            if hasattr(model_type, '__fields__'):\n                # We have a Pydantic model class in the list\n                # Identify a unique key to merge on. \n                candidate_keys = ['country', 'region', 'id', 'name']\n                unique_key = None\n                for ck in candidate_keys:\n                    if ck in model_type.__fields__:\n                        unique_key = ck\n                        break\n\n                if unique_key:\n                    # Merge by unique key using case-insensitive comparison\n                    merged_by_key = {}\n                    for item in flattened:\n                        if hasattr(item, 'model_dump'):\n                            item_dict = item.model_dump()\n                        else:\n                            item_dict = item\n                        key_val = item_dict.get(unique_key)\n                        if key_val is not None:\n                            normalized_key = str(key_val).lower()\n                            if normalized_key in merged_by_key:\n                                merged_by_key[normalized_key] = self._merge_two_models(\n                                    merged_by_key[normalized_key],\n                                    item_dict\n                                )\n                            else:\n                                merged_by_key[normalized_key] = item_dict\n                        else:\n                            # If no unique key found for this item, just store it uniquely\n                            merged_by_key[f\"no_key_{len(merged_by_key)}\"] = item_dict\n\n                    return list(merged_by_key.values())\n                else:\n                    # No unique key found, just return flattened list\n                    return flattened\n            else:\n                # Not a pydantic model list\n                return flattened\n        else:\n            # Not a parametrized list type\n            return flattened\n\n    def _merge_two_models(self, existing: Dict[str, Any], new: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge two dictionaries representing the same entity. For scalar values,\n        prefer the existing if both are non-null or prefer non-null values.\n        For lists, combine them.\n        \"\"\"\n        merged = existing.copy()\n        for k, v in new.items():\n            if k not in merged or merged[k] is None:\n                merged[k] = v\n            else:\n                if isinstance(merged[k], list) and isinstance(v, list):\n                    # Extend lists\n                    merged[k].extend(v)\n                # If there's a scalar conflict, we can keep the first non-null,\n                # or implement custom conflict handling here.\n                # For now, do nothing if both have a value, keep existing.\n        return merged\n\n    def _clean_merged_dict(self, merged: Dict[str, Any], response_model: type[BaseModel]) -&gt; Dict[str, Any]:\n        \"\"\"Clean the merged dictionary after conflict resolution to remove any leftover special structures \n        and ensure values are compatible with the response model.\"\"\"\n        cleaned = {}\n        for field_name, field_value in merged.items():\n            # If there's still a conflict structure, remove it or handle it\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                # If somehow unresolved (shouldn't happen), just pick one candidate or None\n                candidates = field_value.get(\"candidates\", [])\n                cleaned[field_name] = candidates[0] if candidates else None\n            else:\n                cleaned[field_name] = field_value\n\n        # Pydantic will do additional type coercion upon instantiation\n        return cleaned\n\n    def _has_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel]) -&gt; bool:\n        \"\"\"Check if result dictionary has any conflicting fields.\"\"\"\n        for field_name, field_value in result_dict.items():\n            if field_name not in response_model.model_fields:\n                continue\n            field_type = response_model.model_fields[field_name].annotation\n\n            # Check for special conflict dictionary (scalar conflict)\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                return True\n\n            # Check list field duplicates (if needed)\n            # Here you could implement more robust checks if required.\n        return False\n\n    def _identify_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel]) -&gt; Dict[str, Any]:\n        \"\"\"Identify conflicting fields in the result dictionary.\"\"\"\n        conflicts = {}\n        for field_name, field_value in result_dict.items():\n            if field_name not in response_model.model_fields:\n                continue\n            field_type = response_model.model_fields[field_name].annotation\n\n            # Scalar conflict\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                conflicts[field_name] = field_value[\"candidates\"]\n            # Could add checks for list conflicts if needed.\n        return conflicts\n\n    def _resolve_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel],\n                           pages_data: List[Any], field_values: Dict[str, List[Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Resolve conflicts in the dictionary using the LLM.\"\"\"\n        conflicts = self._identify_conflicts(result_dict, response_model)\n\n        if not conflicts:\n            return result_dict\n\n        resolved = self._request_conflict_resolution(conflicts, pages_data, field_values)\n        return self._merge_resolved_conflicts(result_dict, resolved, response_model)\n\n    def _request_conflict_resolution(\n        self,\n        conflicts: Dict[str, List[Any]],\n        pages_data: List[Any],\n        field_values: Dict[str, List[Any]]\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Request LLM to resolve conflicts.\"\"\"\n        message_content = self._build_conflict_resolution_prompt(conflicts, pages_data, field_values)\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a server API that resolves field conflicts. You have access to the original page contents and the conflicting values extracted from them.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": message_content\n            }\n        ]\n\n        try:\n            response: ConflictResolution = self.llm.request(messages, ConflictResolution)\n            return response.resolved_fields\n        except Exception as e:\n            raise ValueError(f\"Failed to resolve conflicts: {str(e)}. This may happen if the context was too big or the LLM couldn't resolve the conflicts.\")\n\n    def _build_conflict_resolution_prompt(\n        self,\n        conflicts: Dict[str, List[Any]],\n        pages_data: List[Any],\n        field_values: Dict[str, List[Any]]\n    ) -&gt; Union[str, List[Dict[str, Any]]]:\n        \"\"\"Build prompt for conflict resolution with context from all pages.\"\"\"\n        # Check if any page has vision content\n        has_vision_content = any(\n            isinstance(page_content, dict) and ('image' in page_content or 'images' in page_content)\n            for page_content, _ in pages_data\n        )\n\n        if has_vision_content:\n            # Build vision-compatible message content\n            message_content = []\n\n            # Add initial text content\n            intro_text = [\n                \"Please resolve conflicts in these fields by choosing the correct value and providing a confidence score (1-10).\",\n                \"You have the contents of each page that contributed data, and the conflicting values they produced.\",\n                \"Return JSON in this format:\\n{\\n  \\\"resolved_fields\\\": {\\n    \\\"field_name\\\": {\\\"value\\\": \\\"chosen_value\\\", \\\"confidence\\\": 9}\\n  }\\n}\\n\",\n                \"Conflicts to resolve:\",\n                str(conflicts),\n                \"\\nHere are the original pages and their extracted values:\"\n            ]\n\n            message_content.append({\n                \"type\": \"text\",\n                \"text\": \"\\n\".join(intro_text)\n            })\n\n            # Add each page's content and images\n            for i, (page_content, page_result) in enumerate(pages_data):\n                page_text = [f\"\\n--- Page {i+1} ---\", \"Original page content:\"]\n\n                if isinstance(page_content, dict):\n                    # Add text content if available\n                    content_data = self._process_content_data(page_content)\n                    if content_data:\n                        page_text.append(content_data)\n\n                    # Add extracted values\n                    page_text.append(\"Extracted values for all fields on this page:\")\n                    for field_name, values_for_field in field_values.items():\n                        page_value = values_for_field[i] if i &lt; len(values_for_field) else None\n                        page_text.append(f\"{field_name}: {page_value}\")\n\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": \"\\n\".join(page_text)\n                    })\n\n                    # Add images if present\n                    if 'image' in page_content:\n                        message_content.append({\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{encode_image(page_content['image'])}\"\n                            }\n                        })\n                    elif 'images' in page_content:\n                        for img in page_content['images']:\n                            message_content.append({\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                                }\n                            })\n\n            return message_content\n        else:\n            # Build regular text prompt\n            prompt_parts = []\n            prompt_parts.extend([\n                \"Please resolve conflicts in these fields by choosing the correct value and providing a confidence score (1-10).\",\n                \"You have the contents of each page that contributed data, and the conflicting values they produced.\",\n                \"Return JSON in this format:\\n{\\n  \\\"resolved_fields\\\": {\\n    \\\"field_name\\\": {\\\"value\\\": \\\"chosen_value\\\", \\\"confidence\\\": 9}\\n  }\\n}\\n\",\n                \"Conflicts to resolve:\",\n                str(conflicts),\n                \"\\nHere are the original pages and their extracted values:\"\n            ])\n\n            for i, (page_content, page_result) in enumerate(pages_data):\n                prompt_parts.extend([\n                    f\"\\n--- Page {i+1} ---\",\n                    \"Original page content:\",\n                    yaml.dump(page_content) if isinstance(page_content, dict) else str(page_content),\n                    \"Extracted values for all fields on this page:\"\n                ])\n\n                for field_name, values_for_field in field_values.items():\n                    page_value = values_for_field[i] if i &lt; len(values_for_field) else None\n                    prompt_parts.append(f\"{field_name}: {page_value}\")\n\n            return \"\\n\".join(prompt_parts)\n\n    def _process_content_data(self, content: Union[Dict[str, Any], List[Any], str]) -&gt; Optional[str]:\n        \"\"\"Process content data by filtering out images and converting to a string.\"\"\"\n        if isinstance(content, dict):\n            filtered_content = {\n                k: v for k, v in content.items()\n                if k not in ('images', 'image') and not hasattr(v, 'read')\n            }\n            if filtered_content.get(\"is_spreadsheet\", False):\n                content_str = json_to_formatted_string(filtered_content.get(\"data\", {}))\n            else:\n                content_str = yaml.dump(filtered_content, default_flow_style=True)\n            return content_str\n        return None\n\n    def _merge_resolved_conflicts(\n        self,\n        original: Dict[str, Any],\n        resolved: Dict[str, Dict[str, Any]],\n        response_model: type[BaseModel]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merge resolved conflicts back into the dictionary.\"\"\"\n        result_dict = copy.deepcopy(original)\n\n        for field_name, resolution in resolved.items():\n            if field_name in result_dict:\n                result_dict[field_name] = resolution[\"value\"]\n\n        return result_dict\n\n    def _handle_partial_response(\n        self,\n        exception: IncompleteOutputException,\n        messages: List[Dict[str, Any]],\n        response_model: type[BaseModel]\n    ) -&gt; Any:\n        \"\"\"Handle partial response by continuing the request\"\"\"\n        partial_content = exception.last_completion.choices[0].message.content\n        continuation_messages = self._build_continuation_messages(messages, partial_content)\n\n        try:\n            return self.llm.request(continuation_messages, response_model)\n        except Exception as e:\n            raise ValueError(f\"Failed to complete partial response: {str(e)}\")\n\n    def _build_continuation_messages(\n        self,\n        messages: List[Dict[str, Any]],\n        partial_content: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for continuation request.\"\"\"\n        continuation_messages = copy.deepcopy(messages)\n\n        # Add partial response as assistant message\n        continuation_messages.append({\n            \"role\": \"assistant\",\n            \"content\": partial_content\n        })\n\n        # Add continuation prompt\n        continuation_messages.append({\n            \"role\": \"user\", \n            \"content\": \"## CONTINUE JSON\"\n        })\n\n        return continuation_messages\n\n    def _build_messages(self, content: Any, vision: bool) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for LLM request.\"\"\"\n        system_message = {\n            \"role\": \"system\",\n            \"content\": \"You are a server API that receives document information and returns specific fields in JSON format.\"\n        }\n\n        if vision:\n            message_content = self._build_vision_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n        else:\n            message_content = self._build_text_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n\n        return messages\n\n    def _build_vision_content(self, content: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build content for vision request.\"\"\"\n        message_content = []\n\n        # If there's textual 'content', push it first\n        if isinstance(content, dict) and \"content\" in content:\n            message_content.append({\n                \"type\": \"text\",\n                \"text\": f\"##Content\\n\\n{content['content']}\"\n            })\n\n        # Now handle multiple images\n        if isinstance(content, dict):\n            images = []\n            if \"images\" in content and isinstance(content[\"images\"], list):\n                images.extend(content[\"images\"])\n            if \"image\" in content and content[\"image\"] is not None:\n                images.append(content[\"image\"])\n\n            for img in images:\n                if img:\n                    message_content.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                        }\n                    })\n        return message_content\n\n    def _build_text_content(self, content: Any) -&gt; str:\n        \"\"\"Build content for text request.\"\"\"\n        if isinstance(content, dict):\n            return f\"##Content\\n\\n{yaml.dump(content)}\"\n        elif isinstance(content, str):\n            return f\"##Content\\n\\n{content}\"\n        else:\n            return f\"##Content\\n\\n{str(content)}\"\n\n    def _add_extra_content(self, messages: List[Dict[str, Any]], extra_content: str) -&gt; None:\n        \"\"\"Add extra content to messages.\"\"\"\n        messages.insert(1, {\n            \"role\": \"user\",\n            \"content\": f\"##Extra Content\\n\\n{extra_content}\"\n        })\n</code></pre></p>"},{"location":"core-concepts/completion-strategies/paginate/#when-to-use","title":"When to Use","text":"<p>PAGINATE is the best choice when:</p> <p>Context window is small</p> <ul> <li>For local LLMs with smaller context windows (e.g Llama 3.3 8k context window).</li> </ul> <p>The content is too Big</p> <ul> <li>When the file will not fit in the entire context window (e.g 500 page document)</li> </ul> <p>Model Accuracy</p> <ul> <li>Sometimes LLMs can lose focus when the context is too big, Paginate strategy will solve this problem</li> </ul>"},{"location":"core-concepts/contracts/","title":"Contracts","text":"<p>\ud83d\udea7 In Development</p> <p>This component is currently under active development. The API might change in future releases.</p> <p>Contracts in ExtractThinker are Pydantic models that define the structure of data you want to extract. They provide type safety and validation for your extracted data.</p> Base Contract Implementation <pre><code>from pydantic import BaseModel\n\n\nclass Contract(BaseModel):\n    pass\n</code></pre>"},{"location":"core-concepts/contracts/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Contract\nfrom typing import List, Optional\nfrom pydantic import Field\n\nclass InvoiceLineItem(Contract):\n    description: str = Field(description=\"Description of the item\")\n    quantity: int = Field(description=\"Quantity of items\")\n    unit_price: float = Field(description=\"Price per unit\")\n    amount: float = Field(description=\"Total amount for line\")\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(description=\"Invoice identifier\")\n    date: str = Field(description=\"Invoice date\")\n    total_amount: float = Field(description=\"Total invoice amount\")\n    line_items: List[InvoiceLineItem] = Field(description=\"List of items in invoice\")\n    notes: Optional[str] = Field(description=\"Additional notes\", default=None)\n</code></pre>"},{"location":"core-concepts/document-loaders/","title":"Document Loaders","text":"<p>Document Loaders are the foundation of ExtractThinker's document processing pipeline. They handle the initial loading and preprocessing of documents, converting them into a standardized format that can be used by other components.</p>"},{"location":"core-concepts/document-loaders/#basic-concept","title":"Basic Concept","text":"<p>A Document Loader can return content in two formats: - A simple string containing the extracted text - A structured object with pages and their content, that depends on the loader</p> Base Document Loader <p>The base DocumentLoader class defines the interface and common functionality that all loaders must implement: - <code>load_content_from_file</code>: Process files from disk - <code>load_content_from_stream</code>: Process BytesIO streams - <code>can_handle</code>: Validate file types - <code>convert_to_images</code>: Convert documents to images <pre><code>from abc import ABC, abstractmethod\nimport io\nfrom io import BytesIO\nfrom PIL import Image\nimport pypdfium2 as pdfium\nfrom typing import Any, Dict, Union, List\nfrom cachetools import TTLCache\nimport os\nimport magic\nfrom extract_thinker.utils import get_file_extension, check_mime_type\nfrom playwright.sync_api import sync_playwright\nfrom urllib.parse import urlparse\nimport base64\nimport math\n\nclass DocumentLoader(ABC):\n    # SUPPORTED_FORMATS = [\n    #     \"pdf\", \"jpg\", \"jpeg\", \"png\", \"tiff\", \"bmp\"\n    # ]\n\n    def __init__(self, content: Any = None, cache_ttl: int = 300, screenshot_timeout: int = 1000):\n        \"\"\"Initialize loader.\n\n        Args:\n            content: Initial content\n            cache_ttl: Cache time-to-live in seconds\n            screenshot_timeout: Timeout in milliseconds to wait for page content load when capturing a screenshot.\n        \"\"\"\n        self.content = content\n        self.file_path = None\n        self.cache = TTLCache(maxsize=100, ttl=cache_ttl)\n        self.vision_mode = False\n        self.max_image_size = None  # Changed to None by default\n        self.is_url = False  # Indicates if the source is a URL\n        self.screenshot_timeout = screenshot_timeout\n\n    def set_max_image_size(self, size: int) -&gt; None:\n        \"\"\"Set the maximum image size.\"\"\"\n        self.max_image_size = size\n\n    def set_vision_mode(self, enabled: bool = True) -&gt; None:\n        \"\"\"Enable or disable vision mode processing.\"\"\"\n        self.vision_mode = enabled\n\n    def set_screenshot_timeout(self, timeout: int) -&gt; None:\n        \"\"\"Set the screenshot timeout in milliseconds for capturing a screenshot from a URL.\"\"\"\n        self.screenshot_timeout = timeout\n\n    def can_handle(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the loader can handle the given source.\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            bool: True if the loader can handle the source, False otherwise\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                return self._can_handle_file_path(source)\n            elif isinstance(source, BytesIO):\n                return self._can_handle_stream(source)\n            return False\n        except Exception:\n            return False\n\n    def _can_handle_file_path(self, file_path: str) -&gt; bool:\n        \"\"\"Checks if the loader can handle the given file path.\"\"\"\n        if not os.path.isfile(file_path):\n            return False\n        file_type = get_file_extension(file_path)\n        return file_type.lower() in [fmt.lower() for fmt in self.SUPPORTED_FORMATS]\n\n    def _can_handle_stream(self, stream: BytesIO) -&gt; bool:\n        \"\"\"Checks if the loader can handle the given BytesIO stream.\"\"\"\n        try:\n            mime = magic.from_buffer(stream.getvalue(), mime=True)\n            stream.seek(0)  # Reset stream position\n            return check_mime_type(mime, self.SUPPORTED_FORMATS)\n        except Exception:\n            return False\n\n    @abstractmethod\n    def load(self, source: Union[str, BytesIO]) -&gt; Any:\n        \"\"\"Enhanced load method that handles vision mode.\"\"\"\n        pass\n\n    def getContent(self) -&gt; Any:\n        return self.content\n\n    def convert_to_images(self, file: Union[str, io.BytesIO, io.BufferedReader], scale: float = 300 / 72) -&gt; Dict[int, bytes]:\n        # Determine if the input is a file path or a stream\n        if isinstance(file, str):\n            return self._convert_file_to_images(file, scale)\n        elif isinstance(file, (io.BytesIO, io.BufferedReader)):  # Accept both BytesIO and BufferedReader\n            return self._convert_stream_to_images(file, scale)\n        else:\n            raise TypeError(\"file must be a file path (str) or a file-like stream\")\n\n    def _convert_file_to_images(self, file_path: str, scale: float) -&gt; Dict[int, bytes]:\n        \"\"\"Convert file to images, handling both URLs and local files.\"\"\"\n        # Check if it's a URL\n        if self._is_url(file_path):\n            self.is_url = True  # Set the instance variable if the source is a URL\n            try:\n                screenshot = self._capture_screenshot_from_url(file_path)\n                # Convert screenshot to PIL Image for potential resizing\n                img = Image.open(BytesIO(screenshot))\n                img = self._resize_if_needed(img)\n\n                # Split into vertical chunks\n                chunks = self._split_image_vertically(img)\n\n                # Return dictionary with chunks as list\n                return {0: chunks}  # All chunks from URL are considered \"page 0\"\n\n            except Exception as e:\n                raise ValueError(f\"Failed to capture screenshot from URL: {str(e)}\")\n\n        # Existing code for local files...\n        try:\n            Image.open(file_path)\n            is_image = True\n        except IOError:\n            is_image = False\n\n        if is_image:\n            with open(file_path, \"rb\") as f:\n                return {0: f.read()}\n\n        return self._convert_pdf_to_images(pdfium.PdfDocument(file_path), scale)\n\n    def _convert_stream_to_images(self, file_stream: io.BytesIO, scale: float) -&gt; Dict[int, bytes]:\n        # Check if the stream is already an image\n        try:\n            Image.open(file_stream)\n            is_image = True\n        except IOError:\n            is_image = False\n\n        # Reset stream position\n        file_stream.seek(0)\n\n        if is_image:\n            # If it is, return it as is\n            return {0: file_stream.read()}\n\n        # If it's not an image, proceed with the conversion\n        return self._convert_pdf_to_images(pdfium.PdfDocument(file_stream), scale)\n\n    def _resize_if_needed(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"Resize image if it exceeds maximum dimensions while maintaining aspect ratio.\n\n        Args:\n            image: PIL Image object\n\n        Returns:\n            PIL Image object (resized if necessary)\n        \"\"\"\n        if self.max_image_size is None:  # Skip resizing if max_image_size not set\n            return image\n\n        width, height = image.size\n        if width &gt; self.max_image_size or height &gt; self.max_image_size:\n            # Calculate scaling factor to fit within max dimensions\n            scale = self.max_image_size / max(width, height)\n            new_width = int(width * scale)\n            new_height = int(height * scale)\n            return image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n        return image\n\n    def _convert_pdf_to_images(self, pdf_file, scale: float) -&gt; Dict[int, bytes]:\n        # Get all pages at once\n        renderer = pdf_file.render(\n            pdfium.PdfBitmap.to_pil,\n            page_indices=list(range(len(pdf_file))),\n            scale=scale,\n        )\n\n        # Convert all images to bytes and store in dictionary\n        final_images = {}\n        for page_index, image in enumerate(renderer):\n            # Resize image if needed\n            image = self._resize_if_needed(image)\n            image_byte_array = BytesIO()\n            image.save(image_byte_array, format=\"jpeg\", optimize=True)\n            final_images[page_index] = image_byte_array.getvalue()\n\n        return final_images\n\n    def can_handle_vision(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the loader can handle the source in vision mode.\n\n        Args:\n            source: Either a file path (str), URL, or a BytesIO stream\n\n        Returns:\n            bool: True if the loader can handle the source in vision mode\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                if self._is_url(source):\n                    return True  # URLs are always supported in vision mode\n                ext = get_file_extension(source).lower()\n                return ext in ['pdf', 'jpg', 'jpeg', 'png', 'tiff', 'bmp']\n            elif isinstance(source, BytesIO):\n                try:\n                    Image.open(source)\n                    return True\n                except:\n                    # Try to load as PDF\n                    try:\n                        pdfium.PdfDocument(source)\n                        return True\n                    except:\n                        return False\n            return False\n        except Exception:\n            return False\n\n    def can_handle_paginate(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the source supports pagination (e.g., PDF, PPT).\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            bool: True if the source supports pagination\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                # For file paths, check the extension\n                ext = get_file_extension(source).lower()\n            else:\n                # For BytesIO streams, use magic to detect mime type\n                mime = magic.from_buffer(source.getvalue(), mime=True)\n                source.seek(0)  # Reset stream position\n                return mime == 'application/pdf'\n\n            # List of extensions that support pagination\n            return ext in ['pdf']\n        except Exception:\n            return False\n\n    @staticmethod\n    def _check_playwright_dependencies():\n        \"\"\"\n        Check if the playwright dependency is installed.\n        Raises:\n            ImportError: If playwright is not installed.\n        \"\"\"\n        try:\n            from playwright.sync_api import sync_playwright\n        except ImportError:\n            raise ImportError(\n                \"You are using vision with url. You need to install playwright.\"\n                \"`pip install playwright` and run `playwright install`.\"\n            )\n\n    def _capture_screenshot_from_url(self, url: str) -&gt; bytes:\n        \"\"\"\n        Captures a full-page screenshot of a URL using Playwright.\n\n        Args:\n            url: The URL to capture\n\n        Returns:\n            bytes: The screenshot image data\n        \"\"\"\n        # Optional: Check if playwright is installed before attempting to use it.\n        self._check_playwright_dependencies()\n\n        from playwright.sync_api import sync_playwright  # Import after the dependency check\n\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            page = browser.new_page()\n\n            try:\n                # Navigate to URL\n                page.goto(url, wait_until='networkidle')\n\n                # Optional: Handle cookie consent popups (customize selectors as needed)\n                try:\n                    page.click('button:has-text(\"Accept\")', timeout=10000)\n                except Exception:\n                    pass  # Ignore if no cookie banner is found\n\n                # Wait for content to load with the configurable timeout\n                page.wait_for_timeout(self.screenshot_timeout)\n\n                # Capture full page screenshot\n                screenshot = page.screenshot(full_page=True)\n\n                return screenshot\n\n            finally:\n                browser.close()\n\n    def _split_image_vertically(self, img: Image.Image, chunk_height: int = 1000) -&gt; List[bytes]:\n        \"\"\"\n        Splits a tall PIL Image into vertical chunks of `chunk_height`.\n        Returns a list of bytes in PNG format, in top-to-bottom order.\n\n        Args:\n            img: PIL Image to split\n            chunk_height: Height of each chunk in pixels\n\n        Returns:\n            List of PNG-encoded bytes for each chunk\n        \"\"\"\n        width, height = img.size\n        num_chunks = math.ceil(height / chunk_height)\n\n        chunks_bytes = []\n        for i in range(num_chunks):\n            top = i * chunk_height\n            bottom = min((i + 1) * chunk_height, height)\n            crop_box = (0, top, width, bottom)\n\n            # Crop the chunk\n            chunk_img = img.crop(crop_box)\n\n            # Convert chunk to bytes\n            chunk_bytes = io.BytesIO()\n            chunk_img.save(chunk_bytes, format=\"PNG\", optimize=True)\n            chunk_bytes.seek(0)\n            chunks_bytes.append(chunk_bytes.read())\n\n        return chunks_bytes\n\n    def _is_url(self, source: str) -&gt; bool:\n        \"\"\"Check if the source string is a URL.\"\"\"\n        try:\n            result = urlparse(source)\n            return bool(result.scheme and result.netloc)\n        except:\n            return False\n</code></pre></p>"},{"location":"core-concepts/document-loaders/#core-features","title":"Core Features","text":""},{"location":"core-concepts/document-loaders/#configuration-support","title":"Configuration Support","text":"<p>All Document Loaders support configuration-based initialization through dedicated config classes:</p> <pre><code>from extract_thinker import DocumentLoaderAWSTextract, TextractConfig\n\n# Create configuration\nconfig = TextractConfig(\n    aws_access_key_id=\"your_key\",\n    feature_types=[\"TABLES\", \"FORMS\"],\n    cache_ttl=600\n)\n\n# Initialize with configuration\nloader = DocumentLoaderAWSTextract(config)\n</code></pre>"},{"location":"core-concepts/document-loaders/#caching","title":"Caching","text":"<p>All Document Loaders include built-in caching capabilities through the <code>CachedDocumentLoader</code> base class. This provides automatic caching of document processing results with a configurable TTL:</p> Cached Document Loader <p>The CachedDocumentLoader extends the base loader with caching capabilities: <pre><code>from io import BytesIO\nfrom typing import Any, Union\nfrom cachetools import TTLCache\nfrom extract_thinker.document_loader.document_loader import DocumentLoader\n\n\nclass CachedDocumentLoader(DocumentLoader):\n    def __init__(self, content: Any = None, cache_ttl: int = 300):\n        super().__init__(content)\n        self.cache = TTLCache(maxsize=100, ttl=cache_ttl)\n\n    def load(self, source: Union[str, BytesIO]) -&gt; Any:\n        \"\"\"\n        Load content from source with caching support.\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            The loaded content\n        \"\"\"\n        # Use the source and vision_mode state as the cache key\n        if isinstance(source, str):\n            cache_key = (source, self.vision_mode)\n        else:\n            # For BytesIO, use the content and vision_mode state as the cache key\n            cache_key = (source.getvalue(), self.vision_mode)\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        result = super().load(source)\n        self.cache[cache_key] = result\n        return result\n</code></pre></p> <p>Example usage of caching: <pre><code>from extract_thinker.document_loader import DocumentLoader\n\nclass MyCustomLoader(DocumentLoader):\n    def __init__(self, content: Any = None, cache_ttl: int = 300):\n        super().__init__(content, cache_ttl)  # 300 seconds default TTL\n</code></pre></p>"},{"location":"core-concepts/document-loaders/#file-type-support","title":"File Type Support","text":"<p>Document Loaders automatically validate file types through the <code>can_handle</code> method:</p> <pre><code>loader = MyCustomLoader()\nif loader.can_handle(\"document.pdf\"):\n    content = loader.load(\"document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/#multiple-input-types","title":"Multiple Input Types","text":"<p>Loaders support both file paths and BytesIO streams:</p> <pre><code># Load from file\ncontent = loader.load(\"document.pdf\")\n\n# Load from stream\nwith open(\"document.pdf\", \"rb\") as f:\n    stream = BytesIO(f.read())\n    content = loader.load(stream)\n</code></pre>"},{"location":"core-concepts/document-loaders/#vision-mode-support","title":"Vision Mode Support","text":"<p>Many loaders support vision mode for handling images and visual content:</p> <pre><code># Enable vision mode\nloader.set_vision_mode(True)\n\n# Load document with images\npages = loader.load(\"document.pdf\")\nfor page in pages:\n    text = page[\"content\"]\n    image = page.get(\"image\")  # Available in vision mode\n</code></pre>"},{"location":"core-concepts/document-loaders/#image-resizing","title":"Image Resizing","text":"<pre><code>loader = DocumentLoader()\nloader.set_max_image_size(2000)\n</code></pre>"},{"location":"core-concepts/document-loaders/#image-conversion","title":"Image Conversion","text":"<p>The base loader includes utilities for converting documents to images:</p> <pre><code>loader = DocumentLoader()\nimages = loader.convert_to_images(\n    \"document.pdf\",\n    scale=300/72  # DPI scaling\n)\n</code></pre>"},{"location":"core-concepts/document-loaders/#common-methods","title":"Common Methods","text":"<p>All Document Loaders implement these core methods:</p> <ul> <li><code>load(source)</code>: Main entry point for loading documents</li> <li><code>set_vision_mode(enabled)</code>: Enable/disable vision mode</li> <li><code>set_max_image_size(size)</code>: Set the maximum image size</li> </ul>"},{"location":"core-concepts/document-loaders/#best-practices","title":"Best Practices","text":"<ul> <li>Use configuration classes for complex initialization</li> <li>Set appropriate cache TTL based on your use case</li> <li>Check file type support before processing</li> <li>Consider memory usage when processing large files</li> <li>Enable vision mode only when needed</li> <li>Handle both file paths and streams for flexibility</li> </ul>"},{"location":"core-concepts/document-loaders/#available-loaders","title":"Available Loaders","text":"<p>ExtractThinker provides several specialized Document Loaders:</p>"},{"location":"core-concepts/document-loaders/#cloud-services","title":"Cloud Services","text":"<ul> <li>AWS Textract: AWS document processing with support for text, tables, forms, and layout analysis</li> <li>Azure Form: Azure's Document Intelligence with multiple model support</li> <li>Google Document AI: Google's document understanding with native PDF parsing</li> </ul>"},{"location":"core-concepts/document-loaders/#local-processing","title":"Local Processing","text":"<ul> <li>PDF Plumber: Advanced PDF text and table extraction</li> <li>PyPDF: Basic PDF processing with password protection support</li> <li>Tesseract: Open-source OCR with multiple language support</li> <li>Doc2txt: Microsoft Word document processing</li> <li>Spreadsheet: Excel and CSV handling</li> <li>Text File: Plain text file handling with encoding support</li> <li>Markitdown: Multi-format document processing</li> <li>Docling: Advanced document layout and table analysis</li> </ul>"},{"location":"core-concepts/document-loaders/#special-purpose","title":"Special Purpose","text":"<ul> <li>Web Loader: Web page extraction with custom element handling</li> <li>LLM Image: Vision-enabled LLM processing</li> <li>Data: Pre-processed data handling with standardized format support</li> </ul>"},{"location":"core-concepts/document-loaders/#coming-soon","title":"Coming Soon","text":"<ul> <li><code>Adobe PDF Services</code> Coming Soon: Adobe's PDF extraction and analysis</li> <li><code>ABBYY FineReader</code> Coming Soon: Enterprise-grade OCR solution</li> <li><code>PaddleOCR</code> Coming Soon: High-performance multilingual OCR</li> <li><code>Unstructured</code> Coming Soon: Open-source document preprocessing</li> <li><code>Mathpix</code> Coming Soon: Math and scientific document processing</li> <li><code>EasyOCR</code> Coming Soon: Ready-to-use OCR with multilingual support</li> <li><code>Nanonets</code> Coming Soon: API-based document processing</li> <li><code>Mindee</code> Coming Soon: Specialized document parsing APIs</li> <li><code>Rossum</code> Coming Soon: AI-powered document understanding</li> <li><code>Kofax</code> Coming Soon: Intelligent document processing</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/","title":"AWS Textract Document Loader","text":"<p>The AWS Textract loader uses Amazon's Textract service to extract text, forms, and tables from documents. It supports both image files and PDFs.</p>"},{"location":"core-concepts/document-loaders/aws-textract/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> <li>jpeg</li> <li>png</li> <li>tiff</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/aws-textract/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAWSTextract\n\n# Initialize with AWS credentials\nloader = DocumentLoaderAWSTextract(\n    aws_access_key_id=\"your_access_key\",\n    aws_secret_access_key=\"your_secret_key\",\n    region_name=\"your_region\"\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if extracted\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/aws-textract/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAWSTextract, TextractConfig\n\n# Create configuration\nconfig = TextractConfig(\n    aws_access_key_id=\"your_access_key\",\n    aws_secret_access_key=\"your_secret_key\",\n    region_name=\"your_region\",\n    feature_types=[\"TABLES\", \"FORMS\", \"SIGNATURES\"],  # Specify features to extract\n    cache_ttl=600,                                    # Cache results for 10 minutes\n    max_retries=3                                     # Number of retry attempts\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderAWSTextract(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/aws-textract/#configuration-options","title":"Configuration Options","text":"<p>The <code>TextractConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>aws_access_key_id</code> str None AWS access key ID <code>aws_secret_access_key</code> str None AWS secret access key <code>region_name</code> str None AWS region name <code>textract_client</code> boto3.client None Pre-configured Textract client <code>feature_types</code> List[str] [] Features to extract (TABLES, FORMS, LAYOUT, SIGNATURES) <code>max_retries</code> int 3 Maximum number of retry attempts"},{"location":"core-concepts/document-loaders/aws-textract/#features","title":"Features","text":"<ul> <li>Text extraction from images and PDFs</li> <li>Table detection and extraction</li> <li>Form field detection</li> <li>Layout analysis</li> <li>Signature detection</li> <li>Configurable feature selection</li> <li>Automatic retry on failure</li> <li>Caching support</li> <li>Support for pre-configured clients</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/#notes","title":"Notes","text":"<ul> <li>Raw text extraction is the default when no feature types are specified</li> <li>\"QUERIES\" feature type is not supported</li> <li>Vision mode is supported for image formats</li> <li>AWS credentials are required unless using a pre-configured client</li> <li>Rate limits and quotas apply based on your AWS account </li> </ul>"},{"location":"core-concepts/document-loaders/azure-form/","title":"Azure Document Intelligence Loader","text":"<p>The Azure Document Intelligence loader (formerly Form Recognizer) uses Azure's Document Intelligence service to extract text, tables, and structured information from documents.</p>"},{"location":"core-concepts/document-loaders/azure-form/#supported-formats","title":"Supported Formats","text":"<p>Supports <code>PDF</code>, <code>JPEG/JPG</code>, <code>PNG</code>, <code>BMP</code>, <code>TIFF</code>, <code>HEIF</code>, <code>DOCX</code>, <code>XLSX</code>, <code>PPTX</code> and <code>HTML</code>.</p>"},{"location":"core-concepts/document-loaders/azure-form/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/azure-form/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAzureForm\n\n# Initialize with Azure credentials\nloader = DocumentLoaderAzureForm(\n    subscription_key=\"your_subscription_key\",\n    endpoint=\"your_endpoint\",\n    model_id=\"prebuilt-document\"  # Use prebuilt document model\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n    # Access form fields if available\n    forms = page.get(\"forms\", {})\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAzureForm, AzureConfig\n\n# Create configuration\nconfig = AzureConfig(\n    subscription_key=\"your_subscription_key\",\n    endpoint=\"your_endpoint\",\n    model_id=\"prebuilt-layout\",     # Use layout model for enhanced layout analysis\n    cache_ttl=600,                  # Cache results for 10 minutes\n    features=[\"ocrHighResolution\", \"barcodes\"]  # Enable advanced features\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderAzureForm(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#advanced-features-usage","title":"Advanced Features Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAzureForm, AzureConfig\n\n# Configuration with multiple advanced features\nconfig = AzureConfig(\n    subscription_key=\"your_subscription_key\",\n    endpoint=\"your_endpoint\",\n    model_id=\"prebuilt-layout\",\n    features=[\n        \"ocrHighResolution\",    # High resolution OCR for small text\n        \"formulas\",             # Extract mathematical formulas in LaTeX\n        \"styleFont\",            # Extract font properties\n        \"barcodes\",             # Extract barcodes and QR codes\n        \"languages\",            # Detect document languages\n        \"keyValuePairs\"         # Extract key-value pairs from forms\n    ]\n)\n\nloader = DocumentLoaderAzureForm(config)\npages = loader.load(\"document_with_advanced_content.pdf\")\n\nfor page in pages:\n    # Standard content\n    print(f\"Text content: {page['content']}\")\n    print(f\"Tables: {page['tables']}\")\n    print(f\"Forms: {page['forms']}\")\n\n    # Advanced features (if detected in document)\n    if 'formulas' in page:\n        print(f\"Mathematical formulas: {page['formulas']}\")\n\n    if 'fonts' in page:\n        print(f\"Font information: {page['fonts']}\")\n\n    if 'barcodes' in page:\n        print(f\"Barcodes found: {page['barcodes']}\")\n\n    if 'languages' in page:\n        print(f\"Detected languages: {page['languages']}\")\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#specialized-models-usage","title":"Specialized Models Usage","text":"<pre><code># Use specialized invoice model\nconfig = AzureConfig(\n    subscription_key=\"your_subscription_key\",\n    endpoint=\"your_endpoint\",\n    model_id=\"prebuilt-invoice\"\n)\n\nloader = DocumentLoaderAzureForm(config)\npages = loader.load(\"invoice.pdf\")\n\n# Access extracted invoice fields\nfor page in pages:\n    forms = page[\"forms\"]\n    vendor_name = forms.get(\"VendorName\", \"\")\n    invoice_total = forms.get(\"InvoiceTotal\", \"\")\n    print(f\"Vendor: {vendor_name}, Total: {invoice_total}\")\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#configuration-options","title":"Configuration Options","text":"<p>The <code>AzureConfig</code> class supports the following options:</p> Option Type Default Description <code>subscription_key</code> str Required Azure subscription key <code>endpoint</code> str Required Azure endpoint URL <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>model_id</code> str \"prebuilt-layout\" Model ID to use <code>max_retries</code> int 3 Maximum retries for failed requests <code>features</code> List[str] None Advanced features to enable"},{"location":"core-concepts/document-loaders/azure-form/#available-models","title":"Available Models","text":""},{"location":"core-concepts/document-loaders/azure-form/#general-purpose-models","title":"General Purpose Models","text":"Model ID Description Best For <code>prebuilt-read</code> OCR/Read model Text extraction from printed and handwritten documents <code>prebuilt-layout</code> Layout analysis Documents with tables, selection marks, and complex layouts <code>prebuilt-document</code> General document Key-value pairs, tables, and general document structure"},{"location":"core-concepts/document-loaders/azure-form/#specialized-models","title":"Specialized Models","text":"Model ID Description <code>prebuilt-invoice</code> Invoice processing <code>prebuilt-receipt</code> Receipt processing <code>prebuilt-idDocument</code> Identity documents <code>prebuilt-businessCard</code> Business cards <code>prebuilt-tax.us.w2</code> US W2 tax forms <code>prebuilt-tax.us.1040</code> US 1040 tax forms <code>prebuilt-contract</code> Contracts <code>prebuilt-healthInsurance</code> US health insurance cards <code>prebuilt-bankStatement</code> Bank statements <code>prebuilt-payStub</code> Pay stubs"},{"location":"core-concepts/document-loaders/azure-form/#advanced-features","title":"Advanced Features","text":"<p>The loader supports advanced extraction features that can be enabled via the <code>features</code> parameter:</p> Feature Description Output Field <code>ocrHighResolution</code> High resolution OCR for better small text recognition Enhanced text in <code>content</code> <code>formulas</code> Extract mathematical formulas in LaTeX format <code>formulas</code> array <code>styleFont</code> Extract font properties (family, style, weight, color) <code>fonts</code> array <code>barcodes</code> Extract barcodes and QR codes <code>barcodes</code> array <code>languages</code> Detect document languages <code>languages</code> array <code>keyValuePairs</code> Extract key-value pairs from forms Enhanced <code>forms</code> dict <code>queryFields</code> Enable custom field extraction Enhanced extraction <code>searchablePDF</code> Convert scanned PDFs to searchable format Enhanced OCR"},{"location":"core-concepts/document-loaders/azure-form/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Form field recognition with specialized models</li> <li>Advanced OCR with high resolution support</li> <li>Mathematical formula extraction (LaTeX format)</li> <li>Font property extraction</li> <li>Barcode and QR code detection</li> <li>Multi-language document support</li> <li>Caching support with configurable TTL</li> <li>Vision mode support for image formats</li> <li>Retry logic for robust processing</li> </ul>"},{"location":"core-concepts/document-loaders/azure-form/#notes","title":"Notes","text":"<ul> <li>Azure subscription key and endpoint are required</li> <li>Advanced features may increase processing time and costs</li> <li>Specialized models are optimized for specific document types</li> <li>Rate limits and quotas apply based on your Azure subscription</li> <li>Vision mode is supported for image formats</li> <li>High resolution OCR is recommended for documents with small text</li> <li>Formula extraction works best with clear mathematical notation</li> </ul>"},{"location":"core-concepts/document-loaders/data/","title":"Data Document Loader","text":"<p>The Data loader is a specialized loader that handles pre-processed data in a standardized format. It provides caching support and vision mode compatibility.</p>"},{"location":"core-concepts/document-loaders/data/#supported-format","title":"Supported Format","text":"<p>The loader expects data in the following standard format: <pre><code>[\n  {\n    \"content\": \"...some text...\",\n    \"image\": None or [] or bytes\n  }\n]\n</code></pre></p>"},{"location":"core-concepts/document-loaders/data/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/data/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderData\n\n# Initialize with default settings\nloader = DocumentLoaderData()\n\n# Load pre-formatted data\ndata = [{\"content\": \"Sample text\", \"image\": None}]\npages = loader.load(data)\n\n# Process content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access image data if present\n    image = page[\"image\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/data/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderData, DataLoaderConfig\n\n# Create configuration\nconfig = DataLoaderConfig(\n    content=None,                # Initial content\n    cache_ttl=600,              # Cache results for 10 minutes\n    supports_vision=True         # Enable vision support\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderData(config)\n\n# Load and process content\npages = loader.load(\"raw text content\")\n</code></pre>"},{"location":"core-concepts/document-loaders/data/#configuration-options","title":"Configuration Options","text":"<p>The <code>DataLoaderConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>supports_vision</code> bool True Whether vision mode is supported"},{"location":"core-concepts/document-loaders/doc2txt/","title":"Doc2txt Document Loader","text":"<p>The Doc2txt loader extracts text from Microsoft Word documents. It supports both legacy (.doc) and modern (.docx) file formats.</p>"},{"location":"core-concepts/document-loaders/doc2txt/#supported-formats","title":"Supported Formats","text":"<ul> <li>doc</li> <li>docx</li> </ul>"},{"location":"core-concepts/document-loaders/doc2txt/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/doc2txt/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDoc2txt\n\n# Initialize with default settings\nloader = DocumentLoaderDoc2txt()\n\n# Load document\npages = loader.load(\"path/to/your/document.docx\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/doc2txt/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDoc2txt, Doc2txtConfig\n\n# Create configuration\nconfig = Doc2txtConfig(\n    page_separator=\"\\n\\n---\\n\\n\",  # Custom page separator\n    preserve_whitespace=True,      # Preserve original whitespace\n    extract_images=True,           # Extract embedded images\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderDoc2txt(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.docx\")\n</code></pre>"},{"location":"core-concepts/document-loaders/doc2txt/#configuration-options","title":"Configuration Options","text":"<p>The <code>Doc2txtConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>page_separator</code> str \"\\n\\n\" Text to use as page separator <code>preserve_whitespace</code> bool False Whether to preserve whitespace <code>extract_images</code> bool False Whether to extract embedded images"},{"location":"core-concepts/document-loaders/doc2txt/#features","title":"Features","text":"<ul> <li>Text extraction from Word documents</li> <li>Support for both .doc and .docx</li> <li>Custom page separation</li> <li>Whitespace preservation</li> <li>Image extraction (optional)</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/doc2txt/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported</li> <li>Image extraction requires additional memory</li> <li>Local processing with no external dependencies</li> <li>May not preserve complex formatting</li> <li>Handles both legacy and modern Word formats</li> </ul>"},{"location":"core-concepts/document-loaders/docling/","title":"Docling Document Loader","text":"<p>The Docling loader is a specialized document processor that excels at handling complex document layouts and table structures. It provides advanced OCR capabilities and precise table detection.</p>"},{"location":"core-concepts/document-loaders/docling/#supported-formats","title":"Supported Formats","text":""},{"location":"core-concepts/document-loaders/docling/#documents","title":"Documents","text":"<ul> <li>pdf</li> <li>doc/docx</li> <li>ppt/pptx</li> <li>xls/xlsx</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#images","title":"Images","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>bmp</li> <li>gif</li> <li>webp</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#text","title":"Text","text":"<ul> <li>txt</li> <li>html</li> <li>xml</li> <li>json</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#others","title":"Others","text":"<ul> <li>csv</li> <li>tsv</li> <li>zip</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/docling/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDocling\n\n# Initialize with default settings\nloader = DocumentLoaderDocling()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/docling/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDocling, DoclingConfig\n\n# Create configuration\nconfig = DoclingConfig(\n    ocr_enabled=True,                # Enable OCR processing\n    table_structure_enabled=True,    # Enable table structure detection\n    tesseract_cmd=\"path/to/tesseract\", # Custom Tesseract path\n    force_full_page_ocr=False,      # Use selective OCR\n    do_cell_matching=True,          # Enable cell content matching\n    format_options={                # Format-specific options\n        \"pdf\": {\"dpi\": 300},\n        \"image\": {\"enhance\": True}\n    },\n    cache_ttl=600                   # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderDocling(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/docling/#configuration-options","title":"Configuration Options","text":"<p>The <code>DoclingConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>ocr_enabled</code> bool False Enable OCR processing <code>table_structure_enabled</code> bool True Enable table structure detection <code>tesseract_cmd</code> str None Path to Tesseract executable <code>force_full_page_ocr</code> bool False Force OCR on entire page <code>do_cell_matching</code> bool True Enable cell content matching <code>format_options</code> Dict None Format-specific processing options"},{"location":"core-concepts/document-loaders/docling/#features","title":"Features","text":"<ul> <li>Advanced table structure detection</li> <li>Selective OCR processing</li> <li>Cell content matching</li> <li>Format-specific optimizations</li> <li>Custom Tesseract integration</li> <li>Table content deduplication</li> <li>Multi-format support</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>OCR requires Tesseract installation</li> <li>Table detection works best with structured documents</li> <li>Performance depends on document complexity</li> <li>Handles both scanned and digital documents</li> <li>Supports multiple document formats through format-specific optimizations </li> </ul>"},{"location":"core-concepts/document-loaders/easy_ocr/","title":"EasyOCR Document Loader","text":"<p>The EasyOCR loader uses the EasyOCR engine to extract text from images and PDF files. It's known for its ease of use and support for a wide range of languages.</p>"},{"location":"core-concepts/document-loaders/easy_ocr/#supported-formats","title":"Supported Formats","text":"<ul> <li>png</li> <li>jpg/jpeg</li> <li>tiff/tif</li> <li>webp</li> <li>pdf</li> </ul>"},{"location":"core-concepts/document-loaders/easy_ocr/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/easy_ocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderEasyOCR, EasyOCRConfig\n\n# Initialize with default settings\nconfig = EasyOCRConfig()\nloader = DocumentLoaderEasyOCR(config)\n\n# Load document\npages = loader.load(\"path/to/your/image.png\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    print(text)\n</code></pre>"},{"location":"core-concepts/document-loaders/easy_ocr/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderEasyOCR, EasyOCRConfig\n\n# Create configuration\nconfig = EasyOCRConfig(\n    lang_list=['en', 'fr'],        # Use English and French\n    gpu=True,                      # Enable GPU acceleration\n    cache_ttl=600,                 # Cache results for 10 minutes\n    include_bbox=True              # Include bounding box details\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderEasyOCR(config)\n\n# Load a PDF document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content with details\nfor page in pages:\n    print(f\"Content: {page['content']}\")\n    if 'detail' in page:\n        for detail in page['detail']:\n            print(f\"  - Text: {detail['text']}, BBox: {detail['bbox']}\")\n</code></pre>"},{"location":"core-concepts/document-loaders/easy_ocr/#configuration-options","title":"Configuration Options","text":"<p>The <code>EasyOCRConfig</code> class supports the following options:</p> Option Type Default Description <code>lang_list</code> List[str] <code>['en']</code> List of language codes for OCR (e.g., <code>['en', 'es']</code>). <code>gpu</code> bool <code>True</code> Whether to use GPU for processing (if available). <code>download_enabled</code> bool <code>True</code> Automatically download language models if not found. <code>cache_ttl</code> int <code>300</code> Time-to-live for cached results, in seconds. <code>include_bbox</code> bool <code>False</code> Whether to include detailed bounding box information in the output."},{"location":"core-concepts/document-loaders/easy_ocr/#features","title":"Features","text":"<ul> <li>Text extraction from images and PDFs</li> <li>Multi-language support with automatic model downloading</li> <li>Optional inclusion of detailed bounding box data</li> <li>GPU acceleration for faster processing</li> <li>Caching support to improve performance for repeated requests</li> <li>Local processing with no external API calls</li> </ul>"},{"location":"core-concepts/document-loaders/easy_ocr/#installation","title":"Installation","text":"<p>EasyOCR requires the <code>easyocr</code> and <code>torch</code> libraries. You can install them using pip:</p> <p><pre><code>pip install easyocr torch\n</code></pre> For GPU support, you may need to install a specific version of PyTorch that matches your CUDA version. Please refer to the official PyTorch installation guide for more details.</p>"},{"location":"core-concepts/document-loaders/easy_ocr/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported by this loader.</li> <li>Performance is significantly better when a GPU is available.</li> <li>The first time a language is used, the corresponding model will be downloaded, which may take some time. </li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/","title":"Google Document AI Loader","text":"<p>The Google Document AI loader uses Google Cloud's Document AI service to extract text, tables, and structured information from documents.</p>"},{"location":"core-concepts/document-loaders/google-document-ai/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>gif</li> <li>bmp</li> <li>webp</li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/google-document-ai/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderGoogleDocumentAI\n\n# Initialize with Google Cloud credentials\nloader = DocumentLoaderGoogleDocumentAI(\n    project_id=\"your_project_id\",\n    location=\"your_location\",\n    processor_id=\"your_processor_id\",\n    credentials_path=\"path/to/credentials.json\"\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/google-document-ai/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderGoogleDocumentAI, GoogleDocAIConfig\n\n# Create configuration\nconfig = GoogleDocAIConfig(\n    project_id=\"your_project_id\",\n    location=\"your_location\",\n    processor_id=\"your_processor_id\",\n    credentials_path=\"path/to/credentials.json\",\n    mime_type=\"application/pdf\",    # Specify MIME type\n    process_options={               # Additional processing options\n        \"ocr_config\": {\"enable_native_pdf_parsing\": True}\n    },\n    cache_ttl=600                   # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderGoogleDocumentAI(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/google-document-ai/#configuration-options","title":"Configuration Options","text":"<p>The <code>GoogleDocAIConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>project_id</code> str None Google Cloud project ID <code>location</code> str None Processing location <code>processor_id</code> str None Document AI processor ID <code>credentials_path</code> str None Path to credentials file <code>credentials</code> Credentials None Pre-configured credentials <code>mime_type</code> str None Document MIME type <code>process_options</code> Dict None Additional processing options"},{"location":"core-concepts/document-loaders/google-document-ai/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Form field recognition</li> <li>Multiple processor support</li> <li>Native PDF parsing</li> <li>Custom processing options</li> <li>Caching support</li> <li>Support for pre-configured credentials</li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>Google Cloud credentials are required</li> <li>Rate limits and quotas apply based on your Google Cloud account</li> <li>Different processors may support different document types</li> <li>Native PDF parsing can improve performance for PDF documents</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/","title":"LLM Image Document Loader","text":"<p>The LLM Image loader is a specialized loader designed to handle images and PDFs for vision-enabled Language Models. It serves as a fallback loader when no other loader is available and vision mode is required.</p>"},{"location":"core-concepts/document-loaders/llm-image/#supported-formats","title":"Supported Formats","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>gif</li> <li>bmp</li> <li>webp</li> <li>tiff</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/llm-image/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderLLMImage\n\n# Initialize with default settings\nloader = DocumentLoaderLLMImage()\n\n# Load document\npages = loader.load(\"path/to/your/image.jpg\")\n\n# Process extracted content\nfor page in pages:\n    # Access image content\n    image_bytes = page[\"image\"]\n    # Access metadata if available\n    metadata = page.get(\"metadata\", {})\n</code></pre>"},{"location":"core-concepts/document-loaders/llm-image/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderLLMImage, LLMImageConfig\n\n# Create configuration\nconfig = LLMImageConfig(\n    max_image_size=1024 * 1024,    # Maximum image size in bytes\n    image_format=\"jpeg\",           # Target image format\n    compression_quality=85,        # JPEG compression quality\n    llm=\"gpt-4-vision\",           # Target LLM model\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderLLMImage(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/image.jpg\")\n</code></pre>"},{"location":"core-concepts/document-loaders/llm-image/#configuration-options","title":"Configuration Options","text":"<p>The <code>LLMImageConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>llm</code> str None Target LLM model <code>max_image_size</code> int 1048576 Maximum image size in bytes <code>image_format</code> str \"jpeg\" Target image format <code>compression_quality</code> int 85 JPEG compression quality"},{"location":"core-concepts/document-loaders/llm-image/#features","title":"Features","text":"<ul> <li>Processing documents where text extraction is difficult or unreliable</li> <li>Working with image-heavy documents</li> <li>Using vision-enabled LLMs for document understanding</li> <li>Fallback option when other loaders fail</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/#notes","title":"Notes","text":"<ul> <li>This loader is specifically designed for vision/image processing</li> <li>It doesn't extract text content (content field will be empty)</li> <li>Each page will contain the image data in the 'image' field</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/","title":"MarkItDown Document Loader","text":"<p>MarkItDown is a versatile document processing library from Microsoft that can handle multiple file formats. The MarkItDown loader provides a robust interface for text extraction with optional vision mode support.</p>"},{"location":"core-concepts/document-loaders/markitdown/#supported-formats","title":"Supported Formats","text":""},{"location":"core-concepts/document-loaders/markitdown/#documents","title":"Documents","text":"<ul> <li>pdf</li> <li>doc/docx</li> <li>ppt/pptx</li> <li>xls/xlsx</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#text","title":"Text","text":"<ul> <li>txt</li> <li>html</li> <li>xml</li> <li>json</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#images","title":"Images","text":"<ul> <li>jpg/jpeg</li> <li>png</li> <li>bmp</li> <li>gif</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#audio","title":"Audio","text":"<ul> <li>wav</li> <li>mp3</li> <li>m4a</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#others","title":"Others","text":"<ul> <li>csv</li> <li>tsv</li> <li>zip</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/markitdown/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderMarkItDown\n\n# Initialize with default settings\nloader = DocumentLoaderMarkItDown()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/markitdown/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderMarkItDown, MarkItDownConfig\n\n# Create configuration\nconfig = MarkItDownConfig(\n    page_separator=\"---\",          # Custom page separator\n    preserve_whitespace=True,      # Preserve original whitespace\n    mime_type_detection=True,      # Enable MIME type detection\n    default_extension=\".md\",       # Default file extension\n    llm_client=\"gpt-4\",           # LLM client for enhanced parsing\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderMarkItDown(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.md\")\n</code></pre>"},{"location":"core-concepts/document-loaders/markitdown/#configuration-options","title":"Configuration Options","text":"<p>The <code>MarkItDownConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>page_separator</code> str \"\\n\\n\" Text to use as page separator <code>preserve_whitespace</code> bool False Whether to preserve whitespace <code>mime_type_detection</code> bool True Enable MIME type detection <code>default_extension</code> str \".txt\" Default file extension <code>llm_client</code> str None LLM client for enhanced parsing <code>llm_model</code> str None LLM model for enhanced parsing"},{"location":"core-concepts/document-loaders/markitdown/#features","title":"Features","text":"<ul> <li>Multi-format document processing</li> <li>Text and layout preservation</li> <li>MIME type detection</li> <li>Custom page separation</li> <li>Whitespace preservation</li> <li>LLM-enhanced parsing</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>LLM enhancement is optional</li> <li>Local processing with no external dependencies</li> <li>Preserves document structure</li> <li>Handles a wide variety of file formats</li> </ul>"},{"location":"core-concepts/document-loaders/mistral-ocr/","title":"Mistral OCR Document Loader","text":"<p>The Mistral OCR document loader leverages the Mistral OCR API to extract text and images from various document formats. It provides high-quality OCR capabilities with advanced machine learning models.</p>"},{"location":"core-concepts/document-loaders/mistral-ocr/#about-mistral-ocr","title":"About Mistral OCR","text":"<p>Mistral OCR is an industry-leading Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents\u2014media, text, tables, equations\u2014with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images format.</p> <p>Key capabilities: - State-of-the-art understanding of complex documents (tables, equations, layouts) - Natively multilingual support across thousands of scripts and languages - Superior performance on benchmarks - Fast processing (up to 2000 pages per minute on a single node) - Structured output in markdown format</p>"},{"location":"core-concepts/document-loaders/mistral-ocr/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Mistral OCR consistently outperforms other leading OCR models in benchmark tests:</p> Model Overall Math Multilingual Scanned Tables Google Document AI 83.42 80.29 86.42 92.77 78.16 Azure OCR 89.52 85.72 87.52 94.65 89.52 Gemini-1.5-Flash-002 90.23 89.11 86.76 94.87 90.48 Gemini-1.5-Pro-002 89.92 88.48 86.33 96.15 89.71 Gemini-2.0-Flash-001 88.69 84.18 85.80 95.11 91.46 GPT-4o-2024-11-20 89.77 87.55 86.00 94.58 91.70 Mistral OCR 2503 94.89 94.29 89.55 98.96 96.12"},{"location":"core-concepts/document-loaders/mistral-ocr/#multilingual-performance","title":"Multilingual Performance","text":"<p>Mistral OCR excels at processing documents in multiple languages:</p> Language Azure OCR Google Doc AI Gemini-2.0-Flash-001 Mistral OCR 2503 ru 97.35 95.56 96.58 99.09 fr 97.50 96.36 97.06 99.20 hi 96.45 95.65 94.99 97.55 zh 91.40 90.89 91.85 97.11 pt 97.96 96.24 97.25 99.42 de 98.39 97.09 97.19 99.51 es 98.54 97.52 97.75 99.54 tr 95.91 93.85 94.66 97.00 uk 97.81 96.24 96.70 99.29 it 98.31 97.69 97.68 99.42 ro 96.45 95.14 95.88 98.79"},{"location":"core-concepts/document-loaders/mistral-ocr/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF documents</li> <li>Image files:</li> <li>JPG/JPEG</li> <li>PNG</li> <li>TIFF</li> <li>BMP</li> </ul>"},{"location":"core-concepts/document-loaders/mistral-ocr/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/mistral-ocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderMistralOCR, MistralOCRConfig\n\n# Create configuration\nconfig = MistralOCRConfig(\n    api_key=\"your_mistral_api_key\",\n    model=\"mistral-ocr-latest\"\n)\n\n# Initialize loader\nloader = DocumentLoaderMistralOCR(config)\n\n# Load from URL\npages = loader.load(\"https://example.com/document.pdf\")\n\n# Load from file path\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content (in markdown format)\n    markdown_text = page[\"content\"]\n\n    # Access images if available\n    if \"images\" in page:\n        for image in page[\"images\"]:\n            image_id = image[\"id\"]\n            image_base64 = image[\"image_base64\"]  # If include_image_base64=True\n</code></pre>"},{"location":"core-concepts/document-loaders/mistral-ocr/#configuration-options","title":"Configuration Options","text":"<p>The <code>MistralOCRConfig</code> class supports the following options:</p> Option Type Default Description <code>api_key</code> str Required Mistral API key <code>model</code> str \"mistral-ocr-latest\" OCR model to use <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>include_image_base64</code> bool False Include image base64 in response <code>pages</code> List[int] None Specific pages to process (PDF only) <code>image_limit</code> int None Maximum number of images to extract <code>image_min_size</code> int None Minimum image size to extract"},{"location":"core-concepts/document-loaders/mistral-ocr/#features","title":"Features","text":"<ul> <li>High-quality OCR with Mistral AI's models</li> <li>Support for PDF and image formats</li> <li>Text extraction in markdown format</li> <li>Image extraction with positioning information</li> <li>Support for pagination in PDF documents</li> <li>Caching for improved performance</li> <li>URL, file path, and BytesIO input support</li> <li>Processing speed up to 2000 pages per minute</li> <li>Superior handling of complex elements like tables, math equations, and diagrams</li> <li>Native support for thousands of languages and scripts</li> </ul>"},{"location":"core-concepts/document-loaders/mistral-ocr/#how-it-works","title":"How It Works","text":"<p>When processing a document with the Mistral OCR loader:</p> <ol> <li>For URLs: The URL is sent directly to the Mistral OCR API</li> <li>For file paths or BytesIO objects:</li> <li>The file is first uploaded to Mistral's file storage system</li> <li>A signed URL is generated for the uploaded file</li> <li>The OCR API processes the document using the signed URL</li> </ol> <p>This approach follows Mistral's recommended workflow for document processing and complies with their API requirements.</p>"},{"location":"core-concepts/document-loaders/mistral-ocr/#common-use-cases","title":"Common Use Cases","text":"<p>Mistral OCR can be used for a variety of document processing tasks:</p> <ul> <li>Scientific research: Convert scientific papers with complex equations and diagrams into AI-ready formats</li> <li>Historical document preservation: Digitize historical documents and artifacts</li> <li>Customer service enhancement: Transform documentation and manuals into indexed knowledge</li> <li>Educational content processing: Extract information from lecture notes, presentations, and educational materials</li> <li>Legal document analysis: Process regulatory filings and legal documents with high accuracy</li> <li>Multilingual document handling: Process documents in multiple languages with superior accuracy</li> </ul>"},{"location":"core-concepts/document-loaders/mistral-ocr/#api-usage-notes","title":"API Usage Notes","text":"<ul> <li>The Mistral OCR API requires authentication with an API key</li> <li>API usage is subject to Mistral AI's terms and pricing (approximately 1000 pages / $)</li> <li>Response time depends on document size and complexity</li> <li>Extracted text is returned in markdown format</li> <li>Image positions and dimensions are provided for visual context</li> <li>Pagination is only supported for PDF documents</li> <li>Maximum document size: 50 MB</li> <li>Maximum page limit: 1,000 pages</li> <li>Local files are uploaded to Mistral's file storage with a purpose of \"ocr\"</li> </ul>"},{"location":"core-concepts/document-loaders/mistral-ocr/#requirements","title":"Requirements","text":"<ul> <li>An active Mistral AI API key</li> <li><code>requests</code> library for API communication</li> <li>Internet connectivity for API access </li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/","title":"PDFPlumber Document Loader","text":"<p>The PDFPlumber loader uses the pdfplumber library to extract text and tables from PDF documents with precise layout preservation.</p>"},{"location":"core-concepts/document-loaders/pdf-plumber/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/pdf-plumber/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPdfPlumber\n\n# Initialize with default settings\nloader = DocumentLoaderPdfPlumber()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if extracted\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/pdf-plumber/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPdfPlumber, PDFPlumberConfig\n\n# Create configuration\nconfig = PDFPlumberConfig(\n    table_settings={                # Custom table extraction settings\n        \"vertical_strategy\": \"text\",\n        \"horizontal_strategy\": \"lines\",\n        \"intersection_y_tolerance\": 10\n    },\n    vision_enabled=True,           # Enable vision mode for images\n    extract_tables=True,           # Enable table extraction\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderPdfPlumber(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/pdf-plumber/#configuration-options","title":"Configuration Options","text":"<p>The <code>PDFPlumberConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>table_settings</code> Dict None Custom table extraction settings <code>vision_enabled</code> bool False Enable vision mode for images <code>extract_tables</code> bool True Enable table extraction"},{"location":"core-concepts/document-loaders/pdf-plumber/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Custom table extraction settings</li> <li>Vision mode support</li> <li>Precise positioning information</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/#notes","title":"Notes","text":"<ul> <li>Vision mode can be enabled for image extraction</li> <li>Table extraction can be disabled for better performance</li> <li>Custom table settings can improve extraction accuracy</li> <li>Local processing with no external dependencies</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/","title":"PyPDF Document Loader","text":"<p>The PyPDF loader uses the PyPDF library to extract text and images from PDF documents. It provides basic text extraction and supports password-protected PDFs.</p>"},{"location":"core-concepts/document-loaders/pypdf/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/pypdf/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPyPdf\n\n# Initialize with default settings\nloader = DocumentLoaderPyPdf()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/pypdf/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPyPdf, PyPDFConfig\n\n# Create configuration\nconfig = PyPDFConfig(\n    password=\"your_password\",      # For password-protected PDFs\n    vision_enabled=True,           # Enable vision mode for images\n    extract_text=True,             # Enable text extraction\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderPyPdf(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/pypdf/#configuration-options","title":"Configuration Options","text":"<p>The <code>PyPDFConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>password</code> str None Password for protected PDFs <code>vision_enabled</code> bool False Enable vision mode for images <code>extract_text</code> bool True Enable text extraction"},{"location":"core-concepts/document-loaders/pypdf/#features","title":"Features","text":"<ul> <li>Basic text extraction</li> <li>Password-protected PDF support</li> <li>Image extraction (with vision mode)</li> <li>Caching support</li> <li>No cloud service required</li> <li>Lightweight and fast processing</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/#notes","title":"Notes","text":"<ul> <li>Vision mode can be enabled for image extraction</li> <li>Text extraction can be disabled for better performance</li> <li>Supports encrypted/password-protected PDFs</li> <li>Local processing with no external dependencies</li> <li>May not preserve complex layouts or tables</li> </ul>"},{"location":"core-concepts/document-loaders/spreadsheet/","title":"Spreadsheet Document Loader","text":"<p>The spreadsheet loader is designed to handle various spreadsheet formats including Excel files (xls, xlsx, xlsm, xlsb) and OpenDocument formats (odf, ods, odt).</p>"},{"location":"core-concepts/document-loaders/spreadsheet/#installation","title":"Installation","text":"<p>To use the spreadsheet loader, you need to install the required dependencies:</p> <pre><code>pip install openpyxl xlrd\n</code></pre>"},{"location":"core-concepts/document-loaders/spreadsheet/#supported-formats","title":"Supported Formats","text":"<p><code>xls</code>, <code>xlsx</code>, <code>xlsm</code>, <code>xlsb</code>, <code>odf</code>, <code>ods</code>, <code>odt</code>, <code>csv</code></p>"},{"location":"core-concepts/document-loaders/spreadsheet/#usage","title":"Usage","text":"<pre><code>from extract_thinker import DocumentLoaderSpreadSheet\n\n# Initialize the loader\nloader = DocumentLoaderSpreadSheet()\n\n# Load from file\npages = loader.load(\"path/to/your/spreadsheet.xlsx\")\n\n# Load CSV file\ncsv_content = loader.load_content_from_file(\"data.csv\")\n</code></pre>"},{"location":"core-concepts/document-loaders/spreadsheet/#features","title":"Features","text":"<ul> <li>Excel file support (.xlsx, .xls)</li> <li>CSV file support</li> <li>Multiple sheet handling</li> <li>Data type preservation</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/","title":"Tesseract Document Loader","text":"<p>The Tesseract loader uses the Tesseract OCR engine to extract text from images. It supports multiple languages and provides various OCR optimization options.</p>"},{"location":"core-concepts/document-loaders/tesseract/#supported-formats","title":"Supported Formats","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>bmp</li> <li>gif</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/tesseract/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTesseract\n\n# Initialize with default settings\nloader = DocumentLoaderTesseract()\n\n# Load document\npages = loader.load(\"path/to/your/image.png\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/tesseract/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTesseract, TesseractConfig\n\n# Create configuration\nconfig = TesseractConfig(\n    lang=\"eng+fra\",                # Use English and French\n    psm=6,                         # Assume uniform block of text\n    oem=3,                         # Default LSTM OCR Engine Mode\n    config_params={                # Additional Tesseract parameters\n        \"tessedit_char_whitelist\": \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n    },\n    timeout=30,                    # OCR timeout in seconds\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderTesseract(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/image.png\")\n</code></pre>"},{"location":"core-concepts/document-loaders/tesseract/#configuration-options","title":"Configuration Options","text":"<p>The <code>TesseractConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>lang</code> str \"eng\" Language(s) for OCR <code>psm</code> int 3 Page segmentation mode <code>oem</code> int 3 OCR Engine Mode <code>config_params</code> Dict None Additional Tesseract parameters <code>timeout</code> int 0 OCR timeout in seconds"},{"location":"core-concepts/document-loaders/tesseract/#features","title":"Features","text":"<ul> <li>Text extraction from images</li> <li>Multi-language support</li> <li>Configurable page segmentation</li> <li>Multiple OCR engine modes</li> <li>Custom Tesseract parameters</li> <li>Timeout control</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/#windows-installation","title":"\ud83e\ude9f Windows Installation","text":"<p>If you're using Windows, follow these steps to install Tesseract OCR:</p> <ol> <li>Download the Tesseract installer from UB Mannheim's GitHub repository</li> <li>Choose the appropriate installer:</li> <li>For 64-bit Windows: <code>tesseract-ocr-w64-setup-xxx.exe</code></li> <li>For 32-bit Windows: <code>tesseract-ocr-w32-setup-xxx.exe</code></li> <li>During installation:</li> <li>Choose the default installation path (<code>C:\\Program Files\\Tesseract-OCR</code>)</li> <li>Important: Check the box for \"Add to system PATH\"</li> <li>Complete the installation</li> <li>Set up environment variables by creating a <code>.env</code> file in your project's root directory: <pre><code>TESSERACT_PATH=\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n</code></pre></li> <li>Verify installation by opening a new PowerShell window and running: <pre><code>where.exe tesseract\n</code></pre></li> </ol>"},{"location":"core-concepts/document-loaders/tesseract/#notes","title":"Notes","text":"<ul> <li>Vision mode is always enabled</li> <li>Requires Tesseract installation</li> <li>Performance depends on image quality</li> <li>Local processing with no external API calls</li> <li>Language data files must be installed separately</li> </ul>"},{"location":"core-concepts/document-loaders/txt/","title":"Text File Document Loader","text":"<p>The Text File loader is a simple loader for reading plain text files. It has no external dependencies as it uses Python's built-in file handling.</p>"},{"location":"core-concepts/document-loaders/txt/#supported-formats","title":"Supported Formats","text":"<ul> <li>txt</li> </ul>"},{"location":"core-concepts/document-loaders/txt/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/txt/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTxt\n\n# Initialize the loader with default settings\nloader = DocumentLoaderTxt()\n\n# Load document\npages = loader.load(\"path/to/your/document.txt\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/txt/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTxt, TxtConfig\n\n# Create configuration\nconfig = TxtConfig(\n    encoding='utf-8',              # Specify text encoding\n    preserve_whitespace=True,      # Preserve original whitespace\n    split_paragraphs=True,         # Split text into paragraphs\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderTxt(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.txt\")\n</code></pre>"},{"location":"core-concepts/document-loaders/txt/#configuration-options","title":"Configuration Options","text":"<p>The <code>TxtConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>encoding</code> str 'utf-8' Text encoding to use <code>preserve_whitespace</code> bool False Whether to preserve whitespace in text <code>split_paragraphs</code> bool False Whether to split text into paragraphs"},{"location":"core-concepts/document-loaders/txt/#features","title":"Features","text":"<ul> <li>Simple text file reading</li> <li>Configurable text encoding</li> <li>Whitespace preservation control</li> <li>Paragraph splitting option</li> <li>Stream-based loading support</li> <li>Caching support</li> <li>No external dependencies required</li> </ul>"},{"location":"core-concepts/document-loaders/txt/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported for text files</li> <li>BytesIO streams are supported for in-memory processing</li> <li>Default encoding is UTF-8</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/","title":"Web Document Loader","text":"<p>The Web loader extracts content from web pages using BeautifulSoup. It supports HTML parsing, content cleaning, and custom element handling.</p>"},{"location":"core-concepts/document-loaders/web-loader/#supported-formats","title":"Supported Formats","text":"<ul> <li>html</li> <li>htm</li> <li>xhtml</li> <li>url</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/web-loader/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderBeautifulSoup\n\n# Initialize with default settings\nloader = DocumentLoaderBeautifulSoup()\n\n# Load document\npages = loader.load(\"https://example.com\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/web-loader/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderBeautifulSoup, BeautifulSoupConfig\n\n# Create configuration\nconfig = BeautifulSoupConfig(\n    header_handling=\"extract\",     # Extract headers as separate content\n    parser=\"lxml\",                # Use lxml parser\n    remove_elements=[             # Elements to remove\n        \"script\", \"style\", \"nav\", \"footer\"\n    ],\n    max_tokens=8192,             # Maximum tokens per page\n    request_timeout=30,          # Request timeout in seconds\n    cache_ttl=600               # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderBeautifulSoup(config)\n\n# Load and process document\npages = loader.load(\"https://example.com\")\n</code></pre>"},{"location":"core-concepts/document-loaders/web-loader/#configuration-options","title":"Configuration Options","text":"<p>The <code>BeautifulSoupConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>header_handling</code> str \"ignore\" How to handle headers <code>parser</code> str \"html.parser\" HTML parser to use <code>remove_elements</code> List[str] None Elements to remove <code>max_tokens</code> int None Maximum tokens per page <code>request_timeout</code> int 10 Request timeout in seconds"},{"location":"core-concepts/document-loaders/web-loader/#features","title":"Features","text":"<ul> <li>Web page content extraction</li> <li>Header handling options</li> <li>Custom element removal</li> <li>Multiple parser support</li> <li>Token limit control</li> <li>Request timeout control</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported</li> <li>Requires internet connection for URLs</li> <li>Local HTML files are supported</li> <li>Respects robots.txt</li> <li>May require custom headers for some sites</li> </ul>"},{"location":"core-concepts/evals/","title":"Evaluation Framework \ud83e\uddea In Beta","text":"<p>The evaluation framework helps measure the performance and reliability of your extraction models across different document types.</p>"},{"location":"core-concepts/evals/#overview","title":"Overview","text":"<p>ExtractThinker's evaluation system provides comprehensive metrics to:</p> <ul> <li>Measure extraction accuracy at both field and document levels</li> <li>Track schema validation success rates</li> <li>Monitor execution times</li> <li>Detect potential hallucinations in extracted data</li> <li>Track token usage and associated costs</li> <li>Compare performance across different models or datasets</li> </ul>"},{"location":"core-concepts/evals/#required-components","title":"Required Components","text":"<p>To use the evaluation framework, you'll need:</p> <ul> <li>An initialized <code>Extractor</code> instance</li> <li>A <code>Contract</code> class that defines your extraction schema</li> <li>A dataset containing documents and their expected outputs</li> </ul>"},{"location":"core-concepts/evals/#basic-usage","title":"Basic Usage","text":"<p>Here's how to set up and run a basic evaluation:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.eval import Evaluator, FileSystemDataset\nfrom typing import List\n\n# 1. Define your contract class\nclass InvoiceContract(Contract):\n    invoice_number: str\n    date: str\n    total_amount: float\n    line_items: List[dict]\n\n# 2. Initialize your extractor\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# 3. Create a dataset\ndataset = FileSystemDataset(\n    documents_dir=\"./test_invoices/\",\n    labels_path=\"./test_invoices/labels.json\",\n    name=\"Invoice Test Set\"\n)\n\n# 4. Set up evaluator\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract\n)\n\n# 5. Run evaluation\nreport = evaluator.evaluate(dataset)\n\n# 6. Print summary and save detailed report\nreport.print_summary()\nevaluator.save_report(report, \"evaluation_results.json\")\n</code></pre> <p>Tip: For consistent evaluations, use a temperature of 0.0 in your model configuration to ensure deterministic outputs.</p> <p>\ud83d\udca1 Model Temperature</p> <p>For consistent evaluations, use a temperature of 0.0 in your model configuration to ensure deterministic outputs.</p>"},{"location":"core-concepts/evals/#command-line-interface","title":"Command Line Interface","text":"<p>ExtractThinker includes a CLI for running evaluations from configuration files:</p> <pre><code>extract_thinker-eval --config eval_config.json --output results.json\n</code></pre> <p>Example configuration file:</p> <pre><code>{\n  \"evaluation_name\": \"Invoice Extraction Test\",\n  \"dataset_name\": \"Invoice Dataset\",\n  \"contract_path\": \"./contracts/invoice_contract.py\",\n  \"documents_dir\": \"./test_invoices/\",\n  \"labels_path\": \"./test_invoices/labels.json\",\n  \"file_pattern\": \"*.pdf\",\n  \"llm\": {\n    \"model\": \"gpt-4o\",\n    \"params\": {\n      \"temperature\": 0.0\n    }\n  },\n  \"vision\": false,\n  \"skip_failures\": false\n}\n</code></pre>"},{"location":"core-concepts/evals/#available-metrics","title":"Available Metrics","text":"<p>ExtractThinker captures several key metrics during evaluation:</p> Metric Type Description Use Case Field-level Precision, recall, F1 scores for each field Identify problematic fields Document-level Overall accuracy across all documents General model performance Schema validation Success rate of schema validation Data structure correctness Execution time Average and per-document processing time Performance optimization Hallucination Detection of fabricated information Trust and reliability Cost Token usage and associated costs Budget optimization"},{"location":"core-concepts/evals/#sample-output","title":"Sample Output","text":"<pre><code>=== Invoice Extraction Evaluation ===\nDataset: Invoice Test Set\nModel: gpt-4o\nTimestamp: 2023-08-15T14:30:45\n\n=== Overall Metrics ===\nDocuments tested: 50\nDocument accuracy: 92.00%\nSchema validation rate: 96.00%\nAverage precision: 95.40%\nAverage recall: 94.80%\nAverage F1 score: 95.10%\nAverage execution time: 2.34s\n\n=== Field-Level Metrics ===\ninvoice_number (comparison: exact):\n  Precision: 98.00%\n  Recall: 98.00%\n  F1 Score: 98.00%\n  Accuracy: 98.00%\ndate:\n  Precision: 94.00%\n  Recall: 94.00%\n  F1 Score: 94.00%\n  Accuracy: 94.00%\n...\n</code></pre>"},{"location":"core-concepts/evals/#evaluation-features","title":"Evaluation Features","text":"<p>ExtractThinker offers several specialized evaluation capabilities:</p>"},{"location":"core-concepts/evals/#field-comparison-types","title":"Field Comparison Types","text":"<p>Different fields may require different comparison methods:</p> <pre><code>from extract_thinker.eval import ComparisonType\n\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract,\n    field_comparisons={\n        \"invoice_number\": ComparisonType.EXACT,  # Exact match required\n        \"description\": ComparisonType.SEMANTIC,  # Semantic similarity\n        \"total_amount\": ComparisonType.NUMERIC   # Allows percentage tolerance\n    }\n)\n</code></pre> <p>Learn more about field comparison types \u2192</p>"},{"location":"core-concepts/evals/#teacher-student-evaluation","title":"Teacher-Student Evaluation","text":"<p>Benchmark your model against a more capable \"teacher\" model:</p> <pre><code>from extract_thinker.eval import TeacherStudentEvaluator\n\nevaluator = TeacherStudentEvaluator(\n    student_extractor=student_extractor,\n    teacher_extractor=teacher_extractor,\n    response_model=InvoiceContract\n)\n</code></pre> <p>Learn more about teacher-student evaluation \u2192</p>"},{"location":"core-concepts/evals/#hallucination-detection","title":"Hallucination Detection","text":"<p>Identify potentially hallucinated content:</p> <pre><code>evaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract,\n    detect_hallucinations=True\n)\n</code></pre> <p>Learn more about hallucination detection \u2192</p>"},{"location":"core-concepts/evals/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor token usage and costs:</p> <pre><code>evaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract,\n    track_costs=True\n)\n</code></pre> <p>Learn more about cost tracking \u2192</p>"},{"location":"core-concepts/evals/#best-practices","title":"Best Practices","text":"<ul> <li>Dataset diversity: Include a wide range of document variations in your test set</li> <li>Consistent formatting: Use consistent file formats and naming conventions</li> <li>Benchmark different models: Run evaluations on different model configurations</li> <li>Field-level analysis: Monitor field-level metrics to identify specific areas for improvement</li> <li>Specialized test sets: Create separate test sets for different document types</li> <li>Hallucination checks: Enable hallucination detection for critical applications</li> <li>Cost optimization: Track costs to optimize the performance/price ratio</li> <li>Version control: Keep evaluation datasets under version control to track improvements over time</li> </ul>"},{"location":"core-concepts/evals/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more complex evaluation needs:</p> <pre><code># Advanced evaluator setup with multiple features\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract,\n    vision=True,  # Enable vision mode for image-based documents\n    content=\"Focus on the header section for invoice number and date.\",\n    field_comparisons={\n        \"invoice_number\": ComparisonType.EXACT,\n        \"description\": ComparisonType.SEMANTIC\n    },\n    detect_hallucinations=True,\n    track_costs=True\n)\n\n# Run evaluation with special options\nreport = evaluator.evaluate(\n    dataset=dataset,\n    evaluation_name=\"Comprehensive Invoice Evaluation\",\n    skip_failures=True  # Continue even when schema validation fails\n)\n</code></pre>"},{"location":"core-concepts/evals/cost-tracking/","title":"Cost Tracking \ud83e\uddea In Beta","text":""},{"location":"core-concepts/evals/cost-tracking/#overview","title":"Overview","text":"<p>ExtractThinker provides built-in cost tracking for evaluations, helping you monitor token usage and associated costs when using various LLM models.</p>"},{"location":"core-concepts/evals/cost-tracking/#basic-usage","title":"Basic Usage","text":"<p>To enable cost tracking in your evaluations:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.eval import Evaluator, FileSystemDataset\n\n# Initialize your extractor\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Create evaluator with cost tracking enabled\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=YourContract,\n    track_costs=True  # Enable cost tracking\n)\n\n# Run evaluation\nreport = evaluator.evaluate(dataset)\n</code></pre>"},{"location":"core-concepts/evals/cost-tracking/#command-line-usage","title":"Command Line Usage","text":"<p>You can also enable cost tracking through the CLI:</p> <pre><code>extract_thinker-eval --config eval_config.json --output results.json --track-costs\n</code></pre> <p>Or in your config file:</p> <pre><code>{\n  \"evaluation_name\": \"Invoice Extraction Test\",\n  \"dataset_name\": \"Invoice Dataset\",\n  \"contract_path\": \"./contracts/invoice_contract.py\",\n  \"documents_dir\": \"./test_invoices/\",\n  \"labels_path\": \"./test_invoices/labels.json\",\n  \"track_costs\": true,\n  \"llm\": {\n    \"model\": \"gpt-4o\"\n  }\n}\n</code></pre>"},{"location":"core-concepts/evals/cost-tracking/#how-it-works","title":"How It Works","text":"<p>Cost tracking leverages LiteLLM's built-in cost calculation features to:</p> <ol> <li>Count input and output tokens for each extraction</li> <li>Calculate costs based on current model pricing</li> <li>Aggregate metrics across all evaluated documents</li> </ol>"},{"location":"core-concepts/evals/cost-tracking/#interpreting-results","title":"Interpreting Results","text":"<p>Cost metrics appear in the evaluation report:</p> <pre><code>=== Cost Metrics ===\nTotal cost: $2.4768\nAverage cost per document: $0.0495\nTotal tokens: 123,840\n  - Input tokens: 98,450\n  - Output tokens: 25,390\n</code></pre> <p>The cost data is also available programmatically:</p> <pre><code># Access overall cost metrics\ntotal_cost = report.metrics[\"total_cost\"]\naverage_cost = report.metrics[\"average_cost\"]\ntotal_tokens = report.metrics[\"total_tokens\"]\n\n# Access document-specific costs\nfor result in report.results:\n    doc_id = result[\"doc_id\"]\n    doc_tokens = result[\"tokens\"]\n    doc_cost = result[\"cost\"]\n\n    print(f\"Document: {doc_id}\")\n    print(f\"  Cost: ${doc_cost:.4f}\")\n    print(f\"  Input tokens: {doc_tokens['input']}\")\n    print(f\"  Output tokens: {doc_tokens['output']}\")\n    print(f\"  Total tokens: {doc_tokens['total']}\")\n</code></pre>"},{"location":"core-concepts/evals/cost-tracking/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":"<p>Cost tracking is particularly useful for:</p> <ol> <li>Model comparison: Understand the cost-accuracy tradeoffs between different models</li> <li>Optimization: Identify expensive documents that might need prompt optimization</li> <li>Budgeting: Estimate production deployment costs based on evaluation results</li> <li>ROI calculation: Calculate return on investment by comparing accuracy improvements to increased costs</li> </ol>"},{"location":"core-concepts/evals/cost-tracking/#teacher-student-integration","title":"Teacher-Student Integration","text":"<p>Cost tracking works seamlessly with the teacher-student evaluation approach to help quantify the cost-benefit relationship of using more capable models:</p> <pre><code>from extract_thinker.eval import TeacherStudentEvaluator\n\n# Set up teacher-student evaluator with cost tracking\nevaluator = TeacherStudentEvaluator(\n    student_extractor=student_extractor,\n    teacher_extractor=teacher_extractor,\n    response_model=InvoiceContract,\n    track_costs=True  # Enable cost tracking\n)\n\n# Run evaluation\nreport = evaluator.evaluate(dataset)\n\n# The report will include cost differences between teacher and student models\nstudent_cost = report.metrics[\"student_average_cost\"]\nteacher_cost = report.metrics[\"teacher_average_cost\"]\ncost_ratio = teacher_cost / student_cost\n\nprint(f\"Cost ratio (teacher/student): {cost_ratio:.2f}x\")\nprint(f\"Accuracy improvement: {report.metrics['document_accuracy_improvement']:.2f}%\")\n</code></pre> <p>This helps answer questions like \"Is a 15% accuracy improvement worth a 3x cost increase?\"</p>"},{"location":"core-concepts/evals/cost-tracking/#supported-models","title":"Supported Models","text":"<p>Cost tracking works with all models supported by LiteLLM, including: - OpenAI models (GPT-3.5, GPT-4, etc.) - Claude models (Claude 3 Opus, Sonnet, etc.) - Mistral models - Most other major LLM providers</p>"},{"location":"core-concepts/evals/cost-tracking/#limitations","title":"Limitations","text":"<ul> <li>Costs are estimated based on current pricing and may not reflect custom pricing arrangements</li> <li>For some models, costs may be approximate if token counting methods vary</li> <li>Document loading/preprocessing costs are not included </li> </ul>"},{"location":"core-concepts/evals/field-comparison/","title":"Field Comparison Types \ud83e\uddea In Beta","text":"<p>When evaluating extraction results, different fields may require different comparison methods. For example:</p> <ul> <li>ID fields (like invoice numbers) typically require exact matching</li> <li>Text descriptions might benefit from semantic similarity comparison</li> <li>Numeric values could use tolerance-based comparison</li> <li>Notes or comments might allow for fuzzy matching</li> </ul> <p>ExtractThinker's evaluation framework supports multiple comparison methods to address these different requirements.</p>"},{"location":"core-concepts/evals/field-comparison/#available-comparison-types","title":"Available Comparison Types","text":"Comparison Type Description Best For <code>EXACT</code> Perfect string/value match (default) IDs, codes, dates, categorical values <code>FUZZY</code> Approximate string matching using Levenshtein distance Text with potential minor variations <code>SEMANTIC</code> Semantic similarity using embeddings Descriptions, summaries, longer text <code>NUMERIC</code> Numeric comparison with percentage tolerance Amounts, quantities, measurements <code>CUSTOM</code> Custom comparison function Complex or domain-specific comparisons"},{"location":"core-concepts/evals/field-comparison/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.eval import Evaluator, FileSystemDataset, ComparisonType\n\n# Define your contract\nclass InvoiceContract(Contract):\n    invoice_number: str  # Needs exact matching\n    description: str     # Can use semantic similarity\n    total_amount: float  # Can use numeric tolerance\n\n# Initialize your extractor\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Create a dataset\ndataset = FileSystemDataset(\n    documents_dir=\"./test_invoices/\",\n    labels_path=\"./test_invoices/labels.json\",\n    name=\"Invoice Test Set\"\n)\n\n# Set up evaluator with different field comparison types\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=InvoiceContract,\n    field_comparisons={\n        \"invoice_number\": ComparisonType.EXACT,  # Must match exactly\n        \"description\": ComparisonType.SEMANTIC,  # Compare meaning\n        \"total_amount\": ComparisonType.NUMERIC   # Allow small % difference\n    }\n)\n\n# Run evaluation\nreport = evaluator.evaluate(dataset)\n</code></pre>"},{"location":"core-concepts/evals/field-comparison/#configuring-comparison-parameters","title":"Configuring Comparison Parameters","text":"<p>Each comparison type has configurable parameters:</p> <pre><code># Configure thresholds for semantic similarity (description should be at least 80% similar)\nevaluator.set_field_comparison(\n    \"description\",\n    ComparisonType.SEMANTIC,\n    similarity_threshold=0.8\n)\n\n# Configure tolerance for numeric fields (total_amount can be within 2% of expected)\nevaluator.set_field_comparison(\n    \"total_amount\",\n    ComparisonType.NUMERIC,\n    numeric_tolerance=0.02\n)\n</code></pre>"},{"location":"core-concepts/evals/field-comparison/#custom-comparison-functions","title":"Custom Comparison Functions","text":"<p>For specialized comparisons, you can define custom comparison functions:</p> <pre><code>def compare_dates(expected, predicted):\n    \"\"\"Custom date comparison that handles different date formats.\"\"\"\n    from datetime import datetime\n    # Try to parse both as dates\n    try:\n        expected_date = datetime.strptime(expected, \"%Y-%m-%d\")\n        # Try different formats for predicted\n        for fmt in [\"%Y-%m-%d\", \"%m/%d/%Y\", \"%d-%m-%Y\", \"%B %d, %Y\"]:\n            try:\n                predicted_date = datetime.strptime(predicted, fmt)\n                return expected_date == predicted_date\n            except ValueError:\n                continue\n        return False\n    except ValueError:\n        return expected == predicted\n\n# Set custom comparison\nevaluator.set_field_comparison(\n    \"invoice_date\",\n    ComparisonType.CUSTOM,\n    custom_comparator=compare_dates\n)\n</code></pre>"},{"location":"core-concepts/evals/field-comparison/#results-interpretation","title":"Results Interpretation","text":"<p>The evaluation report will show which comparison type was used for each field:</p> <pre><code>=== Field-Level Metrics ===\ninvoice_number (comparison: exact):\n  Precision: 98.00%\n  Recall: 98.00%\n  F1 Score: 98.00%\n  Accuracy: 98.00%\ndescription (comparison: semantic):\n  Precision: 92.00%\n  Recall: 92.00%\n  F1 Score: 92.00%\n  Accuracy: 92.00%\ntotal_amount (comparison: numeric):\n  Precision: 96.00%\n  Recall: 96.00%\n  F1 Score: 96.00%\n  Accuracy: 96.00%\n</code></pre>"},{"location":"core-concepts/evals/field-comparison/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>EXACT</code> for fields where precise matching is critical (IDs, codes)</li> <li>Use <code>SEMANTIC</code> for long-form text that may vary in wording but should convey the same meaning</li> <li>Use <code>NUMERIC</code> for financial data, allowing for small rounding differences</li> <li>Use <code>FUZZY</code> for fields that may contain typos or minor variations</li> <li>Configure thresholds based on your application's tolerance for errors</li> </ul>"},{"location":"core-concepts/evals/hallucination-detection/","title":"Hallucination Detection \ud83e\uddea In Beta","text":""},{"location":"core-concepts/evals/hallucination-detection/#overview","title":"Overview","text":"<p>When extracting information from documents, language models sometimes \"hallucinate\" content by generating information that isn't actually present in the source document. ExtractThinker's hallucination detection helps identify these cases.</p> <p>\ud83d\udca1 When To Use</p> <p>Enable hallucination detection when working with summarizations.</p>"},{"location":"core-concepts/evals/hallucination-detection/#basic-usage","title":"Basic Usage","text":"<p>To enable hallucination detection in your evaluations:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.eval import Evaluator, FileSystemDataset\n\n# Initialize your extractor\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Create evaluator with hallucination detection enabled\nevaluator = Evaluator(\n    extractor=extractor,\n    response_model=YourContract,\n    detect_hallucinations=True  # Enable hallucination detection\n)\n\n# Run evaluation\nreport = evaluator.evaluate(dataset)\n</code></pre>"},{"location":"core-concepts/evals/hallucination-detection/#command-line-usage","title":"Command Line Usage","text":"<p>You can also enable hallucination detection through the CLI:</p> <pre><code>extract_thinker-eval --config eval_config.json --output results.json --detect-hallucinations\n</code></pre> <p>Or in your config file:</p> <pre><code>{\n  \"evaluation_name\": \"Invoice Extraction Test\",\n  \"dataset_name\": \"Invoice Dataset\",\n  \"contract_path\": \"./contracts/invoice_contract.py\",\n  \"documents_dir\": \"./test_invoices/\",\n  \"labels_path\": \"./test_invoices/labels.json\",\n  \"detect_hallucinations\": true,\n  \"llm\": {\n    \"model\": \"gpt-4o\"\n  }\n}\n</code></pre>"},{"location":"core-concepts/evals/hallucination-detection/#how-it-works","title":"How It Works","text":"<p>The hallucination detector compares extracted field values against the source document text to determine whether the information could reasonably have been derived from the document.</p> <p>For each field, a hallucination score between 0.0 and 1.0 is calculated:</p> <ul> <li>0.0 - 0.3: Content is present in the document</li> <li>0.3 - 0.7: Content might be partially inferred from the document</li> <li>0.7 - 1.0: Content appears to be hallucinated</li> </ul> <p>The detector uses two strategies:</p> <ol> <li>Heuristic detection: Uses pattern matching and text similarity to check if extracted values appear in the document</li> <li>LLM-assisted detection: Uses an LLM to determine if extracted information could reasonably be inferred from the document</li> </ol>"},{"location":"core-concepts/evals/hallucination-detection/#interpreting-results","title":"Interpreting Results","text":"<p>Hallucination scores appear in the evaluation report:</p> <pre><code>=== Hallucination Metrics ===\nAverage hallucination score: 0.32\nFields with potential hallucinations: 3\n\n=== Field-Level Metrics ===\ninvoice_number (comparison: exact):\n  Precision: 96.00%\n  Recall: 96.00%\n  F1 Score: 96.00%\n  Accuracy: 96.00%\n  Hallucination score: 0.05\ndescription (comparison: semantic):\n  Precision: 89.00%\n  Recall: 89.00%\n  F1 Score: 89.00%\n  Accuracy: 89.00%\n  Hallucination score: 0.78\n</code></pre> <p>The hallucination data is also available programmatically:</p> <pre><code># Access hallucination data for specific documents\nfor result in report.results:\n    if \"hallucination_results\" in result:\n        hallucination_data = result[\"hallucination_results\"]\n        print(f\"Document {result['doc_id']} overall hallucination score: {hallucination_data['overall_score']}\")\n\n        # Field-specific hallucination scores\n        for field_name, score in hallucination_data[\"field_scores\"].items():\n            print(f\"  {field_name}: {score}\")\n\n        # Detailed reasoning for hallucination detection\n        for detail in hallucination_data[\"detailed_results\"]:\n            print(f\"  {detail['field_name']}: {detail['hallucination_score']} - {detail['reasoning']}\")\n</code></pre>"},{"location":"core-concepts/evals/hallucination-detection/#advanced-usage","title":"Advanced Usage","text":"<p>You can customize the hallucination detector by directly working with the <code>HallucinationDetector</code> class:</p> <pre><code>from extract_thinker.eval import HallucinationDetector\n\n# Create a custom detector with a different threshold\ndetector = HallucinationDetector(\n    llm=extractor.llm,\n    threshold=0.8  # More tolerant threshold (default is 0.7)\n)\n\n# Run detection on extracted data\nresults = detector.detect_hallucinations(\n    extracted_data=extracted_data,\n    document_text=document_text\n)\n</code></pre>"},{"location":"core-concepts/evals/hallucination-detection/#best-practices","title":"Best Practices","text":"<ol> <li>Always enable for sensitive data extraction: For financial, legal, or medical documents</li> <li>Review high hallucination scores: Fields with scores above 0.7 should be manually verified</li> <li>Use with LLM-enabled detection: Providing an LLM for detection offers more nuanced results</li> <li>Compare with ground truth: High hallucination scores combined with incorrect extraction indicate model confabulation </li> </ol>"},{"location":"core-concepts/evals/teacher-student/","title":"Teacher-Student Evaluation \ud83e\uddea In Beta","text":"<p>This approach is inspired by the teacher-student progressive learning technique described in the paper YODA: Teacher-Student Progressive Learning for Language Models. While commonly used for model distillation, ExtractThinker adapts this concept for evaluation purposes, where more capable \"teacher\" models assess and benchmark smaller \"student\" models in extraction pipelines.</p> <p>The teacher-student approach allows you to benchmark your extractor against a more capable \"teacher\" model to identify performance gaps and potential areas for improvement.</p> <p>This approach is particularly useful for:</p> <ul> <li>Understanding the performance ceiling with better models</li> <li>Identifying fields that benefit most from model upgrades</li> <li>Quantifying the cost-performance tradeoff between different models</li> <li>Creating a baseline for progressive model improvement</li> </ul>"},{"location":"core-concepts/evals/teacher-student/#basic-setup","title":"Basic Setup","text":"<pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.eval import TeacherStudentEvaluator, FileSystemDataset\nfrom extract_thinker.document_loader import DocumentLoaderPyPdf, DocumentLoaderAWSTextract\n\n# Define your contract\nclass InvoiceContract(Contract):\n    invoice_number: str\n    date: str\n    total_amount: float\n    line_items: List[dict]\n\n# Initialize \"student\" extractor with standard configuration\nstudent_extractor = Extractor()\nstudent_extractor.load_document_loader(DocumentLoaderPyPdf())\nstudent_extractor.load_llm(\"gpt-4o-mini\") # More affordable model\n\n# Initialize \"teacher\" extractor with superior configuration\nteacher_extractor = Extractor()\nteacher_extractor.load_document_loader(DocumentLoaderAWSTextract())\nteacher_extractor.load_llm(\"gpt-4o\") # More capable model\n\n# Create dataset\ndataset = FileSystemDataset(\n    documents_dir=\"./test_invoices/\",\n    labels_path=\"./test_invoices/labels.json\",\n    name=\"Invoice Test Set\"\n)\n\n# Set up teacher-student evaluator\nevaluator = TeacherStudentEvaluator(\n    student_extractor=student_extractor,\n    teacher_extractor=teacher_extractor,\n    response_model=InvoiceContract\n)\n\n# Run comparative evaluation\nreport = evaluator.evaluate(dataset)\n\n# Print comparative summary\nreport.print_summary()\n</code></pre>"},{"location":"core-concepts/evals/teacher-student/#interpreting-comparative-results","title":"Interpreting Comparative Results","text":"<p>The evaluation report provides side-by-side metrics for both models and the improvement percentages:</p> <pre><code>=== Teacher-Student Evaluation ===\nDataset: Invoice Test Set\nModel(s): Student: gpt-4o-mini, Teacher: gpt-4o\nTimestamp: 2024-06-01T12:34:56.789012\n\n=== Student Model Metrics ===\nDocuments tested: 50\nDocument accuracy: 75.00%\nSchema validation rate: 92.00%\nAverage precision: 85.50%\nAverage recall: 82.00%\nAverage F1 score: 83.70%\nAverage execution time: 1.85s\n\n=== Teacher Model Metrics ===\nDocument accuracy: 94.00%\nSchema validation rate: 100.00%\nAverage precision: 96.50%\nAverage recall: 95.00%\nAverage F1 score: 95.74%\nAverage execution time: 3.25s\n\n=== Comparison Metrics ===\nDocument accuracy improvement: 25.33%\nExecution time ratio (teacher/student): 1.76x\n\n=== Field-Level Improvements ===\ninvoice_number:\n  Student F1: 92.00%\n  Teacher F1: 98.00%\n  Improvement: 6.52%\ndate:\n  Student F1: 88.00%\n  Teacher F1: 96.00%\n  Improvement: 9.09%\ntotal_amount:\n  Student F1: 78.00%\n  Teacher F1: 94.00%\n  Improvement: 20.51%\n...\n</code></pre>"},{"location":"core-concepts/evals/teacher-student/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure different settings for the student and teacher models:</p> <pre><code>evaluator = TeacherStudentEvaluator(\n    student_extractor=student_extractor,\n    teacher_extractor=teacher_extractor,\n    response_model=InvoiceContract,\n    student_vision=False,\n    teacher_vision=True, # Enable vision only for teacher\n    student_content=\"Extract the basic invoice details.\",\n    teacher_content=\"Extract all invoice details with high precision.\"\n)\n</code></pre>"},{"location":"core-concepts/evals/teacher-student/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":"<p>The teacher-student approach helps you make informed decisions about model selection by quantifying the performance gains relative to the additional cost and processing time of more capable models.</p> <p>Consider the following when interpreting results: - Is the accuracy improvement worth the increased cost? - Are there specific fields that benefit more from the teacher model? - Would a different document loader provide better results without increasing model costs? - Can you tailor prompts to close the gap between student and teacher performance?</p>"},{"location":"core-concepts/extractors/","title":"Extractor","text":"<p>Extractor is the component that coordinates the extraction from documents. Contains a group of features for document processing like classify. Can be used alone or in group, inside of a Process.</p>"},{"location":"core-concepts/extractors/#basic-extraction","title":"Basic Extraction","text":"<p>The simplest way to extract data is using the <code>Extractor</code> class with a defined contract:</p> <pre><code>from extract_thinker import Extractor, DocumentLoaderPyPdf, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    total_amount: float\n\n#Initialize the extractor\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\nextractor.load_llm(\"gpt-4o-mini\") # or any other supported model\n\n#Extract data from your document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n\nprint(f\"Invoice #{result.invoice_number}\")\nprint(f\"Date: {result.invoice_date}\")\nprint(f\"Total: ${result.total_amount}\")\n</code></pre>"},{"location":"core-concepts/extractors/#choosing-the-right-model","title":"Choosing the Right Model","text":"<p>When performing extraction, selecting the appropriate model is crucial for balancing performance, accuracy, and cost:</p> <ul> <li>GPT-4o-mini: Best for basic text extraction tasks, similar to OCR. Cost-effective for high-volume processing.</li> <li>GPT-4o: Ideal for tasks requiring deeper understanding of document structure and content.</li> <li>o1 and o1-mini: Perfect for complex extraction requiring reasoning and calculations.</li> </ul>"},{"location":"core-concepts/extractors/#advanced-extraction-with-vision","title":"Advanced Extraction with Vision","text":"<p>For documents that contain images or require visual understanding, you can enable vision capabilities:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom typing import List\n\nclass ChartData(Contract):\n    title: str\n    data_points: List[float]\n    description: str\n\n#Initialize with vision support\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n#Extract with vision enabled\nresult = extractor.extract(\n    \"chart.png\",\n    ChartData,\n    vision=True # Enable vision processing\n)\n</code></pre> <p>Note: When using vision capabilities, ensure your documents are high quality images or PDFs for optimal results.</p>"},{"location":"core-concepts/extractors/#adding-context-to-extraction","title":"Adding Context to Extraction","text":"<p>You can provide additional context to help guide the extraction process:</p> <pre><code>from extract_thinker import Extractor, Contract\nclass ResumeContract(Contract):\n    name: str\n    skills: List[str]\n    experience: List[dict]\n\n#Add context about the job requirements\njob_description = {\n    \"role\": \"Software Engineer\",\n    \"required_skills\": [\"Python\", \"AWS\", \"Docker\"]\n}\n\nresult = extractor.extract(\n    \"resume.pdf\",\n    ResumeContract,\n    content=job_description # Add extra context\n)\n</code></pre>"},{"location":"core-concepts/extractors/batch/","title":"Batch Processing with Extractors","text":"<p>ExtractThinker provides powerful batch processing capabilities for handling large volumes of documents efficiently. This feature enables cost-effective processing when immediate response time is not critical.</p>"},{"location":"core-concepts/extractors/batch/#basic-batch-processing","title":"Basic Batch Processing","text":"<p>Here's how to use batch processing with the Extractor:</p> <pre><code>from extract_thinker import Extractor, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    total_amount: float\n\n# Initialize batch processing\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o-mini\")\n\n# Create batch job\nbatch_job = extractor.extract_batch(\n    \"invoices/*.pdf\",\n    InvoiceContract\n)\n\n# Monitor status and get results\nstatus = await batch_job.get_status()\nresults = await batch_job.get_result()\n</code></pre>"},{"location":"core-concepts/extractors/batch/#batch-job-status","title":"Batch Job Status","text":"<p>Batch jobs can have the following statuses:</p> <ul> <li><code>queued</code>: Job is waiting to be processed</li> <li><code>processing</code>: Job is currently being processed</li> <li><code>completed</code>: Job has finished successfully</li> <li><code>failed</code>: Job encountered an error</li> </ul>"},{"location":"core-concepts/extractors/image_charts/","title":"Image and Chart Processing","text":"<p>ExtractThinker provides specialized capabilities for processing images and extracting data from charts using vision-enabled models. This guide covers how to effectively use these features.</p>"},{"location":"core-concepts/extractors/image_charts/#basic-vision-processing","title":"Basic Vision Processing","text":"<p>For documents containing images or requiring visual understanding:</p> <pre><code>from extract_thinker import Extractor, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n\n# Initialize with vision support\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Extract with vision enabled\nresult = extractor.extract(\n    \"invoice.pdf\",\n    InvoiceContract,\n    vision=True  # Enable vision processing\n)\n</code></pre>"},{"location":"core-concepts/extractors/image_charts/#chart-analysis","title":"Chart Analysis","text":"<p>For extracting data from charts and graphs:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom typing import List, Literal\n\nclass Chart(Contract):\n    classification: Literal['line', 'bar', 'pie']\n    coordinates: List[XYCoordinate]\n    description: str\n\nclass ChartWithContent(Contract):\n    content: str  # Text content from the page\n    chart: Chart  # Extracted chart data\n\n# Initialize extractor for chart analysis\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")  # Required for chart analysis\n\n# Extract chart data\nresult = extractor.extract(\n    \"chart.png\",\n    ChartWithContent,\n    vision=True\n)\n</code></pre>"},{"location":"core-concepts/extractors/image_charts/#model-selection-for-visual-tasks","title":"Model Selection for Visual Tasks","text":"<p>Different models are optimized for different visual tasks:</p> <ul> <li>GPT-4o: Required for vision tasks, chart analysis, and complex visual understanding</li> <li>GPT-4o-mini: Not suitable for vision tasks - use for text extraction only</li> </ul>"},{"location":"core-concepts/extractors/image_charts/#best-practices","title":"Best Practices","text":"<ul> <li>Enable vision processing (<code>vision=True</code>) when working with images or charts</li> <li>Use GPT-4o or higher models for vision tasks</li> <li>Consider using a DocumentLoader in combination with vision for optimal results</li> <li>Ensure high-quality input images for best accuracy</li> </ul>"},{"location":"core-concepts/extractors/image_charts/#limitations","title":"Limitations","text":"<ul> <li>Vision processing requires GPT-4o or higher models</li> <li>Processing time may be longer for vision-enabled extraction</li> <li>Image quality significantly impacts extraction accuracy</li> </ul>"},{"location":"core-concepts/llm-integration/","title":"LLM Integration","text":"<p>The LLM component in ExtractThinker acts as a bridge between your document processing pipeline and various Language Model providers. It handles request formatting, response parsing, and provider-specific optimizations.</p> Base LLM Implementation <pre><code>import asyncio\nfrom typing import List, Dict, Any, Optional\nimport instructor\nimport litellm\nfrom litellm import Router\nfrom extract_thinker.llm_engine import LLMEngine\nfrom extract_thinker.utils import add_classification_structure, extract_thinking_json\n\n# Helper to build the dynamic prompt used when `is_dynamic=True`.\n# We expose it as a standalone function so that callers (or subclasses)\n# can supply their own variants if needed.\n\n\ndef build_dynamic_prompt(structure: str, *, think_tag: str = \"think\") -&gt; str:\n    \"\"\"Return the dynamic prompt used for classification style requests.\n\n    Args:\n        structure: The JSON structure/fields to be returned by the model.\n        think_tag: The XML-style tag that should wrap the model's chain-of-thought.\n\n    This helper allows downstream users to customise the surrounding text\n    (for example, changing the tag name or adding extra instructions) rather\n    than editing a hard-coded string inside *llm.py*.\n    \"\"\"\n\n    return (\n        f\"Please provide your thinking process within &lt;{think_tag}&gt; tags, \"\n        \"followed by your JSON output.\\n\\n\"\n        \"JSON structure:\\n\"\n        f\"{structure}\\n\\n\"\n        \"OUTPUT example:\\n\"\n        f\"&lt;{think_tag}&gt;\\n\"\n        \"Your step-by-step reasoning and analysis goes here...\\n\"\n        f\"&lt;/{think_tag}&gt;\\n\\n\"\n        \"##JSON OUTPUT\\n\"\n        \"{\\n    ...\\n}\"  # placeholder keeps JSON fence out of model context\n    )\n\nclass LLM:\n    TIMEOUT = 3000  # Timeout in milliseconds\n    DEFAULT_TEMPERATURE = 0\n    THINKING_BUDGET_TOKENS = 8000\n    DEFAULT_PAGE_TOKENS = 1500  # Each page has this many tokens (text + image)\n    DEFAULT_THINKING_RATIO = 1/3  # Thinking budget as a fraction of content tokens\n    MAX_TOKEN_LIMIT = 120000  # Maximum token limit (for Claude 3.7 Sonnet)\n    MAX_THINKING_BUDGET = 64000  # Maximum thinking budget\n    MIN_THINKING_BUDGET = 1200  # Minimum thinking budget\n    DEFAULT_OUTPUT_TOKENS = 32000\n\n    # A single default completion-token limit that is accepted by the vast\n    # majority of models.  If a model supports more (or you need fewer), pass\n    # `token_limit=` when instantiating `LLM` to override this value.\n    DEFAULT_MAX_COMPLETION_TOKENS = 8000\n\n    def __init__(\n        self,\n        model: str,\n        token_limit: int = None,\n        backend: LLMEngine = LLMEngine.DEFAULT\n    ):\n        \"\"\"Initialize LLM with specified backend.\n\n        Args:\n            model: The model name (e.g. \"gpt-4\", \"claude-3\")\n            token_limit: Optional maximum tokens\n            backend: LLMBackend enum (default: LITELLM)\n        \"\"\"\n        self.model = model\n        self.token_limit = token_limit\n        self.router = None\n        self.is_dynamic = False\n        self.backend = backend\n        self.temperature = self.DEFAULT_TEMPERATURE\n        self.is_thinking = False  # Initialize is_thinking flag\n        self.page_count = None  # Initialize page count\n        self.thinking_budget = self.THINKING_BUDGET_TOKENS  # Default thinking budget\n        self.thinking_token_limit: Optional[int] = None\n\n        if self.backend == LLMEngine.DEFAULT:\n            self.client = instructor.from_litellm(\n                litellm.completion,\n                mode=instructor.Mode.MD_JSON\n            )\n            self.agent = None\n        elif self.backend == LLMEngine.PYDANTIC_AI:\n            self._check_pydantic_ai()\n            from pydantic_ai import Agent\n            from pydantic_ai.models import KnownModelName\n            from typing import cast\n\n            self.client = None\n            self.agent = Agent(\n                cast(KnownModelName, self.model)\n            )\n        else:\n            raise ValueError(f\"Unsupported backend: {self.backend}\")\n\n    @staticmethod\n    def _check_pydantic_ai():\n        \"\"\"Check if pydantic-ai is installed.\"\"\"\n        try:\n            import pydantic_ai\n        except ImportError:\n            raise ImportError(\n                \"Could not import pydantic-ai package. \"\n                \"Please install it with `pip install pydantic-ai`.\"\n            )\n\n    @staticmethod\n    def _get_pydantic_ai():\n        \"\"\"Lazy load pydantic-ai.\"\"\"\n        try:\n            import pydantic_ai\n            return pydantic_ai\n        except ImportError:\n            raise ImportError(\n                \"Could not import pydantic-ai package. \"\n                \"Please install it with `pip install pydantic-ai`.\"\n            )\n\n    def load_router(self, router: Router) -&gt; None:\n        \"\"\"Load a LiteLLM router for model fallbacks.\"\"\"\n        if self.backend != LLMEngine.DEFAULT:\n            raise ValueError(\"Router is only supported with LITELLM backend\")\n        self.router = router\n\n    def set_temperature(self, temperature: float) -&gt; None:\n        \"\"\"Set the temperature for LLM requests.\n\n        Args:\n            temperature (float): Temperature value between 0 and 1\n        \"\"\"\n        self.temperature = temperature\n\n    def set_thinking(self, is_thinking: bool) -&gt; None:\n        \"\"\"Set whether the LLM should handle thinking.\n\n        Args:\n            is_thinking (bool): Whether to enable thinking\n        \"\"\"\n        self.is_thinking = is_thinking\n        self.temperature = 1\n\n    def set_dynamic(self, is_dynamic: bool) -&gt; None:\n        \"\"\"Set whether the LLM should handle dynamic content.\n\n        When dynamic is True, the LLM will attempt to parse and validate JSON responses.\n        This is useful for handling structured outputs like masking mappings.\n\n        Args:\n            is_dynamic (bool): Whether to enable dynamic content handling\n        \"\"\"\n        self.is_dynamic = is_dynamic\n\n    def set_page_count(self, page_count: int) -&gt; None:\n        \"\"\"Set the page count to calculate token limits for thinking.\n\n        Each page is assumed to have DEFAULT_PAGE_TOKENS tokens (text + image).\n        Thinking budget is calculated as DEFAULT_THINKING_RATIO of the content tokens.\n\n        Args:\n            page_count (int): Number of pages in the document\n        \"\"\"\n        if page_count &lt;= 0:\n            raise ValueError(\"Page count must be a positive integer\")\n\n        self.page_count = page_count\n\n        # Calculate content tokens\n        content_tokens = min(page_count * self.DEFAULT_PAGE_TOKENS, self.MAX_TOKEN_LIMIT)\n\n        # Calculate thinking budget (1/3 of content tokens)\n        thinking_tokens = int(page_count * self.DEFAULT_PAGE_TOKENS * self.DEFAULT_THINKING_RATIO)\n\n        # Apply min/max constraints\n        thinking_tokens = max(thinking_tokens, self.MIN_THINKING_BUDGET)\n        thinking_tokens = min(thinking_tokens, self.MAX_THINKING_BUDGET)\n\n        # Update token limit and thinking budget\n        self.thinking_token_limit = content_tokens\n        self.thinking_budget = thinking_tokens\n\n    def request(\n        self,\n        messages: List[Dict[str, str]],\n        response_model: Optional[str] = None\n    ) -&gt; Any:\n        # Handle Pydantic-AI backend differently\n        if self.backend == LLMEngine.PYDANTIC_AI:\n            # Combine messages into a single prompt\n            combined_prompt = \" \".join([m[\"content\"] for m in messages])\n            try:\n                result = asyncio.run(\n                    self.agent.run(\n                        combined_prompt, \n                        result_type=response_model if response_model else str\n                    )\n                )\n                return result.data\n            except Exception as e:\n                raise ValueError(f\"Failed to extract from source: {str(e)}\")\n\n        # Uncomment the following lines if you need to calculate max_tokens\n        # contents = map(lambda message: message['content'], messages)\n        # all_contents = ' '.join(contents)\n        # max_tokens = num_tokens_from_string(all_contents)\n\n        # if is sync, response model is None if dynamic true and used for dynamic parsing after llm request\n        request_model = None if self.is_dynamic else response_model\n\n        # Add model structure and prompt engineering if dynamic parsing is enabled\n        working_messages = messages.copy()\n        if self.is_dynamic and response_model:\n            structure = add_classification_structure(response_model)\n            prompt = build_dynamic_prompt(structure)\n            working_messages.append({\n                \"role\": \"system\",\n                \"content\": prompt\n            })\n\n        # Use router or direct call based on thinking state\n        if self.router:\n            response = self._request_with_router(working_messages, request_model)\n        else:\n            response = self._request_direct(working_messages, request_model)\n\n        # If response_model is provided, return the response directly\n        if self.is_dynamic == False:\n            return response\n\n        # Otherwise get content and handle dynamic parsing if enabled\n        content = response.choices[0].message.content\n        if self.is_dynamic:\n            return extract_thinking_json(content, response_model)\n\n        return content\n\n    def _request_with_router(self, messages: List[Dict[str, str]], response_model: Optional[str]) -&gt; Any:\n        \"\"\"Handle request using router with or without thinking parameter\"\"\"\n        max_tokens = self._get_model_max_tokens()\n        if self.token_limit is not None:\n            max_tokens = min(self.token_limit, max_tokens)\n        elif self.is_thinking:\n            max_tokens = min(self.thinking_token_limit, max_tokens) if self.thinking_token_limit else max_tokens\n\n        params = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"response_model\": response_model,\n            \"temperature\": self.temperature,\n            \"timeout\": self.TIMEOUT,\n            \"max_completion_tokens\": max_tokens,\n        }\n        if self.is_thinking:\n            if litellm.supports_reasoning(self.model):\n                # Add thinking parameter for supported models\n                thinking_param = {\n                    \"type\": \"enabled\",\n                    \"budget_tokens\": self.thinking_budget\n                }\n                params[\"thinking\"] = thinking_param\n            else:\n                print(f\"Warning: Model {self.model} doesn't support thinking parameter, proceeding without it.\")\n\n        return self.router.completion(**params)\n\n    def _request_direct(self, messages: List[Dict[str, str]], response_model: Optional[str]) -&gt; Any:\n        \"\"\"Handle direct request with or without thinking parameter\"\"\"\n        max_tokens = self._get_model_max_tokens()\n        if self.token_limit is not None:\n            max_tokens = min(self.token_limit, max_tokens)\n        elif self.is_thinking:\n            max_tokens = min(self.thinking_token_limit, max_tokens) if self.thinking_token_limit else max_tokens\n\n        base_params = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"temperature\": self.temperature,\n            \"response_model\": response_model,\n            \"max_retries\": 1,\n            \"max_completion_tokens\": max_tokens,\n            \"timeout\": self.TIMEOUT,\n        }\n\n        if self.is_thinking:\n            if litellm.supports_reasoning(self.model):\n                # Try with thinking parameter\n                thinking_param = {\n                    \"type\": \"enabled\",\n                    \"budget_tokens\": self.thinking_budget\n                }\n                base_params[\"thinking\"] = thinking_param\n            else:\n                print(f\"Warning: Model {self.model} doesn't support thinking parameter, proceeding without it.\")\n\n        return self.client.chat.completions.create(**base_params)\n\n    def raw_completion(self, messages: List[Dict[str, str]]) -&gt; str:\n        \"\"\"Make raw completion request without response model.\"\"\"\n        if self.backend == LLMEngine.PYDANTIC_AI:\n            # Combine messages into a single prompt\n            combined_prompt = \" \".join([m[\"content\"] for m in messages])\n            try:\n                result = asyncio.run(\n                    self.agent.run(\n                        combined_prompt, \n                        result_type=str\n                    )\n                )\n                return result.data\n            except Exception as e:\n                raise ValueError(f\"Failed to extract from source: {str(e)}\")\n\n        max_tokens = self._get_model_max_tokens()\n        if self.token_limit is not None:\n            max_tokens = min(self.token_limit, max_tokens)\n        elif self.is_thinking:\n            max_tokens = min(self.thinking_token_limit, max_tokens) if self.thinking_token_limit else max_tokens\n\n        params = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"max_completion_tokens\": max_tokens,\n        }\n\n        if self.is_thinking:\n            if litellm.supports_reasoning(self.model):\n                # Add thinking parameter for supported models\n                thinking_param = {\n                    \"type\": \"enabled\",\n                    \"budget_tokens\": self.thinking_budget\n                }\n                params[\"thinking\"] = thinking_param\n            else:\n                print(f\"Warning: Model {self.model} doesn't support thinking parameter, proceeding without it.\")\n\n        if self.router:\n            raw_response = self.router.completion(**params)\n        else:\n            raw_response = litellm.completion(**params)\n\n        return raw_response.choices[0].message.content\n\n    def set_timeout(self, timeout_ms: int) -&gt; None:\n        \"\"\"Set the timeout value for LLM requests in milliseconds.\"\"\"\n        self.TIMEOUT = timeout_ms\n\n    def _get_model_max_tokens(self) -&gt; int:\n        \"\"\"Return the default maximum completion-token limit.\n\n        This constant (DEFAULT_MAX_COMPLETION_TOKENS) is meant to work for ~99 %\n        of models.  If you need a different value, supply `token_limit=` when\n        creating the `LLM` instance.\n        \"\"\"\n\n        return self.DEFAULT_MAX_COMPLETION_TOKENS\n</code></pre> <p>The architecture supports two different stacks:</p> <p>Default Stack: Combines instructor and litellm</p> <ul> <li>Uses instructor for structured outputs with Pydantic</li> <li>Leverages litellm for unified model interface</li> </ul> <p>Pydantic AI Stack \ud83e\uddea In Beta</p> <ul> <li>All-in-one solution for Pydantic model integration</li> <li>Handles both model interfacing and structured outputs</li> <li>Built by the Pydantic team (Learn more)</li> </ul>"},{"location":"core-concepts/llm-integration/#backend-options","title":"Backend Options","text":"<pre><code>from extract_thinker import LLM\nfrom extract_thinker.llm_engine import LLMEngine\n\n# Initialize with default stack (instructor + litellm)\nllm = LLM(\"gpt-4o\")\n\n# Or use Pydantic AI stack (Beta)\nllm = LLM(\"openai:gpt-4o\", backend=LLMEngine.PYDANTIC_AI)\n</code></pre> <p>ExtractThinker supports two LLM stacks:</p>"},{"location":"core-concepts/llm-integration/#default-stack-instructor-litellm","title":"Default Stack (instructor + litellm)","text":"<p>The default stack combines instructor for structured outputs and litellm for model interfacing. It leverages LiteLLM's unified API for consistent model access:</p> <pre><code>llm = LLM(\"gpt-4o\", backend=LLMEngine.DEFAULT)\n</code></pre>"},{"location":"core-concepts/llm-integration/#pydantic-ai-stack-beta","title":"Pydantic AI Stack (Beta)","text":"<p>An alternative all-in-one solution for model integration powered by Pydantic AI:</p> <pre><code>llm = LLM(\"openai:gpt-4o\", backend=LLMEngine.PYDANTIC_AI)\n</code></pre> <p>Pydantic AI Limitations</p> <ul> <li>Batch processing is not supported with the Pydantic AI backend</li> <li>Router functionality is not available</li> <li>Requires the <code>pydantic-ai</code> package to be installed</li> </ul> <p>Read more about Pydantic AI features</p>"},{"location":"core-concepts/llm-integration/#features","title":"Features","text":""},{"location":"core-concepts/llm-integration/#thinking-models","title":"Thinking Models","text":"<p>ExtractThinker's LLM integration includes support for \"thinking models\" that expose their reasoning process:</p> <pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"gpt-4o\")\n\n# Enable thinking mode\nllm.set_thinking(True)  # Automatically sets temperature to 1.0\n</code></pre> <p>Learn more about Thinking Models and how they improve extraction results.</p>"},{"location":"core-concepts/llm-integration/#router-support","title":"Router Support","text":"<p>ExtractThinker supports LiteLLM's router functionality for model fallbacks:</p> <pre><code>from extract_thinker import LLM\nfrom litellm import Router\n\n# Initialize router with fallbacks\nrouter = Router(\n    model_list=[\n        {\"model_name\": \"gpt-4o\", \"litellm_params\": {\"model\": \"gpt-4o\"}},\n        {\"model_name\": \"claude-3-opus-20240229\", \"litellm_params\": {\"model\": \"claude-3-opus-20240229\"}},\n    ],\n    fallbacks=[\n        {\"gpt-4o\": \"claude-3-opus-20240229\"}\n    ]\n)\n\n# Initialize LLM with router\nllm = LLM(\"gpt-4o\")\nllm.load_router(router)\n</code></pre> <p>This enables seamless fallbacks between different providers if a request fails.</p>"},{"location":"core-concepts/llm-integration/dynamic-parsing/","title":"Dynamic Parsing","text":"<p>Recommendation</p> <p>For most extraction tasks, we recommend using Thinking Models, which provide more comprehensive reasoning capabilities and broader provider support.</p> <p>Dynamic parsing enables flexible handling of structured outputs from LLM responses. This feature is particularly useful when reasoning models are used (e.g. DeepSeek R1).</p>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#overview","title":"Overview","text":"<p>The dynamic parsing feature can be enabled using the <code>set_dynamic()</code> method on your LLM instance. When enabled, the LLM will:</p> <ol> <li>Attempt to parse and validate JSON responses</li> <li>Include structured thinking process in the output</li> <li>Handle complex response models dynamically</li> </ol>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#usage","title":"Usage","text":""},{"location":"core-concepts/llm-integration/dynamic-parsing/#heres-how-to-enable-dynamic-parsing","title":"Here's how to enable dynamic parsing:","text":"<pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"ollama/deepseek-r1:1.5b\")\n\n# Enable dynamic parsing\nllm.set_dynamic(True)\n</code></pre>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#uses-this-template-structure","title":"Uses this template structure:","text":"<pre><code>Please provide your thinking process within &lt;think&gt; tags, followed by your JSON output.\n\nJSON structure:\n{your_structure}\n\nOUTPUT example:\n&lt;think&gt;\nYour step-by-step reasoning and analysis goes here...\n&lt;/think&gt;\n\n##JSON OUTPUT\n{\n    ...\n}\n</code></pre>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#example-invoice-extraction","title":"Example: Invoice Extraction","text":"<p>Here's a complete example of using dynamic parsing for invoice extraction:</p> <pre><code>from extract_thinker import LLM, Extractor\nfrom extract_thinker.document_loader import DocumentLoaderPyPdf\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n# Define your invoice model\nclass InvoiceData(BaseModel):\n    invoice_number: str\n    date: str\n    total_amount: float\n    vendor_name: str\n    line_items: List[dict]\n    payment_terms: Optional[str]\n\n# Initialize LLM with dynamic parsing\nllm = LLM(\"ollama/deepseek-r1:1.5b\")\nllm.set_dynamic(True)  # Enable dynamic JSON parsing\n\n# Setup document loader and extractor\ndocument_loader = DocumentLoaderPyPdf()\nextractor = Extractor(document_loader=document_loader, llm=llm)\n\n# Extract information from invoice\nresult = extractor.extract(\"path/to/invoice.pdf\", response_model=InvoiceData)\n</code></pre>"},{"location":"core-concepts/llm-integration/thinking-models/","title":"Thinking Models","text":"<p>Thinking models enable LLMs to expose their reasoning process when generating structured outputs. This feature is particularly valuable for complex extraction tasks and document understanding.</p>"},{"location":"core-concepts/llm-integration/thinking-models/#overview","title":"Overview","text":"<p>The thinking feature can be enabled using the <code>set_thinking()</code> method on your LLM instance. When enabled, the LLM will:</p> <ol> <li>Expose its step-by-step reasoning process within <code>&lt;think&gt;</code> tags</li> <li>Adjust token allocation to balance between reasoning and final output</li> <li>Automatically set optimal temperature (1.0) for creative reasoning</li> <li>Calculate appropriate thinking budgets based on document size</li> </ol>"},{"location":"core-concepts/llm-integration/thinking-models/#provider-support","title":"Provider Support","text":"<p>ExtractThinker supports thinking models across various providers:</p>"},{"location":"core-concepts/llm-integration/thinking-models/#openai","title":"OpenAI","text":"<p>OpenAI models support explicit thinking via the liteLLM integration. When enabled, the temperature is automatically set to 1.0 to encourage creative reasoning:</p> <pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"o3-mini\")\n\n# Enable thinking\nllm.set_thinking(True)  # Automatically sets temperature to 1.0\n</code></pre>"},{"location":"core-concepts/llm-integration/thinking-models/#claude","title":"Claude","text":"<p>Claude models have native support for thinking processes:</p> <pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"claude-3-7-sonnet-20250219\")\n\n# Enable thinking\nllm.set_thinking(True)\n</code></pre>"},{"location":"core-concepts/llm-integration/thinking-models/#automatic-budget-allocation","title":"Automatic Budget Allocation","text":"<p>A key advantage of thinking models is their automatic budget allocation for complex documents. When you enable thinking with <code>set_thinking(True)</code>, ExtractThinker automatically manages token allocation:</p> <pre><code>from extract_thinker import LLM, Extractor\nfrom extract_thinker.document_loader import DocumentLoaderPyPdf\n\n# Initialize components\nllm = LLM(\"claude-3-7-sonnet-20250219\")\ndocument_loader = DocumentLoaderPyPdf()\nextractor = Extractor(document_loader=document_loader, llm=llm)\n\n# Enable thinking with automatic budget management\nllm.set_thinking(True)\n\n# Extract information\nresult = extractor.extract(\"path/to/document.pdf\")\n</code></pre> <p>The budget calculation is done automatically based on document size: - Each page is estimated at <code>DEFAULT_PAGE_TOKENS</code> (1,500 tokens) - Thinking budget is set to <code>DEFAULT_THINKING_RATIO</code> (1/3) of content tokens - Budgets are capped between <code>MIN_THINKING_BUDGET</code> (1,200) and <code>MAX_THINKING_BUDGET</code> (64,000) - Total token limits respect the model's <code>MAX_TOKEN_LIMIT</code></p>"},{"location":"core-concepts/llm-integration/thinking-models/#deepseek","title":"DeepSeek","text":"<p>Reasoning-focused models like DeepSeek R1 work exceptionally well with thinking enabled:</p> <pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"ollama/deepseek-r1:1.5b\")\n\n# Enable thinking\nllm.set_thinking(True)\n</code></pre>"},{"location":"core-concepts/llm-integration/thinking-models/#thinking-output-format","title":"Thinking Output Format","text":"<p>When thinking is enabled, the LLM output includes a reasoning section within <code>&lt;think&gt;</code> tags:</p> <pre><code>&lt;think&gt;\nLet me analyze this invoice...\nI can see the invoice number at the top right: INV-2023-0042\nThe date appears to be January 15, 2024\nThe total amount is $1,234.56\n&lt;/think&gt;\n\n##JSON OUTPUT\n{\n  \"invoice_number\": \"INV-2023-0042\",\n  \"date\": \"2024-01-15\",\n  \"total_amount\": 1234.56\n}\n</code></pre>"},{"location":"core-concepts/markdown-conversion/","title":"Markdown Conversion","text":"<p>The <code>MarkdownConverter</code> class provides functionality to convert documents (including text and images) into Markdown format. It leverages a configured Language Model (LLM) for sophisticated conversion, especially when dealing with images or requiring structured output.</p>"},{"location":"core-concepts/markdown-conversion/#core-concepts","title":"Core Concepts","text":"<ul> <li>LLM Integration: The converter requires a configured LLM (<code>extract_thinker.llm.LLM</code>) to interpret document content and generate well-formatted Markdown. This is essential for both text and vision-based tasks (processing images) and for generating structured JSON output alongside Markdown.</li> <li>Document Loader: It relies on a <code>DocumentLoader</code> (<code>extract_thinker.document_loader.DocumentLoader</code>) to load the source document(s) and potentially extract text and images. The behavior might vary depending on the specific loader used.</li> <li>Vision Support: The <code>to_markdown</code> and <code>to_markdown_structured</code> methods have a <code>vision</code> parameter or operate in vision mode by default. When enabled, the converter attempts to process images within the document using the LLM's vision capabilities (if the LLM supports it).</li> <li>Structured Output: The <code>to_markdown_structured</code> method specifically instructs the LLM to provide not only the Markdown content but also a JSON structure breaking down the content with certainty scores. This method inherently requires vision capabilities in the LLM.</li> <li>Page Selection: Both <code>to_markdown</code> and <code>to_markdown_structured</code> methods support selective page processing through the <code>pages</code> parameter, allowing you to convert specific pages from multi-page documents.</li> </ul>"},{"location":"core-concepts/markdown-conversion/#initialization","title":"Initialization","text":"<pre><code>from extract_thinker.markdown import MarkdownConverter\nfrom extract_thinker.document_loader import DocumentLoaderPyPdf # Example loader\nfrom extract_thinker.llm import LLM\nfrom extract_thinker.global_models import get_lite_model, get_big_model # Helpers for model config\n\n# Initialize with or without components\nmarkdown_converter = MarkdownConverter()\n\n# Load components later\nloader = DocumentLoaderPyPdf() # Configure as needed\n# Use helper functions to get model configurations\n# Replace with your actual logic for selecting/configuring models if needed\nllm = LLM(get_lite_model()) \n\nmarkdown_converter.load_document_loader(loader)\nmarkdown_converter.load_llm(llm)\n\n# Or initialize directly\nmarkdown_converter = MarkdownConverter(document_loader=loader, llm=llm)\n</code></pre>"},{"location":"core-concepts/markdown-conversion/#usage","title":"Usage","text":""},{"location":"core-concepts/markdown-conversion/#simple-markdown-conversion-llm-required","title":"Simple Markdown Conversion (LLM Required)","text":"<p>This method uses the configured LLM to generate Markdown. If <code>vision=True</code>, it processes images (requires an LLM with vision capabilities). Note: An LLM must be configured via <code>load_llm()</code> or during initialization for this method to work.</p> <pre><code># Assuming markdown_converter is initialized with loader and LLM\nsource_path = \"path/to/your/document.pdf\" # Or image file like .png, .jpg\n\n# Convert with vision disabled (processes text only using LLM)\nmarkdown_pages_text = markdown_converter.to_markdown(source_path, vision=False) \n# Returns List[str]\n\n# Convert with vision enabled (processes text and images using LLM)\nmarkdown_pages_vision = markdown_converter.to_markdown(source_path, vision=True) \n# Returns List[str]\n\n# Convert specific pages (1-indexed)\nmarkdown_specific_pages = markdown_converter.to_markdown(source_path, vision=True, pages=[1, 3, 5])\n# Returns List[str] with only the specified pages\n\nfor i, page_md in enumerate(markdown_pages_vision):\n    print(f\"--- Page {i+1} ---\")\n    print(page_md)\n\n# Async version\nmarkdown_pages_vision_async = await markdown_converter.to_markdown_async(source_path, vision=True)\n# Async version with page selection\nmarkdown_specific_pages_async = await markdown_converter.to_markdown_async(source_path, vision=True, pages=[1, 3, 5])\n</code></pre>"},{"location":"core-concepts/markdown-conversion/#structured-markdown-conversion-llm-vision-required","title":"Structured Markdown Conversion (LLM Vision Required)","text":"<p>This method requires an LLM with vision capabilities and a document containing images. It returns structured data including Markdown and a JSON breakdown. Note: An LLM must be configured via <code>load_llm()</code> or during initialization for this method to work.</p> <p><pre><code>from extract_thinker.markdown import PageContent\n\n# Assuming markdown_converter is initialized with loader and LLM (with vision)\nimage_path = \"path/to/your/image.png\" \n\ntry:\n    # This method inherently uses vision\n    structured_output: List[PageContent] = markdown_converter.to_markdown_structured(image_path)\n    # Returns List[PageContent]\n\n    # Process specific pages (1-indexed)\n    structured_output_specific: List[PageContent] = markdown_converter.to_markdown_structured(\n        image_path, \n        pages=[1, 3, 5]\n    )\n    # Returns List[PageContent] with only the specified pages\n\n    for i, page_content in enumerate(structured_output):\n        print(f\"--- Page {i+1} ---\")\n        # Access structured items\n        for item in page_content.items:\n            print(f\"Certainty: {item.certainty}, Content: {item.content[:50]}...\") # Print snippet\n\nexcept ValueError as e:\n    print(f\"Error: {e}\") # e.g., if no images found or LLM not set\n\n# Async version\nstructured_output_async: List[PageContent] = await markdown_converter.to_markdown_structured_async(image_path)\n# Async version with page selection\nstructured_output_specific_async: List[PageContent] = await markdown_converter.to_markdown_structured_async(\n    image_path, \n    pages=[1, 3, 5]\n)\n</code></pre> Note: The <code>to_markdown_structured</code> method expects the LLM to return both Markdown and a specific JSON format. The <code>extract_thinking_json</code> utility is used internally to parse this.</p>"},{"location":"core-concepts/markdown-conversion/#prompts","title":"Prompts","text":"<p>The converter uses specific system prompts depending on the method called: - <code>DEFAULT_PAGE_PROMPT</code>: Used by <code>to_markdown_structured</code>. Instructs the LLM to output Markdown and a JSON structure. - <code>DEFAULT_MARKDOWN_PROMPT</code>: Used by <code>to_markdown</code> (when using LLM). Instructs the LLM to output only well-formatted Markdown. - <code>MARKDOWN_VERIFICATION_PROMPT</code>: Potentially used for refining existing text (internal flag <code>allow_verification</code>).</p> <p>These prompts guide the LLM's output format.</p>"},{"location":"core-concepts/process/","title":"Process","text":"<p>Process is a component that orchestrates the document processing workflow, allowing you to combine multiple DocumentLoaders and Extractors for complex document processing tasks.</p> <p>The workflow consists of: <pre><code>split_content = process.load_file(path)\\\n    .split(classifications)\\\n    .extract()\n</code></pre></p> <p>This creates a pipeline that:</p> <ol> <li> <p>Loads the document</p> </li> <li> <p>Splits it into logical sections</p> </li> <li> <p>Extracts structured data from each section</p> </li> </ol> Process Implementation <pre><code>import asyncio\nfrom typing import IO, Any, Dict, List, Optional, Union\nfrom extract_thinker.image_splitter import ImageSplitter\nfrom extract_thinker.models.classification_response import ClassificationResponse\nfrom extract_thinker.models.classification_strategy import ClassificationStrategy\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\nfrom extract_thinker.models.splitting_strategy import SplittingStrategy\nfrom extract_thinker.extractor import Extractor\nfrom extract_thinker.models.classification import Classification\nfrom extract_thinker.document_loader.document_loader import DocumentLoader\nfrom extract_thinker.models.classification_tree import ClassificationTree\nfrom extract_thinker.splitter import Splitter\nfrom extract_thinker.models.doc_groups import (\n    DocGroups,\n)\nfrom extract_thinker.utils import get_image_type\n\nclass Process:\n    def __init__(self):\n        # self.extractors: List[Extractor] = []\n        self.doc_groups: Optional[DocGroups] = None\n        self.split_classifications: List[Classification] = []\n        self.extractor_groups: List[List[Extractor]] = []  # for classication\n        self.document_loaders_by_file_type: Dict[str, DocumentLoader] = {}\n        self.document_loader: Optional[DocumentLoader] = None\n        self.file_path: Optional[str] = None\n        self.file_stream: Optional[IO] = None\n        self.splitter: Optional[Splitter] = None\n        self._content_loaded: bool = False  # New internal flag\n\n    def set_document_loader_for_file_type(self, file_type: str, document_loader: DocumentLoader):\n        if self.document_loader is not None:\n            raise ValueError(\"Cannot set a document loader for a specific file type when a default loader is already set.\")\n        self.document_loaders_by_file_type[file_type] = document_loader\n\n    def load_document_loader(self, document_loader: DocumentLoader):\n        if self.document_loaders_by_file_type:\n            raise ValueError(\"Cannot set a default document loader when specific loaders are already set.\")\n        self.document_loader = document_loader\n        return self\n\n    def load_splitter(self, splitter: Splitter):\n        \"\"\"\n        Load a splitter and configure vision mode if needed.\n\n        Args:\n            splitter: The splitter instance to use\n        Returns:\n            self for method chaining\n        \"\"\"\n        self.splitter = splitter\n\n        # Check if the splitter is an ImageSplitter\n        is_vision = isinstance(splitter, ImageSplitter)\n\n        # Configure vision mode for any loaded document loaders\n        if self.document_loader:\n            self.document_loader.set_vision_mode(is_vision)\n\n        for loader in self.document_loaders_by_file_type.values():\n            loader.set_vision_mode(is_vision)\n\n        return self\n\n    def add_classify_extractor(self, extractor_groups: List[List[Extractor]]):\n        for extractors in extractor_groups:\n            self.extractor_groups.append(extractors)\n        return self\n\n    async def _classify_async(self, extractor: Extractor, file: str, classifications: List[Classification], image: bool = False):\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, extractor.classify, file, classifications, image)\n\n    def classify(self, file: str, classifications, strategy: ClassificationStrategy = ClassificationStrategy.CONSENSUS, threshold: int = 9, image: bool = False) -&gt; Optional[Classification]:\n        if not isinstance(threshold, int) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be an integer between 1 and 10\")\n\n        result = asyncio.run(self.classify_async(file, classifications, strategy, threshold, image))\n        return result\n\n    async def classify_async(\n        self,\n        file: str,\n        classifications: Union[List[Classification], ClassificationTree],\n        strategy: ClassificationStrategy = ClassificationStrategy.CONSENSUS,\n        threshold: int = 9,\n        image: str = False\n    ) -&gt; Optional[Classification]:\n        if not isinstance(threshold, int) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be an integer between 1 and 10\")\n\n        if isinstance(classifications, ClassificationTree):\n            return await self._classify_tree_async(file, classifications, threshold, image)\n\n        # Try each layer of extractors until we get a valid result\n        for extractor_group in self.extractor_groups:\n            group_classifications = await asyncio.gather(*(\n                self._classify_async(extractor, file, classifications, image) \n                for extractor in extractor_group\n            ))\n\n            try:\n                # Attempt to get result based on strategy\n                if strategy == ClassificationStrategy.CONSENSUS:\n                    if len(set(c.name for c in group_classifications)) == 1:\n                        return group_classifications[0]\n\n                elif strategy == ClassificationStrategy.HIGHER_ORDER:\n                    return max(group_classifications, key=lambda c: c.confidence)\n\n                elif strategy == ClassificationStrategy.CONSENSUS_WITH_THRESHOLD:\n                    if len(set(c.name for c in group_classifications)) == 1:\n                        if all(c.confidence &gt;= threshold for c in group_classifications):\n                            return group_classifications[0]\n\n                # If we get here, current layer didn't meet criteria - continue to next layer\n                continue\n\n            except Exception as e:\n                # If there's an error processing this layer, try the next one\n                print(f\"Layer failed with error: {str(e)}\")\n                continue\n\n        # If we've tried all layers and none worked\n        raise ValueError(\"No consensus could be reached on the classification of the document across any layer. Please try again with a different strategy or threshold.\")\n\n    async def _classify_tree_async(\n        self, \n        file: str, \n        classification_tree: ClassificationTree, \n        threshold: float,\n        image: bool\n    ) -&gt; Optional[ClassificationResponse]:\n        if not isinstance(threshold, (int, float)) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be a number between 1 and 10\")\n\n        \"\"\"\n        Perform classification in a hierarchical, level-by-level approach.\n        \"\"\"\n        best_classification = None\n        current_nodes = classification_tree.nodes\n\n        while current_nodes:\n            # Get the list of classifications at the current level\n            classifications = [node.classification for node in current_nodes]\n\n            # Classify among the current level's classifications\n            classification_response = await self._classify_async(\n                extractor=self.extractor_groups[0][0],\n                file=file, \n                classifications=classifications, \n                image=image\n            )\n\n            # Handle cases where classification fails at this level\n            if classification_response is None:\n                raise ValueError(\n                    \"Classification failed at the current level. Could not determine a match.\"\n                )\n\n            if classification_response.confidence &lt; threshold:\n                raise ValueError(\n                    f\"Classification confidence {classification_response.confidence} \"\n                    f\"for '{classification_response.name}' is below the threshold of {threshold}.\"\n                )\n\n            best_classification = classification_response\n\n            # Use UUID for robust matching instead of name\n            matching_node = next(\n                (\n                    node for node in current_nodes\n                    if node.classification.uuid == best_classification.classification.uuid\n                ),\n                None\n            )\n\n            if matching_node is None:\n                raise ValueError(\n                    f\"No matching node found for classification '{classification_response.name}'.\"\n                )\n\n            if matching_node.children:\n                current_nodes = matching_node.children\n            else:\n                break\n\n        return best_classification if best_classification else None\n\n    async def classify_extractor(self, session, extractor, file):\n        return await session.run(extractor.classify, file)\n\n    # check if there is only the default one, if not, get from the file type. if none is present, raise an error\n    def get_document_loader(self, file):\n        if self.document_loader is not None:\n            return self.document_loader\n\n        filetype = get_image_type(file)\n        return self.document_loaders_by_file_type.get(filetype, None)\n\n    def load_file(self, file):\n        self.file_path = file\n        return self\n\n    def split(self, classifications: List[Classification], strategy: SplittingStrategy = SplittingStrategy.EAGER):\n        \"\"\"Split the document into groups based on classifications.\"\"\"\n        if self.splitter is None:\n            raise ValueError(\"No splitter loaded. Please load a splitter using load_splitter() before splitting.\")\n\n        self.split_classifications = classifications\n\n        document_loader = self.get_document_loader(self.file_path)\n        if document_loader is None:\n            raise ValueError(\"No suitable document loader found for file type\")\n\n        # Load content using the new unified load() method\n        if self.file_path:\n            pages = document_loader.load(self.file_path)\n        elif self.file_stream:\n            pages = document_loader.load(self.file_stream)\n        else:\n            raise ValueError(\"No file or stream available\")\n\n        if len(pages) &lt; 2:\n            raise ValueError(\"Document must have at least 2 pages\")\n\n        # Process based on strategy\n        if strategy == SplittingStrategy.EAGER:\n            eager_group = self.splitter.split_eager_doc_group(pages, classifications)\n            self.doc_groups = eager_group\n        else:  # LAZY strategy\n            if document_loader.can_handle_paginate(self.file_path):\n                processed_groups = self.splitter.split_lazy_doc_group(pages, classifications)\n                self.doc_groups = processed_groups.doc_groups\n            else:\n                raise ValueError(\"Document Type does not support lazy splitting. for now only pdf is supported\")\n\n        return self\n\n    def extract(self, \n                vision: bool = False,\n                completion_strategy: Optional[CompletionStrategy] = CompletionStrategy.FORBIDDEN) -&gt; List[Any]:\n        \"\"\"Extract information from the document groups.\"\"\"\n        if self.doc_groups is None:\n            raise ValueError(\"Document groups have not been initialized\")\n\n        async def _extract(doc_group):\n            # Find matching classification and extractor\n            classificationStr = doc_group.classification\n            extractor: Optional[Extractor] = None\n            contract = None\n\n            for classification in self.split_classifications:\n                if classification.name == classificationStr:\n                    extractor = classification.extractor\n                    # If an extraction_contract is provided, use it; otherwise, use the default contract\n                    contract = classification.extraction_contract or classification.contract\n                    break\n\n            if extractor is None:\n                raise ValueError(\"Extractor not found for classification\")\n\n            # Get document loader\n            document_loader = self.get_document_loader(self.file_path)\n            if document_loader is None:\n                raise ValueError(\"No suitable document loader found for file type\")\n\n            # Load content using the new unified load() method\n            if self.file_path:\n                pages = document_loader.load(self.file_path)\n            elif self.file_stream:\n                pages = document_loader.load(self.file_stream)\n            else:\n                raise ValueError(\"No file or stream available\")\n\n            # Get pages for this group\n            group_pages = [pages[i - 1] for i in doc_group.pages]\n\n            # Set flag to skip loading since content is already processed\n            extractor.set_skip_loading(True)\n            try:\n                result = await extractor.extract_async(\n                    source=group_pages,\n                    response_model=contract,\n                    vision=vision,\n                    content=None,\n                    completion_strategy=completion_strategy\n                )\n            finally:\n                # Reset flag after extraction\n                extractor.set_skip_loading(False)\n\n            return result\n\n        async def process_doc_groups(groups: List[Any]) -&gt; List[Any]:\n            tasks = [_extract(group) for group in groups]\n            try:\n                processedGroups = await asyncio.gather(*tasks)\n                return processedGroups\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n                raise\n\n        loop = asyncio.get_event_loop()\n        processedGroups = loop.run_until_complete(\n            process_doc_groups(self.doc_groups)\n        )\n\n        return processedGroups\n</code></pre>"},{"location":"core-concepts/process/#using-multiple-documentloaders","title":"Using Multiple DocumentLoaders","text":"<p>You can configure different DocumentLoaders for specific file types:</p> <pre><code>from extract_thinker import Process\nfrom extract_thinker.document_loader import (\n    DocumentLoaderTesseract,\n    DocumentLoaderPyPdf,\n    DocumentLoaderAzureForm\n)\n\nprocess = Process()\n\n# Set loaders for specific file types\nprocess.set_document_loader_for_file_type(\n    \"pdf\", DocumentLoaderPyPdf()\n)\nprocess.set_document_loader_for_file_type(\n    \"png\", DocumentLoaderTesseract(tesseract_path)\n)\n\n# Or set a default loader\nprocess.load_document_loader(\n    DocumentLoaderAzureForm(subscription_key, endpoint)\n)\n</code></pre>"},{"location":"core-concepts/process/#using-multiple-extractors","title":"Using Multiple Extractors","text":"<p>You can use multiple extractors for different document types or processing stages:</p> <pre><code>from extract_thinker import Extractor, Classification\n\n# Initialize extractors with different models\ngpt4_extractor = Extractor(document_loader)\ngpt4_extractor.load_llm(\"gpt-4o\")\n\nclaude_extractor = Extractor(document_loader)\nclaude_extractor.load_llm(\"claude-3-haiku-20240307\")\n\n# Create classifications with specific extractors\nclassifications = [\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,\n        extractor=gpt4_extractor\n    ),\n    Classification(\n        name=\"License\",\n        description=\"This is a license\",\n        contract=LicenseContract,\n        extractor=claude_extractor\n    )\n]\n\n# Process will use the appropriate extractor for each document type\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications)\\\n    .extract()\n</code></pre>"},{"location":"core-concepts/splitters/","title":"Splitters","text":"<p>In document processing, splitting enables the separation of individual documents or sections within a combined file. This task is especially crucial when handling batches of documents where different parts may need distinct processing, and always with Sonnet. This can be done with two strategies: Eager and Lazy.</p>"},{"location":"core-concepts/splitters/#page-level-processing","title":"Page-Level Processing","text":"<p>Splitters work at the page level, determining which pages belong together as a single document. For example:</p> <ul> <li> <p>A 10-page PDF might contain three separate invoices</p> </li> <li> <p>A scanned document might contain multiple forms</p> </li> <li> <p>A batch of documents might need to be separated by document type</p> </li> </ul> <p>The challenge is determining where one document ends and another begins, which is where our splitting strategies come in.</p>"},{"location":"core-concepts/splitters/#eager-vs-lazy-approaches","title":"Eager vs. Lazy Approaches","text":"<p>Eager and Lazy splitting have distinct use cases based on document size and the complexity of relationships between pages.</p>"},{"location":"core-concepts/splitters/#eager-splitting","title":"Eager Splitting","text":"<p>Eager splitting processes all pages in a single pass, identifying and dividing all sections at once. It's efficient for small to medium-sized documents where context size does not limit performance.</p> <pre><code>from extract_thinker import Splitter, SplittingStrategy\n\nsplitter = Splitter()\nresult = splitter.split(\n    document,\n    strategy=SplittingStrategy.EAGER\n)\n</code></pre> <p>Benefits of Eager Splitting: - Speed: Faster processing since all split points are determined upfront - Simplicity: Ideal for documents that fit entirely within the model's context window - Consistency: Better for documents where relationships between pages are important</p>"},{"location":"core-concepts/splitters/#lazy-splitting","title":"Lazy Splitting","text":"<p>Lazy splitting processes pages incrementally in chunks, assessing smaller groups of pages at a time to decide if they belong together. In this use case, groups of two pages are processed and checked for continuity, allowing it to scale efficiently for larger documents.</p> <pre><code>result = splitter.split(\n    document,\n    strategy=SplittingStrategy.LAZY\n)\n</code></pre> <p>Benefits of Lazy Splitting: - Scalability: Well-suited for documents that exceed the model's context window - Memory Efficiency: Processes only what's needed when needed - Flexibility: Better for streaming or real-time processing</p> Base Splitter Implementation <p>The base Splitter class provides both eager and lazy implementations: <pre><code>import asyncio\nfrom typing import Any, List\nfrom abc import ABC, abstractmethod\n\nfrom extract_thinker.models.classification import Classification\nfrom extract_thinker.models.doc_group import DocGroups, DocGroup\nfrom extract_thinker.models.doc_groups2 import DocGroups2\nfrom extract_thinker.models.eager_doc_group import EagerDocGroup\n\n\nclass Splitter(ABC):\n    @abstractmethod\n    def belongs_to_same_document(self, page1: Any, page2: Any, contract: str) -&gt; DocGroups2:\n        pass\n\n    @abstractmethod\n    def split_lazy_doc_group(self, lazy_doc_group: List[Any], classifications: List[Classification]) -&gt; DocGroups:\n        pass\n\n    @abstractmethod\n    def split_eager_doc_group(self, lazy_doc_group: List[Any], classifications: List[Classification]) -&gt; DocGroups:\n        pass\n\n    def split_document_into_groups(self, document: List[Any]) -&gt; List[List[Any]]:\n        page_per_split = 2\n        split = []\n        if len(document) == 1:\n            return [document]\n        for i in range(0, len(document) - 1):\n            group = document[i: i + page_per_split]\n            split.append(group)\n        return split\n\n    async def process_split_groups(self, split: List[List[Any]], contract: str) -&gt; List[DocGroups2]:\n        # Create asynchronous tasks for processing each group\n        tasks = [self.process_group(x, contract) for x in split]\n        try:\n            # Execute all tasks concurrently and wait for all to complete\n            doc_groups = await asyncio.gather(*tasks)\n            return doc_groups\n        except Exception as e:\n            # Handle possible exceptions that might occur during task execution\n            print(f\"An error occurred: {e}\")\n            raise\n\n    async def process_group(self, group: List[Any], contract: str) -&gt; DocGroups2:\n        page2 = group[1] if len(group) &gt; 1 else None\n        return self.belongs_to_same_document(group[0], page2, contract)\n\n    def aggregate_doc_groups(self, doc_groups_tasks: List[DocGroups2]) -&gt; DocGroups:\n        \"\"\"\n        Aggregate the results from belongs_to_same_document comparisons into final document groups.\n        This is the base implementation that can be used by all splitter implementations.\n        \"\"\"\n        doc_groups = DocGroups()\n        current_group = DocGroup(pages=[], classification=\"\")\n        page_number = 1\n\n        if not doc_groups_tasks:\n            return doc_groups\n\n        # Handle the first group\n        doc_group = doc_groups_tasks[0]\n        if doc_group.belongs_to_same_document:\n            current_group.pages = [1, 2]\n            current_group.classification = doc_group.classification_page1\n        else:\n            # First page is its own document\n            current_group.pages = [1]\n            current_group.classification = doc_group.classification_page1\n            doc_groups.doc_groups.append(current_group)\n\n            # Start new group with second page\n            current_group = DocGroup(pages=[2], classification=doc_group.classification_page2)\n\n        page_number += 1\n\n        # Process remaining groups\n        for doc_group in doc_groups_tasks[1:]:\n            if doc_group.belongs_to_same_document:\n                current_group.pages.append(page_number + 1)\n            else:\n                doc_groups.doc_groups.append(current_group)\n                current_group = DocGroup(\n                    pages=[page_number + 1],\n                    classification=doc_group.classification_page2\n                )\n            page_number += 1\n\n        # Add the last group\n        doc_groups.doc_groups.append(current_group)\n\n        return doc_groups\n</code></pre></p>"},{"location":"core-concepts/splitters/#available-splitters","title":"Available Splitters","text":"<p>ExtractThinker provides two main splitter implementations:</p> <ul> <li>Text Splitter: For text-based document splitting</li> <li>Image Splitter: For image-based document splitting</li> </ul>"},{"location":"core-concepts/splitters/#recommended-approach","title":"Recommended Approach","text":"<p>For most IDP use cases, Eager Splitting is appropriate since it offers: - Simpler implementation - Better handling of page relationships - Faster processing for typical document sizes (under 50 pages)</p> <p>However, consider Lazy Splitting when: - Processing very large documents (50+ pages) - Working with limited memory - Handling streaming document inputs</p>"},{"location":"core-concepts/splitters/#best-practices","title":"Best Practices","text":"<ul> <li>Choose strategy based on document size and page count</li> <li>Consider context window limitations of your LLM</li> </ul>"},{"location":"core-concepts/splitters/image/","title":"Image Splitter","text":"<p>Image Splitter is specialized for handling image-based document splitting by analyzing visual consistency and layout patterns between pages.</p>"},{"location":"core-concepts/splitters/image/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import ImageSplitter, Process, SplittingStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Initialize process and loader\nprocess = Process()\nprocess.load_document_loader(DocumentLoaderTesseract(tesseract_path))\n\n# Initialize image splitter with vision model\nprocess.load_splitter(ImageSplitter(\"claude-3-5-sonnet-20241022\"))\n\n# Split document\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications, strategy=SplittingStrategy.EAGER)\\\n    .extract()\n</code></pre>"},{"location":"core-concepts/splitters/text/","title":"Text Splitter","text":"<p>Text Splitter is designed to handle text-based document splitting by analyzing content continuity and relationships between pages.</p>"},{"location":"core-concepts/splitters/text/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import TextSplitter, Process, SplittingStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Initialize process and loader\nprocess = Process()\nprocess.load_document_loader(DocumentLoaderTesseract(tesseract_path))\n\n# Initialize text splitter with model\nprocess.load_splitter(TextSplitter(\"claude-3-5-sonnet-20241022\"))\n\n# Split document\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications, strategy=SplittingStrategy.EAGER)\\\n    .extract()\n</code></pre>"},{"location":"examples/aws-stack/","title":"AWS Textract with Claude Example","text":"<p>This guide demonstrates how to use AWS Textract combined with Claude for powerful document processing.</p>"},{"location":"examples/aws-stack/#basic-setup","title":"Basic Setup","text":"<p>Here's how to combine AWS Textract's OCR capabilities with Claude:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderTextract\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize AWS Textract\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderTextract(\n        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=os.getenv(\"AWS_REGION\")\n    )\n)\n\n# Configure Claude\nllm = LLM(\n    \"anthropic/claude-3-haiku-20240307\",\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n)\nextractor.load_llm(llm)\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/aws-stack/#cost-optimization","title":"Cost Optimization","text":"<p>AWS Textract pricing:</p> <ul> <li> <p>Basic text detection: $1.50 per 1,000 pages (first 1M pages/month)                        $0.60 per 1,000 pages (over 1M pages/month)</p> </li> <li> <p>Tables: $15.00 per 1,000 pages (first 1M pages/month)          $10.00 per 1,000 pages (over 1M pages/month)</p> </li> </ul> <p>Combined with Claude (via AWS Bedrock):</p> <ul> <li> <p>Claude 3 Haiku: $0.00025 per 1K input tokens, $0.00125 per 1K output tokens</p> </li> <li> <p>Claude 3 Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens</p> </li> </ul> <p>Approximate cost per page (first 1M pages/month): - Basic text only: $0.0015 per page ($1.50/1000) - With tables: $0.0165 per page ($1.50/1000 + $15.00/1000) - Plus Claude costs (varies by token length and model choice)</p>"},{"location":"examples/azure-stack/","title":"Azure Document Intelligence Example","text":"<p>This guide demonstrates how to use Azure Document Intelligence with Phi-3 models for efficient document processing.</p>"},{"location":"examples/azure-stack/#basic-setup","title":"Basic Setup","text":"<p>Here's a complete example using Azure Document Intelligence with Phi-3:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderAzureForm\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize Azure Document Intelligence\nsubscription_key = os.getenv(\"AZURE_SUBSCRIPTION_KEY\")\nendpoint = os.getenv(\"AZURE_ENDPOINT\")\napi_key = os.getenv(\"AZURE_AI_API_KEY\")\n\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderAzureForm(subscription_key, endpoint)\n)\n\n# Configure environment variables for Azure\nimport os\nos.environ[\"AZURE_API_KEY\"] = api_key\nos.environ[\"AZURE_API_BASE\"] = \"https://your-endpoint.inference.ai.azure.com\"\nos.environ[\"AZURE_API_VERSION\"] = \"v1\"\n\n# Configure Phi-3 mini model\nextractor.load_llm(\"azure/Phi-3-mini-128k-instruct\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/azure-stack/#cost-optimization","title":"Cost Optimization","text":"<p>Azure Document Intelligence offers different pricing tiers:</p> <ul> <li>Read: Basic OCR functionality ($0.001 per page)</li> <li>Prebuilt Layout: Structure detection ($0.01 per page)</li> <li>Custom Layout: Higher cost (discouraged)</li> </ul>"},{"location":"examples/azure-stack/#best-practices","title":"Best Practices","text":"<p>Document Type Selection</p> <ul> <li>Use \"prebuilt-layout\" when vision is not available</li> <li>Use \"read\" for most of the documents</li> <li>Consider custom layout only for specific needs, like signatures</li> </ul>"},{"location":"examples/google-stack/","title":"Google Document AI Example","text":"<p>This guide shows how to use Google Document AI for advanced document processing with ExtractThinker, including integration with Gemini models for enhanced extraction capabilities.</p>"},{"location":"examples/google-stack/#overview","title":"Overview","text":"<p>Google Document AI is a powerful solution that provides:</p> <ul> <li>OCR and structural parsing</li> <li>Classification capabilities</li> <li>Specialized domain extractors (invoices, W2 forms, bank statements, etc.)</li> <li>Layout parsing and form processing</li> </ul>"},{"location":"examples/google-stack/#basic-setup","title":"Basic Setup","text":"<p>First, install the required dependencies:</p> <pre><code>pip install extract-thinker google-cloud-documentai\n</code></pre> <p>Here's how to use Google Document AI with ExtractThinker:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.document_loader import DocumentLoaderGoogleDocumentAI\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceLineItem(Contract):\n    description: str = Field(description=\"Description of the item\")\n    quantity: int = Field(description=\"Quantity of items purchased\")\n    unit_price: float = Field(description=\"Price per unit\")\n    amount: float = Field(description=\"Total amount for this line\")\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(description=\"Unique invoice identifier\")\n    invoice_date: str = Field(description=\"Date of the invoice\")\n    total_amount: float = Field(description=\"Overall total amount\")\n    line_items: List[InvoiceLineItem] = Field(description=\"List of items in this invoice\")\n\n# Initialize Google Document AI\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderGoogleDocumentAI(\n        project_id=os.getenv(\"DOCUMENTAI_PROJECT_ID\"),\n        location=os.getenv(\"DOCUMENTAI_LOCATION\"),  # 'us' or 'eu'\n        processor_id=os.getenv(\"DOCUMENTAI_PROCESSOR_ID\"),\n        credentials=os.getenv(\"DOCUMENTAI_GOOGLE_CREDENTIALS\")\n    )\n)\n\n# Configure Gemini model (recommended for enhanced extraction)\nextractor.load_llm(\"vertex_ai/gemini-2.0-flash-exp\")\n\n# Process document\nresult = extractor.extract(\n    source=\"invoice.pdf\",\n    response_model=InvoiceContract,\n    vision=True  # Enable vision mode for better results with Gemini\n)\n</code></pre>"},{"location":"examples/google-stack/#document-splitting","title":"Document Splitting","text":"<p>ExtractThinker provides powerful document splitting capabilities that can be used with Google Document AI. Here's how to implement document splitting:</p> <pre><code>from extract_thinker.process import Process\nfrom extract_thinker.splitter import SplittingStrategy\nfrom extract_thinker.image_splitter import ImageSplitter\n\n# Create a Process instance\nprocess = Process()\n\n# Configure the splitter with Gemini model\nimage_splitter = ImageSplitter(model=\"vertex_ai/gemini-2.0-flash-exp\")\nprocess.load_splitter(image_splitter)\n\n# Define your classifications (e.g., Invoice, Driver License)\nmy_classifications = [invoice_class, driver_license_class]\n\n# Process a combined document with EAGER strategy\nBULK_DOC_PATH = \"path/to/combined_documents.pdf\"\n\nresult = (process.load_file(BULK_DOC_PATH)\n    .split(my_classifications, strategy=SplittingStrategy.EAGER)\n    .extract(vision=True))\n\n# Process results\nfor doc_content in result:\n    print(f\"Document Type: {type(doc_content).__name__}\")\n    print(doc_content.json(indent=2))\n</code></pre> <p>More information about document splitting can be found in the document splitting section.</p> <p>Document OCR: Basic text extraction and layout analysis</p> <ul> <li>Best paired with vision-enabled models like Gemini</li> <li>Most cost-effective for basic OCR needs</li> </ul> <p>Layout Parser: Advanced structural analysis</p> <ul> <li>Use when vision capabilities aren't available</li> <li>Provides detailed document structure information</li> </ul> <p>Specialized Processors: Domain-specific extraction</p> <ul> <li>Invoice Parser</li> <li>Form Parser</li> <li>US Driver License Parser</li> <li>And more...</li> </ul>"},{"location":"examples/google-stack/#cost-optimization","title":"Cost Optimization","text":""},{"location":"examples/google-stack/#document-ai-pricing-as-of-2024","title":"Document AI Pricing (as of 2024)","text":"<p>Document OCR: $1.50 per 1,000 pages</p> <ul> <li>Volume discounts after 5M pages/month</li> <li>Most cost-effective for basic OCR needs</li> </ul> <p>Layout Parser: $10 per 1,000 pages</p> <ul> <li>Good for structural analysis without vision models</li> </ul> <p>Form Parser and Custom Extractors: $30 per 1,000 pages</p> <ul> <li>Volume discounts after 1M pages/month</li> <li>Best for complex form processing</li> </ul> <p>Specialized Processors: Varies by type</p> <ul> <li>Example: Invoice parsing at $0.10 per 10 pages</li> <li>Includes pre-trained field extraction</li> </ul>"},{"location":"examples/google-stack/#cost-effective-strategies","title":"Cost-Effective Strategies","text":"<p>Basic OCR + Gemini:</p> <ul> <li>Use Document OCR ($0.0015/page)</li> <li>Combine with Gemini 2.0 Flash (~$0.0002/page)</li> <li>Total: ~$0.0017/page</li> </ul> <p>Layout Parser + LLM:</p> <ul> <li>Use Layout Parser ($0.01/page)</li> <li>Add LLM processing (~$0.0002/page)</li> <li>Total: ~$0.0102/page</li> </ul> <p>Pure LLM Approach:</p> <ul> <li>Use Gemini's vision capabilities directly</li> <li>Cost: ~$0.0002/page</li> <li>Note: May have lower accuracy for complex documents</li> </ul>"},{"location":"examples/google-stack/#supported-formats","title":"Supported Formats","text":"<ul> <li><code>PDF</code> (up to 2000 pages or 20MB)</li> <li>Images: <code>JPEG</code>, <code>PNG</code>, <code>TIFF</code>, <code>GIF</code></li> <li>Office formats: <code>DOCX</code>, <code>XLSX</code>, <code>PPTX</code></li> <li>Web: <code>HTML</code></li> </ul> <p>For more examples and implementation details, check out the ExtractThinker repository or the related article on Medium.</p>"},{"location":"examples/groq-processing/","title":"Processing Documents with Groq","text":"<p>\u26a0\ufe0f Warning: Vision-based processing may not be available with Groq models. For image or document processing that requires vision capabilities, consider using other providers like Google Document AI, Azure Document Intelligence, or AWS Textract.</p> <p>This guide demonstrates how to process documents using Groq's powerful LLMs.</p>"},{"location":"examples/groq-processing/#basic-setup","title":"Basic Setup","text":"<p>Here's a basic example of document extraction using Groq:</p> <pre><code>from extract_thinker import Extractor\nfrom extract_thinker.document_loader.document_loader_pypdf import DocumentLoaderPyPdf\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    lines: List[LineItem] = Field(\"List of line items in the invoice\")\n\n# Initialize extractor with PyPDF loader\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\n\n# Configure Groq\nextractor.load_llm(\"groq/llama-3.2-11b-vision-preview\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/groq-processing/#classification-example","title":"Classification Example","text":"<p>You can also use Groq for document classification:</p> <pre><code>from extract_thinker import Process\nfrom extract_thinker.models.classification import Classification\n\n# Setup process\nprocess = Process()\nprocess.add_classify_extractor([[extractor]])\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice document\", \n        contract=InvoiceContract\n    ),\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license document\",\n        contract=DriverLicense\n    )\n]\n\n# Classify document\nresult = process.classify(\"document.pdf\", classifications)\n</code></pre> <p>Benefits - High Performance</p> <ul> <li>Fast inference times</li> <li>State-of-the-art language models</li> </ul>"},{"location":"examples/local-processing/","title":"Local Processing with Ollama and Tesseract","text":"<p>This guide demonstrates how to process documents locally using Tesseract OCR and Ollama.</p>"},{"location":"examples/local-processing/#basic-setup","title":"Basic Setup","text":"<p>Here's how to use Tesseract with Ollama:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderTesseract\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize Tesseract\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderTesseract(os.getenv(\"TESSERACT_PATH\"))\n)\n\nos.environ[\"API_BASE\"] = \"http://localhost:11434\"\n\n# Configure Ollama\nextractor.load_llm(\"ollama/phi3\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre> <p>Benefits - Privacy &amp; Security</p> <ul> <li>All processing done locally</li> <li>No data leaves your network</li> <li>Complete control over data</li> </ul>"},{"location":"examples/resume-processing/","title":"Resume Processing Example","text":"<p>ExtractThinker can process resumes and job descriptions, comparing requirements against candidate qualifications. This guide shows how to use ExtractThinker for automated resume screening and matching.</p>"},{"location":"examples/resume-processing/#basic-resume-processing","title":"Basic Resume Processing","text":"<p>Here's a complete example of processing job requirements and matching them against candidate resumes:</p> <pre><code>from extract_thinker import Extractor, Contract, DocumentLoaderPyPdf, LLM\nfrom typing import List, Optional\nfrom pydantic import Field\n\n# Define the job role contract\nclass RoleContract(Contract):\n    company_name: str = Field(\"Company name\")\n    years_of_experience: int = Field(\"Years of experience required. If not mention, calculate with start date and end date\")\n    is_remote: bool = Field(\"Is the role remote?\")\n    country: str = Field(\"Country of the role\")\n    city: Optional[str] = Field(\"City of the role\")\n    list_of_skills: List[str] = Field(\"\"\"\n        list of strings, e.g [\"5 years experience\", \"3 years in React\", \"Typescript\"]\n        Make the lists of skills to be a yes/no list for matching with candidates\n    \"\"\")\n\n# Define the resume contract\nclass ResumeContract(Contract):\n    name: str = Field(\"First and Last Name\")\n    age: Optional[str] = Field(\"Age with format DD/MM/YYYY. Empty if not available\")\n    email: str = Field(\"Email address\")\n    phone: Optional[str] = Field(\"Phone number\")\n    address: Optional[str] = Field(\"Address\")\n    city: Optional[str] = Field(\"City\")\n    total_experience: int = Field(\"Total experience in years\")\n    can_go_to_office: Optional[bool] = Field(\"Can go to office. If city/location is not provider, is false. If is the same city, is true\")\n    list_of_skills: List[bool] = Field(\"Takes the list of skills and returns a list of true/false, if the candidate has that skill\")\n</code></pre>"},{"location":"examples/resume-processing/#processing-job-requirements","title":"Processing Job Requirements","text":"<p>First, process the job requirements document:</p> <pre><code># Initialize extractor for job role\nextractor_job_role = Extractor()\nextractor_job_role.load_document_loader(DocumentLoaderPyPdf())\nextractor_job_role.load_llm(\"gpt-4o\")\n\n# Extract job requirements\nrole_result = extractor_job_role.extract(\n    \"Job_Offer.pdf\", \n    RoleContract\n)\n\n# Convert to YAML format for better readability\njob_role_content = \"Job Requirements:\\n\" + json_to_yaml(json.loads(role_result.json()))\n</code></pre>"},{"location":"examples/resume-processing/#processing-candidate-resumes","title":"Processing Candidate Resumes","text":"<p>Then process candidate resumes against the job requirements:</p> <pre><code># Initialize extractor for candidate resume\nextractor_candidate = Extractor()\nextractor_candidate.load_document_loader(DocumentLoaderPyPdf())\n\n# Configure LLM with Groq\nllm = LLM(\"groq/llama3-8b-8192\")  # default model\nextractor_candidate.load_llm(llm)\n\n# Process resume with job context\nresult = extractor_candidate.extract(\n    \"CV_Candidate.pdf\",\n    ResumeContract,\n    content=job_role_content  # Provide job requirements as context\n)\n</code></pre>"},{"location":"examples/resume-processing/#advanced-configuration-with-router","title":"Advanced Configuration with Router","text":"<p>For production environments, you can configure a router with multiple models and fallbacks:</p> <pre><code>def config_router():\n    rpm = 5000  # Rate limit in requests per minute\n\n    model_list = [\n        {\n            \"model_name\": \"Meta-Llama-3-8B-Instruct\",\n            \"litellm_params\": {\n                \"model\": \"deepinfra/meta-llama/Meta-Llama-3-8B-Instruct\",\n                \"api_key\": os.getenv(\"DEEPINFRA_API_KEY\"),\n                \"rpm\": rpm,\n            },\n        },\n        {\n            \"model_name\": \"Mistral-7B-Instruct-v0.2\",\n            \"litellm_params\": {\n                \"model\": \"deepinfra/mistralai/Mistral-7B-Instruct-v0.2\",\n                \"api_key\": os.getenv(\"DEEPINFRA_API_KEY\"),\n                \"rpm\": rpm,\n            }\n        },\n        {\n            \"model_name\": \"groq-llama3-8b-8192\",\n            \"litellm_params\": {\n                \"model\": \"groq/llama3-8b-8192\",\n                \"api_key\": os.getenv(\"GROQ_API_KEY\"),\n                \"rpm\": rpm,\n            }\n        },\n    ]\n\n    # Add fallback models\n    fallback_models = [\n        {\n            \"model_name\": \"claude-3-haiku-20240307\",\n            \"litellm_params\": {\n                \"model\": \"claude-3-haiku-20240307\",\n                \"api_key\": os.getenv(\"CLAUDE_API_KEY\"),\n            }\n        }\n    ]\n\n    router = Router(\n        model_list=model_list + fallback_models,\n        default_fallbacks=[\"claude-3-haiku-20240307\"],\n        context_window_fallbacks=[\n            {\"Meta-Llama-3-8B-Instruct\": [\"claude-3-haiku-20240307\"]},\n            {\"groq-llama3-8b-8192\": [\"claude-3-haiku-20240307\"]},\n            {\"Mistral-7B-Instruct-v0.2\": [\"claude-3-haiku-20240307\"]}\n        ],\n        set_verbose=True\n    )\n\n    return router\n</code></pre>"},{"location":"examples/resume-processing/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Automated resume screening</li> <li>Skill matching against job requirements</li> <li>Experience verification</li> <li>Location compatibility checking</li> <li>Batch processing of multiple resumes</li> </ul>"},{"location":"getting-started/","title":"Extract Thinker","text":""},{"location":"getting-started/#the-first-framework-for-document-intelligence-processing-idp-for-llms","title":"The first Framework for Document Intelligence Processing (IDP) - for LLMs","text":"\u2605 Star the Repo Examples Production Workflows <p>Is a flexible document intelligence framework that helps you extract and classify structured data from various documents, acting like an ORM for document processing workflows. One phrase you say is \"Document Intelligence for LLMs\" or \"LangChain for Intelligent Document Processing.\" The motivation is to create niche features required for document processing, like splitting large documents and advanced classification.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install using pip:</p> <pre><code>pip install extract_thinker\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>Here's a simple example that extracts invoice data from a PDF:</p> <pre><code>from extract_thinker import Extractor, DocumentLoaderPyPdf, Contract\n\n# Define what data you want to extract\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    total_amount: float\n\n# Initialize the extractor\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\nextractor.load_llm(\"gpt-4\")  # or any other supported model\n\n# Extract data from your document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n\nprint(f\"Invoice #{result.invoice_number}\")\nprint(f\"Date: {result.invoice_date}\")\nprint(f\"Total: ${result.total_amount}\")\n</code></pre>"},{"location":"getting-started/#native-features-that-you-want","title":"Native Features that you want","text":"<ul> <li> <p> Extraction with Pydantic</p> <p>Extract structured data from any document type using Pydantic models for validation, custom features, and prompt engineering capabilities.</p> </li> <li> <p> Classification &amp; Split</p> <p>Intelligent document classification and splitting with support for consensus strategies, eager/lazy splitting, and confidence thresholds.</p> </li> <li> <p> PII Detection</p> <p>Automatically detect and handle sensitive personal information in documents with privacy-first approach and advanced validation.</p> </li> <li> <p> LLM and OCR Agnostic</p> <p>Freedom to choose and switch between different LLM providers and OCR engines based on your needs and cost requirements.</p> </li> </ul>"}]}